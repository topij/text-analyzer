{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Notebook: Semantic Analysis Pipeline\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = str(Path().resolve().parent)\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from typing import List, Dict, Any, Tuple, Union\n",
    "\n",
    "# Import necessary components\n",
    "from src.nb_helpers.environment import setup_notebook_env, verify_environment\n",
    "from src.semantic_analyzer import SemanticAnalyzer\n",
    "from src.utils.FileUtils.file_utils import FileUtils\n",
    "# from src.loaders.parameter_handler import get_parameter_file_path\n",
    "# from src.analyzers import KeywordAnalyzer, ThemeAnalyzer, CategoryAnalyzer\n",
    "from src.analyzers.category_analyzer import CategoryAnalyzer\n",
    "from src.analyzers.keyword_analyzer import KeywordAnalyzer\n",
    "from src.analyzers.theme_analyzer import ThemeAnalyzer\n",
    "\n",
    "# from src.core.config import AnalyzerConfig\n",
    "from src.core.language_processing import create_text_processor\n",
    "from src.core.llm.factory import create_llm\n",
    "from src.loaders.parameter_handler import (\n",
    "    ParameterHandler,\n",
    "    # get_parameter_file_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.nb_helpers.logging import configure_logging, verify_logging_setup_with_hierarchy, reset_debug_logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_debug_logging():\n",
    "    \"\"\"Set up proper debug logging.\"\"\"\n",
    "    # Reset all loggers to DEBUG\n",
    "    root = logging.getLogger()\n",
    "    root.setLevel(logging.DEBUG)\n",
    "    \n",
    "    # Clear existing handlers\n",
    "    for handler in root.handlers[:]:\n",
    "        root.removeHandler(handler)\n",
    "    \n",
    "    # Add new handler with formatter\n",
    "    handler = logging.StreamHandler()\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    handler.setFormatter(formatter)\n",
    "    handler.setLevel(logging.DEBUG)\n",
    "    root.addHandler(handler)\n",
    "    \n",
    "    # Set specific loggers\n",
    "    debug_loggers = [\n",
    "        \"src.core.language_processing.finnish\",\n",
    "        \"src.analyzers.keyword_analyzer\",\n",
    "        \"src.semantic_analyzer.analyzer\",\n",
    "    ]\n",
    "    \n",
    "    for logger_name in debug_loggers:\n",
    "        logger = logging.getLogger(logger_name)\n",
    "        logger.setLevel(logging.DEBUG)\n",
    "        logger.propagate = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logging Configuration:\n",
      "--------------------------------------------------\n",
      "\n",
      "Logger: root\n",
      "Set Level: WARNING\n",
      "Effective Level: WARNING\n",
      "Propagates to root: True\n",
      "No handlers (uses root handlers)\n",
      "\n",
      "Logger: src.nb_helpers.analyzers\n",
      "Hierarchy:\n",
      "  src: NOTSET\n",
      "  src.nb_helpers: NOTSET\n",
      "  src.nb_helpers.analyzers: NOTSET\n",
      "Set Level: NOTSET\n",
      "Effective Level: WARNING\n",
      "Propagates to root: True\n",
      "No handlers (uses root handlers)\n",
      "\n",
      "Logger: src.analyzers.keyword_analyzer\n",
      "Hierarchy:\n",
      "  src: NOTSET\n",
      "  src.analyzers: NOTSET\n",
      "  src.analyzers.keyword_analyzer: NOTSET\n",
      "Set Level: NOTSET\n",
      "Effective Level: WARNING\n",
      "Propagates to root: True\n",
      "No handlers (uses root handlers)\n",
      "\n",
      "Logger: src.analyzers.theme_analyzer\n",
      "Hierarchy:\n",
      "  src: NOTSET\n",
      "  src.analyzers: NOTSET\n",
      "  src.analyzers.theme_analyzer: NOTSET\n",
      "Set Level: NOTSET\n",
      "Effective Level: WARNING\n",
      "Propagates to root: True\n",
      "No handlers (uses root handlers)\n",
      "\n",
      "Logger: src.analyzers.category_analyzer\n",
      "Hierarchy:\n",
      "  src: NOTSET\n",
      "  src.analyzers: NOTSET\n",
      "  src.analyzers.category_analyzer: NOTSET\n",
      "Set Level: NOTSET\n",
      "Effective Level: WARNING\n",
      "Propagates to root: True\n",
      "No handlers (uses root handlers)\n",
      "\n",
      "Logger: src.utils.FileUtils.file_utils\n",
      "Hierarchy:\n",
      "  src: NOTSET\n",
      "  src.utils: NOTSET\n",
      "  src.utils.FileUtils: NOTSET\n",
      "  src.utils.FileUtils.file_utils: NOTSET\n",
      "Set Level: NOTSET\n",
      "Effective Level: WARNING\n",
      "Propagates to root: True\n",
      "No handlers (uses root handlers)\n",
      "\n",
      "Logger: httpx\n",
      "Hierarchy:\n",
      "  httpx: INFO\n",
      "Set Level: INFO\n",
      "Effective Level: INFO\n",
      "Propagates to root: True\n",
      "No handlers (uses root handlers)\n"
     ]
    }
   ],
   "source": [
    "# Run this before testing\n",
    "debug = False\n",
    "if debug:\n",
    "    setup_debug_logging()\n",
    "# Keep HTTP loggers at INFO\n",
    "for name in [\"httpx\", \"httpcore\", \"openai\", \"anthropic\"]:\n",
    "    logging.getLogger(name).setLevel(logging.INFO)\n",
    "verify_logging_setup_with_hierarchy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Check Results:\n",
      "==================================================\n",
      "\n",
      "Basic Setup:\n",
      "-----------\n",
      "✓ Project root in path\n",
      "✓ FileUtils initialized\n",
      "✓ .env file loaded\n",
      "\n",
      "Environment Variables:\n",
      "---------------------\n",
      "✓ OPENAI_API_KEY set\n",
      "✓ ANTHROPIC_API_KEY set\n",
      "\n",
      "Project Structure:\n",
      "-----------------\n",
      "✓ Raw data exists\n",
      "✓ Processed data exists\n",
      "✓ Configuration exists\n",
      "✓ Main config.yaml exists\n",
      "\n",
      "==================================================\n",
      "Environment Status: Ready ✓\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up environment with DEBUG level for detailed logs\n",
    "# setup_notebook_env(log_level=\"DEBUG\")\n",
    "verify_environment()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify_logging_setup_with_hierarchy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify_logging_setup_with_hierarchy()\n",
    "\n",
    "# Initialize FileUtils\n",
    "file_utils = FileUtils()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test content\n",
    "def load_test_content(language: str) -> dict:\n",
    "    \"\"\"Load test content from Excel file.\"\"\"\n",
    "    content_file = f\"test_content_{language}.xlsx\"\n",
    "    df = file_utils.load_single_file(content_file, input_type=\"raw\")\n",
    "    \n",
    "    # Group content by type\n",
    "    content_by_type = {}\n",
    "    for _, row in df.iterrows():\n",
    "        content_by_type[f\"{row['type']}_{row['id']}\"] = row['content']\n",
    "    return content_by_type\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_individual_analyzer(\n",
    "    analyzer: Union[KeywordAnalyzer, ThemeAnalyzer, CategoryAnalyzer], \n",
    "    text: str, \n",
    "    analyzer_type: str\n",
    "):\n",
    "    \"\"\"Test individual analyzer component.\"\"\"\n",
    "    print(f\"\\nTesting {analyzer_type} Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"\\nInput text:\")\n",
    "    print(text[:200] + \"...\" if len(text) > 200 else text)\n",
    "    \n",
    "    try:\n",
    "        results = await analyzer.analyze(text)\n",
    "        \n",
    "        print(\"\\nResults:\")\n",
    "        print(\"-\" * 20)\n",
    "        \n",
    "        if isinstance(analyzer, KeywordAnalyzer):\n",
    "            if results.keywords:\n",
    "                print(\"\\nKeywords:\")\n",
    "                for kw in results.keywords[:10]:  # Show top 10\n",
    "                    print(f\"• {kw.keyword:<20} ({kw.score:.2f})\")\n",
    "                    if kw.domain:\n",
    "                        print(f\"  Domain: {kw.domain}\")\n",
    "                \n",
    "                if results.compound_words:\n",
    "                    print(\"\\nCompound Words:\")\n",
    "                    print(\", \".join(results.compound_words))\n",
    "                    \n",
    "                if results.domain_keywords:\n",
    "                    print(\"\\nKeywords by Domain:\")\n",
    "                    for domain, kws in results.domain_keywords.items():\n",
    "                        print(f\"\\n{domain}:\")\n",
    "                        print(\", \".join(kws))\n",
    "                        \n",
    "        elif isinstance(analyzer, ThemeAnalyzer):\n",
    "            if results.themes:\n",
    "                print(\"\\nThemes:\")\n",
    "                for theme in results.themes:\n",
    "                    print(f\"\\n• {theme.name}\")\n",
    "                    print(f\"  Confidence: {theme.confidence:.2f}\")\n",
    "                    print(f\"  Description: {theme.description}\")\n",
    "                    if theme.keywords:\n",
    "                        print(f\"  Keywords: {', '.join(theme.keywords)}\")\n",
    "                \n",
    "                if results.theme_hierarchy:\n",
    "                    print(\"\\nTheme Hierarchy:\")\n",
    "                    for parent, children in results.theme_hierarchy.items():\n",
    "                        print(f\"{parent} -> {', '.join(children)}\")\n",
    "                        \n",
    "        elif isinstance(analyzer, CategoryAnalyzer):\n",
    "            if results.categories:\n",
    "                print(\"\\nCategories:\")\n",
    "                for cat in results.categories:\n",
    "                    print(f\"\\n• {cat.name}\")\n",
    "                    print(f\"  Confidence: {cat.confidence:.2f}\")\n",
    "                    if cat.description:\n",
    "                        print(f\"  Description: {cat.description}\")\n",
    "                    if cat.evidence:\n",
    "                        print(\"\\n  Evidence:\")\n",
    "                        for ev in cat.evidence:\n",
    "                            print(f\"  - {ev.text} (relevance: {ev.relevance:.2f})\")\n",
    "                            \n",
    "        if hasattr(results, 'error') and results.error:\n",
    "            print(f\"\\nErrors occurred: {results.error}\")\n",
    "            \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in analysis: {e}\")\n",
    "        return None\n",
    "\n",
    "async def test_all_components():\n",
    "    \"\"\"Test all components individually and the full pipeline.\"\"\"\n",
    "    print(\"Starting Component Tests\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Initialize analyzers with Finnish parameters\n",
    "    file_utils = FileUtils()\n",
    "    parameter_handler = ParameterHandler(\"parameters_fi.xlsx\")\n",
    "    llm = create_llm()\n",
    "    language_processor = create_text_processor(language=\"fi\")\n",
    "    \n",
    "    # Create individual analyzers\n",
    "    keyword_analyzer = KeywordAnalyzer(\n",
    "        llm=llm,\n",
    "        config=parameter_handler.parameters.general.model_dump(),\n",
    "        language_processor=language_processor\n",
    "    )\n",
    "    \n",
    "    theme_analyzer = ThemeAnalyzer(\n",
    "        llm=llm,\n",
    "        config=parameter_handler.parameters.general.model_dump(),\n",
    "        language_processor=language_processor\n",
    "    )\n",
    "    \n",
    "    category_analyzer = CategoryAnalyzer(\n",
    "        categories=parameter_handler.parameters.categories,\n",
    "        llm=llm,\n",
    "        config=parameter_handler.parameters.general.model_dump(),\n",
    "        language_processor=language_processor\n",
    "    )\n",
    "    \n",
    "    # Load test content\n",
    "    test_content = load_test_content(\"fi\")\n",
    "    tech_text = test_content[\"technical_technical_1\"]\n",
    "    business_text = test_content[\"business_business_1\"]\n",
    "    \n",
    "    # Test each analyzer individually\n",
    "    print(\"\\nTesting Technical Content\")\n",
    "    print(\"-\" * 30)\n",
    "    await test_individual_analyzer(keyword_analyzer, tech_text, \"Keyword\")\n",
    "    await test_individual_analyzer(theme_analyzer, tech_text, \"Theme\")\n",
    "    await test_individual_analyzer(category_analyzer, tech_text, \"Category\")\n",
    "    \n",
    "    print(\"\\nTesting Business Content\")\n",
    "    print(\"-\" * 30)\n",
    "    await test_individual_analyzer(keyword_analyzer, business_text, \"Keyword\")\n",
    "    await test_individual_analyzer(theme_analyzer, business_text, \"Theme\")\n",
    "    await test_individual_analyzer(category_analyzer, business_text, \"Category\")\n",
    "    \n",
    "    # Test full pipeline\n",
    "    print(\"\\nTesting Full Pipeline\")\n",
    "    print(\"-\" * 30)\n",
    "    # await run_pipeline_tests()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage in notebook\n",
    "# if __name__ == \"__main__\":\n",
    "# Test individual components\n",
    "# await test_all_components()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "        Pilvipalveluihin siirtyminen paransi järjestelmän skaalautuvuutta ja vähensi kustannuksia.\n",
    "        Uudet DevOps-käytännöt tehostivat merkittävästi käyttöönottoprosessia.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Or test specific analyzer\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m analyzer \u001b[38;5;241m=\u001b[39m \u001b[43mKeywordAnalyzer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_text_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfi\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m test_individual_analyzer(analyzer, text, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKeyword\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\src\\analyzers\\keyword_analyzer.py:201\u001b[0m, in \u001b[0;36mKeywordAnalyzer.__init__\u001b[1;34m(self, llm, config, language_processor)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlanguage_processor \u001b[38;5;241m=\u001b[39m language_processor\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_weights(config)\n\u001b[1;32m--> 201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclustering_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_clustering_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_frequency_cache \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\src\\analyzers\\keyword_analyzer.py:225\u001b[0m, in \u001b[0;36mKeywordAnalyzer._initialize_clustering_config\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_initialize_clustering_config\u001b[39m(\u001b[38;5;28mself\u001b[39m, config: Optional[Dict]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict:\n\u001b[0;32m    224\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Initialize clustering configuration.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m(\n\u001b[0;32m    226\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclustering\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    227\u001b[0m         {\n\u001b[0;32m    228\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimilarity_threshold\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.85\u001b[39m,\n\u001b[0;32m    229\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_cluster_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m    230\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboost_factor\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1.2\u001b[39m,\n\u001b[0;32m    231\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdomain_bonus\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.1\u001b[39m,\n\u001b[0;32m    232\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_cluster_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    233\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_relation_distance\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    234\u001b[0m         },\n\u001b[0;32m    235\u001b[0m     )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "# Or test specific analyzer\n",
    "analyzer = KeywordAnalyzer(language_processor=create_text_processor(language=\"fi\"))\n",
    "await test_individual_analyzer(analyzer, text, \"Keyword\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function for single text analysis\n",
    "async def test_analyze_text(analyzer: SemanticAnalyzer, text: str, language: str):\n",
    "    \"\"\"Run analysis and display results.\"\"\"\n",
    "    print(f\"\\nAnalyzing {language} text:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"\\nInput text:\")\n",
    "    print(text[:200] + \"...\" if len(text) > 200 else text)\n",
    "    \n",
    "    try:\n",
    "        results = await analyzer.analyze(text)\n",
    "        \n",
    "        print(\"\\nResults:\")\n",
    "        print(\"-\" * 20)\n",
    "        \n",
    "        # Keywords\n",
    "        if results.keywords and results.keywords.keywords:\n",
    "            print(\"\\nKeywords:\")\n",
    "            for kw in results.keywords.keywords[:10]:  # Show top 10\n",
    "                print(f\"• {kw.keyword:<20} ({kw.score:.2f})\")\n",
    "                if kw.domain:\n",
    "                    print(f\"  Domain: {kw.domain}\")\n",
    "            \n",
    "            if results.keywords.compound_words:\n",
    "                print(\"\\nCompound Words:\")\n",
    "                print(\", \".join(results.keywords.compound_words))\n",
    "                \n",
    "            if results.keywords.domain_keywords:\n",
    "                print(\"\\nKeywords by Domain:\")\n",
    "                for domain, kws in results.keywords.domain_keywords.items():\n",
    "                    print(f\"\\n{domain}:\")\n",
    "                    print(\", \".join(kws))\n",
    "        else:\n",
    "            print(\"\\nNo keywords found\")\n",
    "            if results.keywords.error:\n",
    "                print(\"Error:\", results.keywords.error)\n",
    "        \n",
    "        # Process other results similarly...\n",
    "        print(f\"\\nProcessing time: {results.processing_time:.2f}s\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError running analysis: {e}\")\n",
    "        return None\n",
    "    \n",
    "# Main test routine\n",
    "async def run_pipeline_tests():\n",
    "    print(\"Starting Semantic Analysis Pipeline Tests\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Test English pipeline\n",
    "    print(\"\\nEnglish Pipeline Test\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Initialize English analyzer\n",
    "    en_analyzer = SemanticAnalyzer(parameter_file=\"parameters_en.xlsx\")\n",
    "    en_content = load_test_content(\"en\")\n",
    "    \n",
    "    # Test technical content\n",
    "    await test_analyze_text(\n",
    "        en_analyzer,\n",
    "        en_content[\"technical_technical_1\"],\n",
    "        \"English\"\n",
    "    )\n",
    "    \n",
    "    # Test business content\n",
    "    await test_analyze_text(\n",
    "        en_analyzer,\n",
    "        en_content[\"business_business_1\"],\n",
    "        \"English\"\n",
    "    )\n",
    "    \n",
    "    # Finnish Pipeline Test\n",
    "    print(\"\\nFinnish Pipeline Test\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Initialize Finnish analyzer\n",
    "    fi_analyzer = SemanticAnalyzer(parameter_file=\"parameters_fi.xlsx\")\n",
    "    fi_content = load_test_content(\"fi\")\n",
    "    \n",
    "    # Test technical content\n",
    "    await test_analyze_text(\n",
    "        fi_analyzer,\n",
    "        fi_content[\"technical_technical_1\"],\n",
    "        \"Finnish\"\n",
    "    )\n",
    "    \n",
    "    # Test business content\n",
    "    await test_analyze_text(\n",
    "        fi_analyzer,\n",
    "        fi_content[\"business_business_1\"],\n",
    "        \"Finnish\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Semantic Analysis Pipeline Tests\n",
      "==================================================\n",
      "\n",
      "English Pipeline Test\n",
      "------------------------------\n",
      "\n",
      "Analyzing English text:\n",
      "==================================================\n",
      "\n",
      "Input text:\n",
      "Machine learning models are trained using large datasets to recognize patterns. The neural network architecture includes multiple layers for feature extraction. Data preprocessing and feature engineer...\n",
      "\n",
      "Results:\n",
      "--------------------\n",
      "\n",
      "Keywords:\n",
      "• machine learning     (0.95)\n",
      "  Domain: technical\n",
      "• neural network       (0.95)\n",
      "  Domain: technical\n",
      "• feature extraction   (0.90)\n",
      "  Domain: technical\n",
      "• data preprocessing   (0.90)\n",
      "  Domain: technical\n",
      "• feature engineering  (0.90)\n",
      "  Domain: technical\n",
      "• pipeline             (0.85)\n",
      "  Domain: technical\n",
      "• architecture         (0.85)\n",
      "  Domain: technical\n",
      "• dataset              (0.80)\n",
      "  Domain: technical\n",
      "\n",
      "Compound Words:\n",
      "machine learning, neural network, feature extraction, data preprocessing, feature engineering, pipeline, architecture, dataset\n",
      "\n",
      "Keywords by Domain:\n",
      "\n",
      "technical:\n",
      "machine learning, neural network, feature extraction, data preprocessing, feature engineering, pipeline, architecture, dataset\n",
      "\n",
      "Processing time: 9.53s\n",
      "\n",
      "Analyzing English text:\n",
      "==================================================\n",
      "\n",
      "Input text:\n",
      "Q3 financial results show 15% revenue growth and improved profit margins. Customer acquisition costs decreased while retention rates increased. Market expansion strategy focuses on emerging technology...\n",
      "\n",
      "Results:\n",
      "--------------------\n",
      "\n",
      "Keywords:\n",
      "• revenue growth       (0.95)\n",
      "  Domain: business\n",
      "• customer acquisition costs (0.95)\n",
      "  Domain: business\n",
      "• retention rates      (0.90)\n",
      "  Domain: business\n",
      "• market expansion strategy (0.90)\n",
      "  Domain: business\n",
      "• profit margins       (0.85)\n",
      "  Domain: business\n",
      "\n",
      "Compound Words:\n",
      "revenue growth, customer acquisition costs, retention rates, market expansion strategy, profit margins\n",
      "\n",
      "Keywords by Domain:\n",
      "\n",
      "business:\n",
      "revenue growth, customer acquisition costs, retention rates, market expansion strategy, profit margins\n",
      "\n",
      "Processing time: 7.32s\n",
      "\n",
      "Finnish Pipeline Test\n",
      "------------------------------\n",
      "\n",
      "Analyzing Finnish text:\n",
      "==================================================\n",
      "\n",
      "Input text:\n",
      "Koneoppimismalleja koulutetaan suurilla datajoukolla tunnistamaan kaavoja. Neuroverkon arkkitehtuuri sisältää useita kerroksia piirteiden erottamiseen. Datan esikäsittely ja piirteiden suunnittelu ova...\n",
      "\n",
      "Results:\n",
      "--------------------\n",
      "\n",
      "Keywords:\n",
      "• koneoppimismalli     (0.95)\n",
      "  Domain: technical\n",
      "• datajoukko           (0.95)\n",
      "  Domain: technical\n",
      "• neuroverkon arkkitehtuuri (0.95)\n",
      "  Domain: technical\n",
      "• esikäsittely         (0.90)\n",
      "  Domain: technical\n",
      "• piirteiden suunnittelu (0.90)\n",
      "  Domain: technical\n",
      "\n",
      "Compound Words:\n",
      "koneoppimismalli, datajoukko\n",
      "\n",
      "Keywords by Domain:\n",
      "\n",
      "technical:\n",
      "koneoppimismalli, datajoukko, neuroverkon arkkitehtuuri, esikäsittely, piirteiden suunnittelu\n",
      "\n",
      "Processing time: 9.87s\n",
      "\n",
      "Analyzing Finnish text:\n",
      "==================================================\n",
      "\n",
      "Input text:\n",
      "Q3 taloudelliset tulokset osoittavat 15% liikevaihdon kasvun ja parantuneet katteet. Asiakashankinnan kustannukset laskivat ja asiakaspysyvyys parani. Markkinalaajennusstrategia keskittyy nouseviin te...\n",
      "\n",
      "Results:\n",
      "--------------------\n",
      "\n",
      "Keywords:\n",
      "• taloudellinen        (0.95)\n",
      "  Domain: business\n",
      "• tulos                (0.95)\n",
      "  Domain: business\n",
      "• liikevaihto          (0.95)\n",
      "  Domain: business\n",
      "• kasvu                (0.95)\n",
      "  Domain: business\n",
      "• kate                 (0.95)\n",
      "  Domain: business\n",
      "\n",
      "Compound Words:\n",
      "liikevaihto\n",
      "\n",
      "Keywords by Domain:\n",
      "\n",
      "business:\n",
      "taloudellinen, tulos, liikevaihto, kasvu, kate\n",
      "\n",
      "Processing time: 16.03s\n"
     ]
    }
   ],
   "source": [
    "# Run the tests\n",
    "await run_pipeline_tests()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semantic-analyzer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
