{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Environment Semantic Analysis Demo\n",
    "\n",
    "See also separate [documentation](../docs/ANALYSIS_DEMO_DOC.md) sheet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "# Add project root to path (for local environment)\n",
    "project_root = str(Path().resolve().parent)\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import main interface to analyzers\n",
    "from src.semantic_analyzer import SemanticAnalyzer\n",
    "\n",
    "# import formatting\n",
    "from src.utils.formatting_config import OutputDetail, ExcelOutputConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import environment setup\n",
    "from src.core.managers import EnvironmentManager, EnvironmentConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-16 17:55:16,829 - FileUtils.core.file_utils - INFO - Project root: /Users/topi/data-science/repos/text-analyzer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-16 17:55:16,829 - FileUtils.core.file_utils - INFO - Project root: /Users/topi/data-science/repos/text-analyzer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-16 17:55:16,829 - FileUtils.core.file_utils - INFO - FileUtils initialized with local storage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-16 17:55:16,829 - FileUtils.core.file_utils - INFO - FileUtils initialized with local storage\n",
      "2025-02-16 17:55:16,848 - src.core.managers.environment_manager - INFO - Environment initialized successfully\n"
     ]
    }
   ],
   "source": [
    "# Set environment type\n",
    "ENV_TYPE = \"local\"  # Change to \"azure\" when running in Azure ML and you want persistent blob storage\n",
    "\n",
    "# Configure environment\n",
    "env_config = EnvironmentConfig(\n",
    "    env_type=ENV_TYPE,\n",
    "    project_root=Path().resolve().parent,\n",
    "    log_level=\"INFO\" # use config.yaml or .env for now to change logging level\n",
    ")\n",
    "environment = EnvironmentManager(env_config)\n",
    "\n",
    "# Get initialized components\n",
    "components = environment.get_components()\n",
    "file_utils = components[\"file_utils\"]\n",
    "\n",
    "# Configure logging for HTTP clients\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"httpcore\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User defined setup\n",
    "- parameter file (how) and content file to be analyzed (what)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter_file = \"parameters_en.xlsx\"\n",
    "# content_file = \"test_content_en.xlsx\"\n",
    "\n",
    "parameter_file = \"support_parameters_fi.xlsx\"\n",
    "content_file = \"support_test_content_fi.xlsx\"\n",
    "# parameter_file = \"business_parameters_en.xlsx\"\n",
    "# content_file = \"business_test_content_en.xlsx\"\n",
    "\n",
    "# Change to True if you want to use Azure OpenAI API, if not already defined in config.yaml\n",
    "azure = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Analyzer\n",
    "\n",
    "-  Initialize analyzer with formatting config\n",
    "-  Parameter file paths are handled automatically by FileUtils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-16 17:55:17,035 - src.core.language_processing.finnish - INFO - Loaded 747 stopwords from /Users/topi/data-science/repos/text-analyzer/data/config/stop_words/fi.txt\n",
      "2025-02-16 17:55:17,035 - src.core.language_processing.finnish.VoikkoHandler - DEBUG - Initialized with config: {'min_keyword_length': 3, 'include_compounds': True}\n",
      "2025-02-16 17:55:17,035 - src.core.language_processing.finnish.VoikkoHandler - DEBUG - Trying library paths: ['/opt/homebrew/lib/libvoikko.dylib', '/usr/local/lib/libvoikko.dylib']\n",
      "2025-02-16 17:55:17,035 - src.core.language_processing.finnish.VoikkoHandler - DEBUG - Trying dictionary paths: ['/opt/homebrew/lib/voikko', '/usr/local/lib/voikko', '/usr/local/share/voikko']\n",
      "2025-02-16 17:55:17,042 - src.core.language_processing.finnish.VoikkoHandler - INFO - Successfully initialized Voikko with path: /opt/homebrew/lib/voikko\n",
      "2025-02-16 17:55:17,174 - src.semantic_analyzer.analyzer - INFO - Verifying analyzer configuration:\n",
      "2025-02-16 17:55:17,174 - src.semantic_analyzer.analyzer - INFO - Language: fi\n",
      "2025-02-16 17:55:17,174 - src.semantic_analyzer.analyzer - INFO - Categories loaded: 3\n",
      "2025-02-16 17:55:17,175 - src.semantic_analyzer.analyzer - INFO -   - kirjautumisongelmat: 4 keywords, threshold: 0.6\n",
      "2025-02-16 17:55:17,175 - src.semantic_analyzer.analyzer - INFO -   - järjestelmävirheet: 4 keywords, threshold: 0.6\n",
      "2025-02-16 17:55:17,175 - src.semantic_analyzer.analyzer - INFO -   - dokumentaatio-ongelmat: 4 keywords, threshold: 0.6\n",
      "2025-02-16 17:55:17,176 - src.semantic_analyzer.analyzer - INFO - Language processor: FinnishTextProcessor\n",
      "2025-02-16 17:55:17,176 - src.semantic_analyzer.analyzer - INFO - All analyzers initialized for language: fi\n",
      "2025-02-16 17:55:17,176 - src.semantic_analyzer.analyzer - INFO - Semantic Analyzer initialized successfully\n"
     ]
    }
   ],
   "source": [
    "# Example texts\n",
    "texts = {\n",
    "    \"en\": \"Machine learning models analyze data efficiently.\",\n",
    "    \"fi\": \"Koneoppimismallit analysoivat dataa tehokkaasti.\"\n",
    "}\n",
    "\n",
    "# Initialize analyzer\n",
    "analyzer = SemanticAnalyzer(\n",
    "    parameter_file=parameter_file,\n",
    "    file_utils=file_utils\n",
    ")\n",
    "\n",
    "async def analyze_text(text: str, language: str):\n",
    "    # Set the language first\n",
    "    analyzer.set_language(language)\n",
    "    \n",
    "    result = await analyzer.analyze(\n",
    "        text=text,\n",
    "        analysis_types=[\"keywords\", \"themes\", \"categories\"]\n",
    "    )\n",
    "    \n",
    "    if result.success:\n",
    "        print(f\"\\nAnalysis results for {language}:\")\n",
    "        print(\"Keywords:\")\n",
    "        for kw in result.keywords.keywords:\n",
    "            print(f\"• {kw.keyword} (score: {kw.score:.2f})\")\n",
    "            \n",
    "        print(\"\\nThemes:\")\n",
    "        for theme in result.themes.themes:\n",
    "            print(f\"• {theme.name} ({theme.confidence:.2f})\")\n",
    "            \n",
    "        if result.categories and result.categories.matches:\n",
    "            print(\"\\nCategories:\")\n",
    "            for cat in result.categories.matches:\n",
    "                print(f\"• {cat.name} ({cat.confidence:.2f})\")\n",
    "    else:\n",
    "        print(f\"Error: {result.error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single text analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Single Text Analysis ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-16 17:55:18,809 - src.semantic_analyzer.analyzer - INFO - Verifying analyzer configuration:\n",
      "2025-02-16 17:55:18,809 - src.semantic_analyzer.analyzer - INFO - Language: en\n",
      "2025-02-16 17:55:18,809 - src.semantic_analyzer.analyzer - INFO - Categories loaded: 3\n",
      "2025-02-16 17:55:18,810 - src.semantic_analyzer.analyzer - INFO -   - kirjautumisongelmat: 4 keywords, threshold: 0.6\n",
      "2025-02-16 17:55:18,810 - src.semantic_analyzer.analyzer - INFO -   - järjestelmävirheet: 4 keywords, threshold: 0.6\n",
      "2025-02-16 17:55:18,810 - src.semantic_analyzer.analyzer - INFO -   - dokumentaatio-ongelmat: 4 keywords, threshold: 0.6\n",
      "2025-02-16 17:55:18,810 - src.semantic_analyzer.analyzer - INFO - Language processor: EnglishTextProcessor\n",
      "2025-02-16 17:55:18,811 - src.semantic_analyzer.analyzer - INFO - All analyzers initialized for language: en\n",
      "2025-02-16 17:55:18,811 - src.semantic_analyzer.analyzer - INFO - Language switched to en\n",
      "2025-02-16 17:55:22,008 - src.core.language_processing.finnish - INFO - Loaded 747 stopwords from /Users/topi/data-science/repos/text-analyzer/data/config/stop_words/fi.txt\n",
      "2025-02-16 17:55:22,009 - src.core.language_processing.finnish.VoikkoHandler - DEBUG - Initialized with config: {'min_keyword_length': 3, 'include_compounds': True}\n",
      "2025-02-16 17:55:22,009 - src.core.language_processing.finnish.VoikkoHandler - DEBUG - Trying library paths: ['/opt/homebrew/lib/libvoikko.dylib', '/usr/local/lib/libvoikko.dylib']\n",
      "2025-02-16 17:55:22,009 - src.core.language_processing.finnish.VoikkoHandler - DEBUG - Trying dictionary paths: ['/opt/homebrew/lib/voikko', '/usr/local/lib/voikko', '/usr/local/share/voikko']\n",
      "2025-02-16 17:55:22,011 - src.core.language_processing.finnish.VoikkoHandler - INFO - Successfully initialized Voikko with path: /opt/homebrew/lib/voikko\n",
      "2025-02-16 17:55:22,012 - src.core.language_processing.finnish - INFO - Loaded 747 stopwords from /Users/topi/data-science/repos/text-analyzer/data/config/stop_words/fi.txt\n",
      "2025-02-16 17:55:22,012 - src.core.language_processing.finnish.VoikkoHandler - DEBUG - Initialized with config: {'min_keyword_length': 3, 'include_compounds': True}\n",
      "2025-02-16 17:55:22,012 - src.core.language_processing.finnish.VoikkoHandler - DEBUG - Trying library paths: ['/opt/homebrew/lib/libvoikko.dylib', '/usr/local/lib/libvoikko.dylib']\n",
      "2025-02-16 17:55:22,012 - src.core.language_processing.finnish.VoikkoHandler - DEBUG - Trying dictionary paths: ['/opt/homebrew/lib/voikko', '/usr/local/lib/voikko', '/usr/local/share/voikko']\n",
      "2025-02-16 17:55:22,014 - src.core.language_processing.finnish.VoikkoHandler - INFO - Successfully initialized Voikko with path: /opt/homebrew/lib/voikko\n",
      "2025-02-16 17:55:22,029 - src.semantic_analyzer.analyzer - INFO - Verifying analyzer configuration:\n",
      "2025-02-16 17:55:22,030 - src.semantic_analyzer.analyzer - INFO - Language: fi\n",
      "2025-02-16 17:55:22,030 - src.semantic_analyzer.analyzer - INFO - Categories loaded: 3\n",
      "2025-02-16 17:55:22,030 - src.semantic_analyzer.analyzer - INFO -   - kirjautumisongelmat: 4 keywords, threshold: 0.6\n",
      "2025-02-16 17:55:22,030 - src.semantic_analyzer.analyzer - INFO -   - järjestelmävirheet: 4 keywords, threshold: 0.6\n",
      "2025-02-16 17:55:22,031 - src.semantic_analyzer.analyzer - INFO -   - dokumentaatio-ongelmat: 4 keywords, threshold: 0.6\n",
      "2025-02-16 17:55:22,031 - src.semantic_analyzer.analyzer - INFO - Language processor: FinnishTextProcessor\n",
      "2025-02-16 17:55:22,031 - src.semantic_analyzer.analyzer - INFO - All analyzers initialized for language: fi\n",
      "2025-02-16 17:55:22,032 - src.semantic_analyzer.analyzer - INFO - Language switched to fi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analysis results for en:\n",
      "Keywords:\n",
      "• machine learning (score: 0.90)\n",
      "• model (score: 0.70)\n",
      "• analyze (score: 0.60)\n",
      "• data (score: 0.80)\n",
      "• efficiently (score: 0.50)\n",
      "\n",
      "Themes:\n",
      "• Efficiency of Machine Learning (0.90)\n",
      "\n",
      "Categories:\n",
      "• Machine Learning (0.90)\n",
      "• Data Analysis (0.85)\n",
      "\n",
      "Analysis results for fi:\n",
      "Keywords:\n",
      "• koneoppimismalli (score: 0.90)\n",
      "• data (score: 0.80)\n",
      "• analysoida (score: 0.70)\n",
      "• tehokas (score: 0.60)\n",
      "\n",
      "Themes:\n",
      "• Koneoppimismallit (0.90)\n",
      "• Datan analysointi (0.85)\n",
      "• Tehokkuus (0.80)\n",
      "\n",
      "Categories:\n",
      "• järjestelmävirheet (0.70)\n",
      "• dokumentaatio-ongelmat (0.50)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Single Text Analysis ===\")\n",
    "for lang, text in texts.items():\n",
    "    await analyze_text(text, lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Excel processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-16 17:55:25,950 - src.semantic_analyzer.analyzer - INFO - Running analysis types: ['categories', 'keywords', 'themes']\n",
      "Processing rows:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing row 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  25%|██▌       | 1/4 [00:09<00:29,  9.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Row completed\n",
      "\n",
      "Processing row 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  50%|█████     | 2/4 [00:22<00:22, 11.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Row completed\n",
      "\n",
      "Processing row 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  75%|███████▌  | 3/4 [00:31<00:10, 10.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Row completed\n",
      "\n",
      "Processing row 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████| 4/4 [00:38<00:00,  9.60s/it]\n",
      "2025-02-16 17:56:04,408 - LocalStorage - INFO - Saved Excel file with sheets: ['Analysis Results', 'Summary']\n",
      "2025-02-16 17:56:04,408 - LocalStorage - INFO - Saved Excel file with sheets: ['Analysis Results', 'Summary']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Row completed\n",
      "2025-02-16 17:56:04,408 - FileUtils.core.file_utils - INFO - Data saved successfully: {'results_20250216_175604': '/Users/topi/data-science/repos/text-analyzer/data/processed/results_20250216_175604.xlsx'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-16 17:56:04,408 - FileUtils.core.file_utils - INFO - Data saved successfully: {'results_20250216_175604': '/Users/topi/data-science/repos/text-analyzer/data/processed/results_20250216_175604.xlsx'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Excel analysis completed successfully\n",
      "Results saved to: results.xlsx\n",
      "\n",
      "Analysis Results:\n",
      "                                          categories  \\\n",
      "0  kirjautumisongelmat (0.90): Tunnistautumis- ja...   \n",
      "1  järjestelmävirheet (0.90): Tekniset virheet ja...   \n",
      "2  järjestelmävirheet (0.90): Tekniset virheet ja...   \n",
      "3  järjestelmävirheet (0.90): Tekniset virheet ja...   \n",
      "\n",
      "                                            keywords  \\\n",
      "0  ongelma (0.90) [tekninen tuki]; kirjautua (0.8...   \n",
      "1  raporttien vientiominaisuus (0.90) [teknologia...   \n",
      "2  virhekoodin e1234 (0.90) [teknologia]; synkron...   \n",
      "3  api-dokumentaatioon (0.90) [teknologia]; kehit...   \n",
      "\n",
      "                                              themes  \\\n",
      "0  Tekninen ongelma (0.90): Tekstissä kuvataan on...   \n",
      "1  Tekninen ongelma (0.90): Raporttien vientiomin...   \n",
      "2  Virhekoodin ongelmat (0.90): Tekstissä mainita...   \n",
      "3  Access Issues (0.95): The text highlights a pr...   \n",
      "\n",
      "                                             content  \\\n",
      "0  Minulla on ongelmia kirjautua hallintapaneelii...   \n",
      "1  Raporttien vientiominaisuus ei toimi. Kun klik...   \n",
      "2  Saan virhekoodin E1234 yrittäessäni synkronoid...   \n",
      "3  En pääse käsiksi API-dokumentaatioon. Kehittäj...   \n",
      "\n",
      "          analysis_timestamp  processing_time language  \n",
      "0 2025-02-16 17:56:04.382995        38.441041       fi  \n",
      "1 2025-02-16 17:56:04.382995        38.441041       fi  \n",
      "2 2025-02-16 17:56:04.382995        38.441041       fi  \n",
      "3 2025-02-16 17:56:04.382995        38.441041       fi  \n"
     ]
    }
   ],
   "source": [
    "# Configure output formatting\n",
    "output_config = ExcelOutputConfig(\n",
    "    output_detail=OutputDetail.MINIMAL,\n",
    "    include_metadata=True,\n",
    "    include_confidence_scores=True\n",
    ")\n",
    "\n",
    "# Analyze Excel file\n",
    "result_df = await analyzer.analyze_excel(\n",
    "    content_file=content_file,\n",
    "    analysis_types=[\"keywords\", \"themes\", \"categories\"],\n",
    "    save_results=True,\n",
    "    output_file=\"results.xlsx\",\n",
    "    output_config=output_config\n",
    ")\n",
    "\n",
    "print(\"\\nExcel analysis completed successfully\")\n",
    "print(f\"Results saved to: results.xlsx\")\n",
    "print(\"\\nAnalysis Results:\")\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "semantic-analyzer"
  },
  "kernelspec": {
   "display_name": "semantic-analyzer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
