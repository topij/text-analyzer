{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Analysis Pipeline\n",
    "\n",
    "This notebook demonstrates the semantic text analysis capabilities using our custom analyzers.\n",
    "\n",
    "## Setup\n",
    "Import required packages and configure the environment:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At start of notebook\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = str(Path().resolve().parent)\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-17 18:22:44,975 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:192] - Initialized FileUtils with log level: INFO\n",
      "2024-11-17 18:22:44,977 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:198] - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n",
      "2024-11-17 18:22:44,983 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:192] - Initialized FileUtils with log level: INFO\n",
      "2024-11-17 18:22:44,984 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:198] - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Check Results:\n",
      "==================================================\n",
      "\n",
      "Basic Setup:\n",
      "-----------\n",
      "✓ Project root in path\n",
      "✓ FileUtils initialized\n",
      "✓ .env file loaded\n",
      "\n",
      "Environment Variables:\n",
      "---------------------\n",
      "✓ OPENAI_API_KEY set\n",
      "✓ ANTHROPIC_API_KEY set\n",
      "\n",
      "Project Structure:\n",
      "-----------------\n",
      "✓ Raw data exists\n",
      "✓ Processed data exists\n",
      "✓ Configuration exists\n",
      "✓ Main config.yaml exists\n",
      "\n",
      "==================================================\n",
      "Environment Status: Ready ✓\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import logging\n",
    "# from src.nb_helpers.logging import configure_logging\n",
    "\n",
    "# Set up environment with DEBUG level\n",
    "from src.nb_helpers.environment import setup_notebook_env, verify_environment\n",
    "setup_notebook_env(log_level=\"DEBUG\")\n",
    "\n",
    "# Any verification needed will maintain DEBUG level\n",
    "verify_environment(log_level=\"DEBUG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary components\n",
    "from src.loaders.parameter_handler import ParameterHandler\n",
    "from src.nb_helpers.analyzers import (\n",
    "    analyze_keywords,\n",
    "    analyze_themes,\n",
    "    analyze_categories,\n",
    "    analyze_text,\n",
    "    AnalysisOptions\n",
    ")\n",
    "\n",
    "from scripts.migrate_parameters import create_example_parameters\n",
    "from src.nb_helpers.logging import configure_logging, verify_logging_setup_with_hierarchy, reset_debug_logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-17 18:22:45,115 - src.nb_helpers.logging - DEBUG - Logging configured at DEBUG level\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logging Configuration:\n",
      "--------------------------------------------------\n",
      "\n",
      "Logger: root\n",
      "Set Level: DEBUG\n",
      "Effective Level: DEBUG\n",
      "Propagates to root: True\n",
      "Handlers:\n",
      "  Handler 1 level: DEBUG\n",
      "\n",
      "Logger: src.nb_helpers.analyzers\n",
      "Hierarchy:\n",
      "  src: NOTSET\n",
      "  src.nb_helpers: NOTSET\n",
      "  src.nb_helpers.analyzers: DEBUG\n",
      "Set Level: DEBUG\n",
      "Effective Level: DEBUG\n",
      "Propagates to root: True\n",
      "No handlers (uses root handlers)\n",
      "\n",
      "Logger: src.analyzers.keyword_analyzer\n",
      "Hierarchy:\n",
      "  src: NOTSET\n",
      "  src.analyzers: NOTSET\n",
      "  src.analyzers.keyword_analyzer: DEBUG\n",
      "Set Level: DEBUG\n",
      "Effective Level: DEBUG\n",
      "Propagates to root: True\n",
      "No handlers (uses root handlers)\n",
      "\n",
      "Logger: src.analyzers.theme_analyzer\n",
      "Hierarchy:\n",
      "  src: NOTSET\n",
      "  src.analyzers: NOTSET\n",
      "  src.analyzers.theme_analyzer: DEBUG\n",
      "Set Level: DEBUG\n",
      "Effective Level: DEBUG\n",
      "Propagates to root: True\n",
      "No handlers (uses root handlers)\n",
      "\n",
      "Logger: src.analyzers.category_analyzer\n",
      "Hierarchy:\n",
      "  src: NOTSET\n",
      "  src.analyzers: NOTSET\n",
      "  src.analyzers.category_analyzer: DEBUG\n",
      "Set Level: DEBUG\n",
      "Effective Level: DEBUG\n",
      "Propagates to root: True\n",
      "No handlers (uses root handlers)\n",
      "\n",
      "Logger: src.utils.FileUtils.file_utils\n",
      "Hierarchy:\n",
      "  src: NOTSET\n",
      "  src.utils: NOTSET\n",
      "  src.utils.FileUtils: NOTSET\n",
      "  src.utils.FileUtils.file_utils: DEBUG\n",
      "Set Level: DEBUG\n",
      "Effective Level: DEBUG\n",
      "Propagates to root: True\n",
      "No handlers (uses root handlers)\n",
      "\n",
      "Logger: httpx\n",
      "Hierarchy:\n",
      "  httpx: INFO\n",
      "Set Level: INFO\n",
      "Effective Level: INFO\n",
      "Propagates to root: True\n",
      "No handlers (uses root handlers)\n"
     ]
    }
   ],
   "source": [
    "# Set initial logging\n",
    "configure_logging(level=\"DEBUG\")\n",
    "# Keep HTTP loggers at INFO\n",
    "for name in [\"httpx\", \"httpcore\", \"openai\", \"anthropic\"]:\n",
    "    logging.getLogger(name).setLevel(logging.INFO)\n",
    "    \n",
    "verify_logging_setup_with_hierarchy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Setup environment first\n",
    "# from src.nb_helpers.environment import setup_notebook_env\n",
    "# setup_notebook_env()\n",
    "\n",
    "# # Configure logging levels\n",
    "# import logging\n",
    "# from src.nb_helpers.logging import configure_logging, verify_logging_setup\n",
    "\n",
    "# # Set root logger to DEBUG\n",
    "# root = logging.getLogger()\n",
    "# root.setLevel(logging.DEBUG)\n",
    "# for handler in root.handlers:\n",
    "#     handler.setLevel(logging.DEBUG)\n",
    "\n",
    "# # Set module loggers to DEBUG\n",
    "# for name in [\"src.nb_helpers.analyzers\", \"src.analyzers.keyword_analyzer\", \n",
    "#             \"src.analyzers.theme_analyzer\", \"src.analyzers.category_analyzer\", \n",
    "#             \"src.utils.FileUtils.file_utils\"]:\n",
    "#     logging.getLogger(name).setLevel(logging.DEBUG)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detailed_logging_info = True\n",
    "# if detailed_logging_info:\n",
    "#     from src.nb_helpers.logging import verify_logging_setup_with_hierarchy\n",
    "#     # Configure logging\n",
    "#     # configure_logging(level=\"DEBUG\")\n",
    "#     # Verify with detailed information\n",
    "#     verify_logging_setup_with_hierarchy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-17 18:22:45,235 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:192] - Initialized FileUtils with log level: INFO\n",
      "2024-11-17 18:22:45,235 - src.utils.FileUtils.file_utils - DEBUG - Initialized FileUtils with log level: INFO\n",
      "2024-11-17 18:22:45,241 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:198] - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n",
      "2024-11-17 18:22:45,241 - src.utils.FileUtils.file_utils - DEBUG - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n",
      "2024-11-17 18:22:46,810 - src.utils.FileUtils.file_utils - INFO [file_utils.py:717] - Data saved to c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\parameters\\parameters_en.xlsx\n",
      "2024-11-17 18:22:46,810 - src.utils.FileUtils.file_utils - INFO - Data saved to c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\parameters\\parameters_en.xlsx\n",
      "2024-11-17 18:22:46,813 - scripts.migrate_parameters - INFO - Created example parameter file: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\parameters\\parameters_en.xlsx\n",
      "2024-11-17 18:22:46,820 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:192] - Initialized FileUtils with log level: INFO\n",
      "2024-11-17 18:22:46,820 - src.utils.FileUtils.file_utils - DEBUG - Initialized FileUtils with log level: INFO\n",
      "2024-11-17 18:22:46,823 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:198] - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n",
      "2024-11-17 18:22:46,823 - src.utils.FileUtils.file_utils - DEBUG - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n",
      "2024-11-17 18:22:46,830 - src.loaders.parameter_handler - DEBUG - Using parameter file: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\parameters\\parameters_en.xlsx\n",
      "2024-11-17 18:22:46,831 - src.loaders.parameter_handler - DEBUG - Loading Excel file from: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\parameters\\parameters_en.xlsx\n",
      "2024-11-17 18:22:46,892 - src.utils.FileUtils.file_utils - INFO [file_utils.py:1018] - Successfully loaded 5 sheets from c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\parameters\\parameters_en.xlsx\n",
      "2024-11-17 18:22:46,892 - src.utils.FileUtils.file_utils - INFO - Successfully loaded 5 sheets from c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\parameters\\parameters_en.xlsx\n",
      "2024-11-17 18:22:46,894 - src.loaders.parameter_handler - DEBUG - Found sheets: ['General Parameters', 'Categories', 'Predefined Keywords', 'Excluded Keywords', 'Analysis Settings']\n",
      "2024-11-17 18:22:46,897 - src.loaders.parameter_handler - DEBUG - Looking for general parameters in sheet: General Parameters\n",
      "2024-11-17 18:22:46,898 - src.loaders.parameter_handler - DEBUG - General sheet columns: ['parameter', 'value', 'description']\n",
      "2024-11-17 18:22:46,906 - src.loaders.parameter_handler - DEBUG - General sheet content:\n",
      "                parameter                           value  \\\n",
      "0            max_keywords                              10   \n",
      "1                focus_on  technical and business content   \n",
      "2  column_name_to_analyze                         content   \n",
      "3      min_keyword_length                               3   \n",
      "4       include_compounds                            True   \n",
      "5                language                              en   \n",
      "\n",
      "                          description  \n",
      "0  Maximum keywords to extract (1-20)  \n",
      "1                 Analysis focus area  \n",
      "2          Name of the content column  \n",
      "3              Minimum keyword length  \n",
      "4              Include compound words  \n",
      "5                    Content language  \n",
      "2024-11-17 18:22:46,907 - src.loaders.parameter_handler - DEBUG - Starting with default general parameters: {'max_keywords': 10, 'min_keyword_length': 3, 'language': 'en', 'focus_on': 'general content analysis', 'include_compounds': True, 'max_themes': 3, 'min_confidence': 0.3, 'column_name_to_analyze': 'text'}\n",
      "2024-11-17 18:22:46,908 - src.loaders.parameter_handler - DEBUG - Found parameter and value columns\n",
      "2024-11-17 18:22:46,914 - src.loaders.parameter_handler - DEBUG - Full dataframe:\n",
      "                parameter                           value  \\\n",
      "0            max_keywords                              10   \n",
      "1                focus_on  technical and business content   \n",
      "2  column_name_to_analyze                         content   \n",
      "3      min_keyword_length                               3   \n",
      "4       include_compounds                            True   \n",
      "5                language                              en   \n",
      "\n",
      "                          description  \n",
      "0  Maximum keywords to extract (1-20)  \n",
      "1                 Analysis focus area  \n",
      "2          Name of the content column  \n",
      "3              Minimum keyword length  \n",
      "4              Include compound words  \n",
      "5                    Content language  \n",
      "2024-11-17 18:22:46,916 - src.loaders.parameter_handler - DEBUG - Setting max_keywords = 10\n",
      "2024-11-17 18:22:46,918 - src.loaders.parameter_handler - DEBUG - Setting focus_on = technical and business content\n",
      "2024-11-17 18:22:46,920 - src.loaders.parameter_handler - DEBUG - Setting column_name_to_analyze = content\n",
      "2024-11-17 18:22:46,923 - src.loaders.parameter_handler - DEBUG - Setting min_keyword_length = 3\n",
      "2024-11-17 18:22:46,924 - src.loaders.parameter_handler - DEBUG - Setting include_compounds = True\n",
      "2024-11-17 18:22:46,926 - src.loaders.parameter_handler - DEBUG - Setting language = en\n",
      "2024-11-17 18:22:46,928 - src.loaders.parameter_handler - DEBUG - Final general parameters: {'max_keywords': 10, 'min_keyword_length': 3, 'language': 'en', 'focus_on': 'technical and business content', 'include_compounds': True, 'max_themes': 3, 'min_confidence': 0.3, 'column_name_to_analyze': 'content'}\n",
      "2024-11-17 18:22:46,931 - src.loaders.parameter_handler - DEBUG - Parsed general parameters: {'max_keywords': 10, 'min_keyword_length': 3, 'language': 'en', 'focus_on': 'technical and business content', 'include_compounds': True, 'max_themes': 3, 'min_confidence': 0.3, 'column_name_to_analyze': 'content'}\n",
      "2024-11-17 18:22:46,936 - src.loaders.parameter_handler - DEBUG - Created parameter set: {'general': {'max_keywords': 10, 'min_keyword_length': 3, 'language': 'en', 'focus_on': 'technical and business content', 'include_compounds': True, 'max_themes': 3, 'min_confidence': 0.3, 'column_name_to_analyze': 'content'}, 'categories': {'technical_content': {'description': 'Technical and software development content', 'keywords': ['software', 'development', 'api', 'programming', 'technical', 'code'], 'threshold': 0.6, 'parent': None}, 'business_content': {'description': 'Business and financial content', 'keywords': ['revenue', 'sales', 'market', 'growth', 'financial', 'business'], 'threshold': 0.6, 'parent': None}, 'educational_content': {'description': 'Educational and training content', 'keywords': ['learning', 'education', 'training', 'teaching', 'skills'], 'threshold': 0.5, 'parent': None}}, 'predefined_keywords': {'machine learning': {'importance': 1.0, 'domain': 'technical', 'compound_parts': []}, 'cloud computing': {'importance': 0.9, 'domain': 'technical', 'compound_parts': []}, 'revenue growth': {'importance': 0.9, 'domain': 'business', 'compound_parts': []}}, 'excluded_keywords': {'the', 'for', 'and'}, 'analysis_settings': {'theme_analysis': {'enabled': True, 'min_confidence': 0.5}, 'weights': {'statistical': 0.4, 'llm': 0.6}}, 'domain_context': {}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logging Configuration:\n",
      "--------------------------------------------------\n",
      "\n",
      "Logger: root\n",
      "Set Level: DEBUG\n",
      "Effective Level: DEBUG\n",
      "Propagates to root: True\n",
      "Handlers:\n",
      "  Handler 1 level: DEBUG\n",
      "\n",
      "Logger: src.nb_helpers.analyzers\n",
      "Hierarchy:\n",
      "  src: NOTSET\n",
      "  src.nb_helpers: NOTSET\n",
      "  src.nb_helpers.analyzers: DEBUG\n",
      "Set Level: DEBUG\n",
      "Effective Level: DEBUG\n",
      "Propagates to root: True\n",
      "No handlers (uses root handlers)\n",
      "\n",
      "Logger: src.analyzers.keyword_analyzer\n",
      "Hierarchy:\n",
      "  src: NOTSET\n",
      "  src.analyzers: NOTSET\n",
      "  src.analyzers.keyword_analyzer: DEBUG\n",
      "Set Level: DEBUG\n",
      "Effective Level: DEBUG\n",
      "Propagates to root: True\n",
      "No handlers (uses root handlers)\n",
      "\n",
      "Logger: src.analyzers.theme_analyzer\n",
      "Hierarchy:\n",
      "  src: NOTSET\n",
      "  src.analyzers: NOTSET\n",
      "  src.analyzers.theme_analyzer: DEBUG\n",
      "Set Level: DEBUG\n",
      "Effective Level: DEBUG\n",
      "Propagates to root: True\n",
      "No handlers (uses root handlers)\n",
      "\n",
      "Logger: src.analyzers.category_analyzer\n",
      "Hierarchy:\n",
      "  src: NOTSET\n",
      "  src.analyzers: NOTSET\n",
      "  src.analyzers.category_analyzer: DEBUG\n",
      "Set Level: DEBUG\n",
      "Effective Level: DEBUG\n",
      "Propagates to root: True\n",
      "No handlers (uses root handlers)\n",
      "\n",
      "Logger: src.utils.FileUtils.file_utils\n",
      "Hierarchy:\n",
      "  src: NOTSET\n",
      "  src.utils: NOTSET\n",
      "  src.utils.FileUtils: NOTSET\n",
      "  src.utils.FileUtils.file_utils: DEBUG\n",
      "Set Level: DEBUG\n",
      "Effective Level: DEBUG\n",
      "Propagates to root: True\n",
      "Handlers:\n",
      "  Handler 1 level: DEBUG\n",
      "\n",
      "Logger: httpx\n",
      "Hierarchy:\n",
      "  httpx: INFO\n",
      "Set Level: INFO\n",
      "Effective Level: INFO\n",
      "Propagates to root: True\n",
      "No handlers (uses root handlers)\n"
     ]
    }
   ],
   "source": [
    "# At the start\n",
    "\n",
    "\n",
    "\n",
    "# Create parameters\n",
    "params_file_name = \"parameters_en.xlsx\"\n",
    "params_file = create_example_parameters(params_file_name)\n",
    "\n",
    "# Reset logging levels if needed\n",
    "reset_debug_logging()\n",
    "verify_logging_setup_with_hierarchy()\n",
    "\n",
    "# Continue with parameter handling\n",
    "handler = ParameterHandler(params_file_name)\n",
    "params = handler.get_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example texts in different languages\n",
    "example_texts = {\n",
    "    \"English Technical\": \"\"\"\n",
    "        The cloud migration project improved system scalability while reducing costs.\n",
    "        New DevOps practices streamlined the deployment pipeline significantly.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Finnish Technical\": \"\"\"\n",
    "        Pilvipalveluihin siirtyminen paransi järjestelmän skaalautuvuutta ja vähensi kustannuksia.\n",
    "        Uudet DevOps-käytännöt tehostivat merkittävästi käyttöönottoprosessia.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"English Business\": \"\"\"\n",
    "        Q3 financial results show 15% revenue growth and improved profit margins.\n",
    "        Customer acquisition costs decreased while retention rates increased.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Finnish Business\": \"\"\"\n",
    "        Q3 taloudelliset tulokset osoittavat 15% liikevaihdon kasvun ja parantuneet katteet.\n",
    "        Asiakashankinnan kustannukset laskivat ja asiakaspysyvyys parani.\n",
    "    \"\"\"\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-17 18:22:46,999 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:192] - Initialized FileUtils with log level: INFO\n",
      "2024-11-17 18:22:46,999 - src.utils.FileUtils.file_utils - DEBUG - Initialized FileUtils with log level: INFO\n",
      "2024-11-17 18:22:47,001 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:198] - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n",
      "2024-11-17 18:22:47,001 - src.utils.FileUtils.file_utils - DEBUG - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n",
      "2024-11-17 18:22:47,154 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:192] - Initialized FileUtils with log level: INFO\n",
      "2024-11-17 18:22:47,154 - src.utils.FileUtils.file_utils - DEBUG - Initialized FileUtils with log level: INFO\n",
      "2024-11-17 18:22:47,156 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:198] - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n",
      "2024-11-17 18:22:47,156 - src.utils.FileUtils.file_utils - DEBUG - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n",
      "2024-11-17 18:22:47,162 - src.loaders.parameter_handler - DEBUG - Using parameter file: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\parameters\\parameters_en.xlsx\n",
      "2024-11-17 18:22:47,164 - src.loaders.parameter_handler - DEBUG - Loading Excel file from: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\parameters\\parameters_en.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing parameter file at: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\parameters\\parameters_en.xlsx\n",
      "\n",
      "Parameter File Verification:\n",
      "Absolute path: C:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\parameters\\parameters_en.xlsx\n",
      "File exists: True\n",
      "\n",
      "Found sheets:\n",
      "\n",
      "General Parameters:\n",
      "                parameter                           value  \\\n",
      "0            max_keywords                              10   \n",
      "1                focus_on  technical and business content   \n",
      "2  column_name_to_analyze                         content   \n",
      "3      min_keyword_length                               3   \n",
      "4       include_compounds                            True   \n",
      "\n",
      "                          description  \n",
      "0  Maximum keywords to extract (1-20)  \n",
      "1                 Analysis focus area  \n",
      "2          Name of the content column  \n",
      "3              Minimum keyword length  \n",
      "4              Include compound words  \n",
      "\n",
      "Categories:\n",
      "              category                                 description  \\\n",
      "0    technical_content  Technical and software development content   \n",
      "1     business_content              Business and financial content   \n",
      "2  educational_content            Educational and training content   \n",
      "\n",
      "                                            keywords  threshold  parent  \n",
      "0  software,development,api,programming,technical...        0.6     NaN  \n",
      "1     revenue,sales,market,growth,financial,business        0.6     NaN  \n",
      "2        learning,education,training,teaching,skills        0.5     NaN  \n",
      "\n",
      "Predefined Keywords:\n",
      "            keyword  importance     domain\n",
      "0  machine learning         1.0  technical\n",
      "1   cloud computing         0.9  technical\n",
      "2    revenue growth         0.9   business\n",
      "\n",
      "Excluded Keywords:\n",
      "  keyword       reason\n",
      "0     the  Common word\n",
      "1     and  Common word\n",
      "2     for  Common word\n",
      "\n",
      "Analysis Settings:\n",
      "                         setting  value  \\\n",
      "0  theme_analysis.min_confidence    0.5   \n",
      "1            weights.statistical    0.4   \n",
      "2                    weights.llm    0.6   \n",
      "\n",
      "                              description  \n",
      "0  Minimum confidence for theme detection  \n",
      "1         Weight for statistical analysis  \n",
      "2                 Weight for LLM analysis  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-17 18:22:47,209 - src.utils.FileUtils.file_utils - INFO [file_utils.py:1018] - Successfully loaded 5 sheets from c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\parameters\\parameters_en.xlsx\n",
      "2024-11-17 18:22:47,209 - src.utils.FileUtils.file_utils - INFO - Successfully loaded 5 sheets from c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\parameters\\parameters_en.xlsx\n",
      "2024-11-17 18:22:47,213 - src.loaders.parameter_handler - DEBUG - Found sheets: ['General Parameters', 'Categories', 'Predefined Keywords', 'Excluded Keywords', 'Analysis Settings']\n",
      "2024-11-17 18:22:47,215 - src.loaders.parameter_handler - DEBUG - Looking for general parameters in sheet: General Parameters\n",
      "2024-11-17 18:22:47,209 - src.utils.FileUtils.file_utils - INFO - Successfully loaded 5 sheets from c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\parameters\\parameters_en.xlsx\n",
      "2024-11-17 18:22:47,213 - src.loaders.parameter_handler - DEBUG - Found sheets: ['General Parameters', 'Categories', 'Predefined Keywords', 'Excluded Keywords', 'Analysis Settings']\n",
      "2024-11-17 18:22:47,215 - src.loaders.parameter_handler - DEBUG - Looking for general parameters in sheet: General Parameters\n",
      "2024-11-17 18:22:47,217 - src.loaders.parameter_handler - DEBUG - General sheet columns: ['parameter', 'value', 'description']\n",
      "2024-11-17 18:22:47,223 - src.loaders.parameter_handler - DEBUG - General sheet content:\n",
      "                parameter                           value  \\\n",
      "0            max_keywords                              10   \n",
      "1                focus_on  technical and business content   \n",
      "2  column_name_to_analyze                         content   \n",
      "3      min_keyword_length                               3   \n",
      "4       include_compounds                            True   \n",
      "5                language                              en   \n",
      "\n",
      "                          description  \n",
      "0  Maximum keywords to extract (1-20)  \n",
      "1                 Analysis focus area  \n",
      "2          Name of the content column  \n",
      "3              Minimum keyword length  \n",
      "4              Include compound words  \n",
      "5                    Content language  \n",
      "2024-11-17 18:22:47,224 - src.loaders.parameter_handler - DEBUG - Starting with default general parameters: {'max_keywords': 10, 'min_keyword_length': 3, 'language': 'en', 'focus_on': 'general content analysis', 'include_compounds': True, 'max_themes': 3, 'min_confidence': 0.3, 'column_name_to_analyze': 'text'}\n",
      "2024-11-17 18:22:47,225 - src.loaders.parameter_handler - DEBUG - Found parameter and value columns\n",
      "2024-11-17 18:22:47,231 - src.loaders.parameter_handler - DEBUG - Full dataframe:\n",
      "                parameter                           value  \\\n",
      "0            max_keywords                              10   \n",
      "1                focus_on  technical and business content   \n",
      "2  column_name_to_analyze                         content   \n",
      "3      min_keyword_length                               3   \n",
      "4       include_compounds                            True   \n",
      "5                language                              en   \n",
      "\n",
      "                          description  \n",
      "0  Maximum keywords to extract (1-20)  \n",
      "1                 Analysis focus area  \n",
      "2          Name of the content column  \n",
      "3              Minimum keyword length  \n",
      "4              Include compound words  \n",
      "5                    Content language  \n",
      "2024-11-17 18:22:47,232 - src.loaders.parameter_handler - DEBUG - Setting max_keywords = 10\n",
      "2024-11-17 18:22:47,233 - src.loaders.parameter_handler - DEBUG - Setting focus_on = technical and business content\n",
      "2024-11-17 18:22:47,235 - src.loaders.parameter_handler - DEBUG - Setting column_name_to_analyze = content\n",
      "2024-11-17 18:22:47,237 - src.loaders.parameter_handler - DEBUG - Setting min_keyword_length = 3\n",
      "2024-11-17 18:22:47,240 - src.loaders.parameter_handler - DEBUG - Setting include_compounds = True\n",
      "2024-11-17 18:22:47,243 - src.loaders.parameter_handler - DEBUG - Setting language = en\n",
      "2024-11-17 18:22:47,247 - src.loaders.parameter_handler - DEBUG - Final general parameters: {'max_keywords': 10, 'min_keyword_length': 3, 'language': 'en', 'focus_on': 'technical and business content', 'include_compounds': True, 'max_themes': 3, 'min_confidence': 0.3, 'column_name_to_analyze': 'content'}\n",
      "2024-11-17 18:22:47,249 - src.loaders.parameter_handler - DEBUG - Parsed general parameters: {'max_keywords': 10, 'min_keyword_length': 3, 'language': 'en', 'focus_on': 'technical and business content', 'include_compounds': True, 'max_themes': 3, 'min_confidence': 0.3, 'column_name_to_analyze': 'content'}\n",
      "2024-11-17 18:22:47,254 - src.loaders.parameter_handler - DEBUG - Created parameter set: {'general': {'max_keywords': 10, 'min_keyword_length': 3, 'language': 'en', 'focus_on': 'technical and business content', 'include_compounds': True, 'max_themes': 3, 'min_confidence': 0.3, 'column_name_to_analyze': 'content'}, 'categories': {'technical_content': {'description': 'Technical and software development content', 'keywords': ['software', 'development', 'api', 'programming', 'technical', 'code'], 'threshold': 0.6, 'parent': None}, 'business_content': {'description': 'Business and financial content', 'keywords': ['revenue', 'sales', 'market', 'growth', 'financial', 'business'], 'threshold': 0.6, 'parent': None}, 'educational_content': {'description': 'Educational and training content', 'keywords': ['learning', 'education', 'training', 'teaching', 'skills'], 'threshold': 0.5, 'parent': None}}, 'predefined_keywords': {'machine learning': {'importance': 1.0, 'domain': 'technical', 'compound_parts': []}, 'cloud computing': {'importance': 0.9, 'domain': 'technical', 'compound_parts': []}, 'revenue growth': {'importance': 0.9, 'domain': 'business', 'compound_parts': []}}, 'excluded_keywords': {'the', 'for', 'and'}, 'analysis_settings': {'theme_analysis': {'enabled': True, 'min_confidence': 0.5}, 'weights': {'statistical': 0.4, 'llm': 0.6}}, 'domain_context': {}}\n"
     ]
    }
   ],
   "source": [
    "from src.loaders.parameter_handler import ParameterHandler, get_parameter_file_path, verify_parameter_file\n",
    "from scripts.migrate_parameters import create_example_parameters\n",
    "\n",
    "# Create and load parameters\n",
    "params_file_name = \"parameters_en.xlsx\"\n",
    "\n",
    "# Get the full parameter file path\n",
    "params_file = get_parameter_file_path(params_file_name)\n",
    "\n",
    "# Create file if it doesn't exist\n",
    "if not params_file.exists():\n",
    "    params_file = create_example_parameters(params_file_name)\n",
    "    print(f\"Created parameter file at: {params_file}\")\n",
    "else:\n",
    "    print(f\"Using existing parameter file at: {params_file}\")\n",
    "\n",
    "# Verify the file\n",
    "verify_parameter_file(params_file)\n",
    "\n",
    "# Load parameters\n",
    "handler = ParameterHandler(params_file_name)  # Can now use just the file name\n",
    "params = handler.get_parameters()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded parameters:\n",
      "ParameterSet(\n",
      "  general=max_keywords=10 min_keyword_length=3 language='en' focus_on='technical and business content' include_compounds=True max_themes=3 min_confidence=0.3 column_name_to_analyze='content',\n",
      "  categories=3 items,\n",
      "  predefined_keywords=3 items,\n",
      "  excluded_keywords=3 items,\n",
      "  analysis_settings=theme_analysis=ThemeAnalysisSettings(enabled=True, min_confidence=0.5) weights=AnalysisWeights(statistical=0.4, llm=0.6),\n",
      "  domain_context=0 items\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLoaded parameters:\")\n",
    "# params.print()  # Uses the new print method\n",
    "\n",
    "# Or just\n",
    "print(params)  # Uses the new __str__ method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not creating new example files.\n"
     ]
    }
   ],
   "source": [
    "# Create example files\n",
    "\n",
    "create_new_exampo_files=False\n",
    "\n",
    "if create_new_exampo_files:\n",
    "    en_params = create_example_parameters(\"parameters_en.xlsx\", \"en\")\n",
    "    fi_params = create_example_parameters(\"parameters_fi.xlsx\", \"fi\")\n",
    "    print(f\"Created parameter files:\\n- {en_params}\\n- {fi_params}\")\n",
    "    print(f\"Created parameter files:\\n- {en_params}\\n- {fi_params}\")\n",
    "else:\n",
    "    print(\"Not creating new example files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter Validation:\n",
      "--------------------------------------------------\n",
      "\n",
      "Parameters validated successfully!\n"
     ]
    }
   ],
   "source": [
    "# Check parameter validation\n",
    "print(\"Parameter Validation:\")\n",
    "print(\"-\" * 50)\n",
    "is_valid, warnings, errors = handler.validate()\n",
    "if warnings:\n",
    "    print(\"\\nWarnings:\")\n",
    "    for warning in warnings:\n",
    "        print(f\"- {warning}\")\n",
    "if not is_valid:\n",
    "    print(\"\\nErrors:\")\n",
    "    for error in errors:\n",
    "        print(f\"- {error}\")\n",
    "else:\n",
    "    print(\"\\nParameters validated successfully!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-17 18:22:47,375 - src.nb_helpers.analyzers - DEBUG - Starting complete text analysis\n",
      "2024-11-17 18:22:47,376 - src.nb_helpers.analyzers - DEBUG - Running keywords analysis\n",
      "2024-11-17 18:22:47,379 - src.nb_helpers.analyzers - DEBUG - Starting keyword analysis\n",
      "2024-11-17 18:22:47,382 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:192] - Initialized FileUtils with log level: INFO\n",
      "2024-11-17 18:22:47,382 - src.utils.FileUtils.file_utils - DEBUG - Initialized FileUtils with log level: INFO\n",
      "2024-11-17 18:22:47,385 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:198] - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n",
      "2024-11-17 18:22:47,385 - src.utils.FileUtils.file_utils - DEBUG - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n",
      "2024-11-17 18:22:47,392 - src.nb_helpers.analyzers - DEBUG - Initialized TextAnalyzer with options: AnalysisOptions(show_confidence=True, show_evidence=True, show_keywords=True, show_raw_data=True, debug_mode=True, language='en', parameter_file=None)\n",
      "2024-11-17 18:22:47,393 - src.nb_helpers.analyzers - DEBUG - Starting Keyword analysis in en\n",
      "2024-11-17 18:22:47,394 - src.nb_helpers.analyzers - DEBUG - Parameter file: None\n",
      "2024-11-17 18:22:47,395 - src.loaders.parameter_handler - DEBUG - File path does not exist: None\n",
      "2024-11-17 18:22:47,397 - src.nb_helpers.analyzers - DEBUG - Creating keyword tester with parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Single Language Analysis:\n",
      "--------------------------------------------------\n",
      "\n",
      "Keyword Analysis\n",
      "==================================================\n",
      "\n",
      "Detected Language: en\n",
      "\n",
      "Input Text:\n",
      "--------------------\n",
      "The cloud migration project improved system scalability while reducing costs.\n",
      "        New DevOps practices streamlined the deployment pipeline significantly.\n",
      "\n",
      "Analyzing...\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-17 18:22:48,912 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:192] - Initialized FileUtils with log level: INFO\n",
      "2024-11-17 18:22:48,912 - src.utils.FileUtils.file_utils - DEBUG - Initialized FileUtils with log level: INFO\n",
      "2024-11-17 18:22:48,914 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:198] - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n",
      "2024-11-17 18:22:48,914 - src.utils.FileUtils.file_utils - DEBUG - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n",
      "2024-11-17 18:22:48,921 - src.core.language_processing.factory - DEBUG - Using default configuration\n",
      "2024-11-17 18:22:48,922 - src.core.language_processing.factory - DEBUG - Creating text processor for language: None\n",
      "2024-11-17 18:22:48,924 - src.core.language_processing.factory - WARNING - Unsupported language: None, defaulting to English\n",
      "2024-11-17 18:22:48,925 - src.core.language_processing.factory - DEBUG - Creating en processor\n",
      "2024-11-17 18:22:48,927 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:192] - Initialized FileUtils with log level: INFO\n",
      "2024-11-17 18:22:48,927 - src.utils.FileUtils.file_utils - DEBUG - Initialized FileUtils with log level: INFO\n",
      "2024-11-17 18:22:48,930 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:198] - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n",
      "2024-11-17 18:22:48,930 - src.utils.FileUtils.file_utils - DEBUG - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n",
      "2024-11-17 18:22:48,955 - src.core.language_processing.english - DEBUG - Loaded 179 NLTK stopwords\n",
      "2024-11-17 18:22:48,976 - src.core.language_processing.english - DEBUG - Loaded 733 additional stopwords from file\n",
      "2024-11-17 18:22:48,977 - src.core.language_processing.english - DEBUG - Total English stopwords: 831\n",
      "2024-11-17 18:22:48,978 - src.core.language_processing.english - DEBUG - Initialized English processor with 831 stopwords\n",
      "2024-11-17 18:22:48,984 - src.analyzers.keyword_analyzer - DEBUG - Initialized with weights: {'statistical': 0.4, 'llm': 0.6, 'compound_bonus': 0.2, 'domain_bonus': 0.15, 'length_factor': 0.1, 'generic_penalty': 0.3}\n",
      "2024-11-17 18:22:48,986 - src.analyzers.keyword_analyzer - DEBUG - Initialized with config: {'general': {'max_keywords': 10, 'min_keyword_length': 3, 'language': 'en', 'focus_on': 'general content analysis', 'include_compounds': True, 'max_themes': 3, 'min_confidence': 0.3, 'column_name_to_analyze': 'text'}, 'categories': {}, 'predefined_keywords': {}, 'excluded_keywords': set(), 'analysis_settings': {'theme_analysis': {'enabled': True, 'min_confidence': 0.5}, 'weights': {'statistical': 0.4, 'llm': 0.6}}, 'domain_context': {}}\n",
      "2024-11-17 18:22:48,990 - src.nb_helpers.analyzers - DEBUG - Running keyword analysis\n",
      "2024-11-17 18:22:48,991 - src.nb_helpers.testers - DEBUG - KeywordTester starting analysis\n",
      "2024-11-17 18:22:48,992 - src.analyzers.keyword_analyzer - DEBUG - Starting keyword analysis\n",
      "2024-11-17 18:22:48,993 - src.analyzers.keyword_analyzer - DEBUG - Analyzing text of length 171\n",
      "2024-11-17 18:22:48,995 - src.analyzers.keyword_analyzer - DEBUG - Starting internal analysis\n",
      "2024-11-17 18:23:10,012 - src.core.language_processing.english - DEBUG - Found compound word 'improve' with parts: ['imp', 'rove']\n",
      "2024-11-17 18:23:10,019 - src.core.language_processing.english - DEBUG - Found compound word 'streamline' with parts: ['stream', 'line']\n",
      "2024-11-17 18:23:10,025 - src.core.language_processing.english - DEBUG - Found compound word 'pipeline' with parts: ['pipe', 'line']\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mCancelledError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\tja\\AppData\\Local\\miniconda3\\envs\\semantic-analyzer\\lib\\site-packages\\langchain_core\\runnables\\base.py:3773\u001b[0m, in \u001b[0;36mRunnableParallel.ainvoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3772\u001b[0m steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps__)\n\u001b[1;32m-> 3773\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\n\u001b[0;32m   3774\u001b[0m     \u001b[38;5;241m*\u001b[39m(\n\u001b[0;32m   3775\u001b[0m         _ainvoke_step(\n\u001b[0;32m   3776\u001b[0m             step,\n\u001b[0;32m   3777\u001b[0m             \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   3778\u001b[0m             \u001b[38;5;66;03m# mark each step as a child run\u001b[39;00m\n\u001b[0;32m   3779\u001b[0m             config,\n\u001b[0;32m   3780\u001b[0m             key,\n\u001b[0;32m   3781\u001b[0m         )\n\u001b[0;32m   3782\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   3783\u001b[0m     )\n\u001b[0;32m   3784\u001b[0m )\n\u001b[0;32m   3785\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(steps, results))\n",
      "\u001b[1;31mCancelledError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Analyze English text\u001b[39;00m\n\u001b[0;32m     15\u001b[0m text \u001b[38;5;241m=\u001b[39m example_texts[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnglish Technical\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 16\u001b[0m results_en \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m analyze_text(text, options_en)\n",
      "File \u001b[1;32m~\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\src\\nb_helpers\\analyzers.py:173\u001b[0m, in \u001b[0;36manalyze_text\u001b[1;34m(text, options)\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m analyzer_type, analyze_func \u001b[38;5;129;01min\u001b[39;00m analyzers:\n\u001b[0;32m    172\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manalyzer_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m analysis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 173\u001b[0m     results[analyzer_type] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m analyze_func(text, options)\n\u001b[0;32m    174\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00manalyzer_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m analysis completed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\src\\nb_helpers\\analyzers.py:94\u001b[0m, in \u001b[0;36manalyze_keywords\u001b[1;34m(text, options)\u001b[0m\n\u001b[0;32m     91\u001b[0m tester \u001b[38;5;241m=\u001b[39m KeywordTester(config\u001b[38;5;241m=\u001b[39mparams\u001b[38;5;241m.\u001b[39mmodel_dump())\n\u001b[0;32m     93\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning keyword analysis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 94\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m tester\u001b[38;5;241m.\u001b[39manalyze(text)\n\u001b[0;32m     95\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKeyword analysis completed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     97\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFormatting results\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\src\\nb_helpers\\testers.py:50\u001b[0m, in \u001b[0;36mKeywordTester.analyze\u001b[1;34m(self, text, **kwargs)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21manalyze\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m     49\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKeywordTester starting analysis\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Add debug logging\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manalyzer\u001b[38;5;241m.\u001b[39manalyze(text)\n\u001b[0;32m     51\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKeywordTester analysis complete\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Add debug logging\u001b[39;00m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\src\\analyzers\\keyword_analyzer.py:253\u001b[0m, in \u001b[0;36mKeywordAnalyzer.analyze\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;66;03m# Get internal analysis results\u001b[39;00m\n\u001b[0;32m    252\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnalyzing text of length \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 253\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_analyze_internal(text)\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;66;03m# Convert to KeywordAnalysisResult\u001b[39;00m\n\u001b[0;32m    256\u001b[0m     result \u001b[38;5;241m=\u001b[39m KeywordAnalysisResult(\n\u001b[0;32m    257\u001b[0m         keywords\u001b[38;5;241m=\u001b[39m[k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mkeywords],\n\u001b[0;32m    258\u001b[0m         compound_words\u001b[38;5;241m=\u001b[39m[],  \u001b[38;5;66;03m# Fill if needed\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    261\u001b[0m         success\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    262\u001b[0m     )\n",
      "File \u001b[1;32m~\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\src\\analyzers\\keyword_analyzer.py:585\u001b[0m, in \u001b[0;36mKeywordAnalyzer._analyze_internal\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[38;5;66;03m# Get keywords with both methods\u001b[39;00m\n\u001b[0;32m    584\u001b[0m statistical_keywords \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_analyze_statistically(text, sections)\n\u001b[1;32m--> 585\u001b[0m llm_keywords \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_analyze_with_llm(text, statistical_keywords)\n\u001b[0;32m    587\u001b[0m \u001b[38;5;66;03m# Process compound words first\u001b[39;00m\n\u001b[0;32m    588\u001b[0m compound_words \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\src\\analyzers\\keyword_analyzer.py:367\u001b[0m, in \u001b[0;36mKeywordAnalyzer._analyze_with_llm\u001b[1;34m(self, text, statistical_keywords)\u001b[0m\n\u001b[0;32m    363\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[0;32m    365\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    366\u001b[0m     \u001b[38;5;66;03m# Get LLM response using the chain\u001b[39;00m\n\u001b[1;32m--> 367\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchain\u001b[38;5;241m.\u001b[39mainvoke(text)\n\u001b[0;32m    369\u001b[0m     \u001b[38;5;66;03m# Extract keywords from response\u001b[39;00m\n\u001b[0;32m    370\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeywords\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m response:\n",
      "File \u001b[1;32mc:\\Users\\tja\\AppData\\Local\\miniconda3\\envs\\semantic-analyzer\\lib\\site-packages\\langchain_core\\runnables\\base.py:3068\u001b[0m, in \u001b[0;36mRunnableSequence.ainvoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3066\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(part(), context\u001b[38;5;241m=\u001b[39mcontext)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   3067\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3068\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(part())\n\u001b[0;32m   3069\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   3070\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 2. Test single language analysis\n",
    "print(\"\\nSingle Language Analysis:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "options_en = AnalysisOptions(\n",
    "    show_confidence=True,\n",
    "    show_evidence=True,\n",
    "    show_keywords=True,\n",
    "    show_raw_data=True,\n",
    "    debug_mode=True,\n",
    "    language=\"en\"  # Explicitly set language\n",
    ")\n",
    "\n",
    "# Analyze English text\n",
    "text = example_texts[\"English Technical\"]\n",
    "results_en = await analyze_keywords(text, options_en)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-17 18:23:30,483 - src.nb_helpers.analyzers - DEBUG - Starting keyword analysis\n",
      "2024-11-17 18:23:30,487 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:192] - Initialized FileUtils with log level: INFO\n",
      "2024-11-17 18:23:30,487 - src.utils.FileUtils.file_utils - DEBUG - Initialized FileUtils with log level: INFO\n",
      "2024-11-17 18:23:30,489 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:198] - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n",
      "2024-11-17 18:23:30,489 - src.utils.FileUtils.file_utils - DEBUG - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n",
      "2024-11-17 18:23:30,495 - src.nb_helpers.analyzers - DEBUG - Initialized TextAnalyzer with options: AnalysisOptions(show_confidence=True, show_evidence=True, show_keywords=True, show_raw_data=True, debug_mode=True, language='fi', parameter_file=None)\n",
      "2024-11-17 18:23:30,497 - src.nb_helpers.analyzers - DEBUG - Starting Keyword analysis in fi\n",
      "2024-11-17 18:23:30,498 - src.nb_helpers.analyzers - DEBUG - Parameter file: None\n",
      "2024-11-17 18:23:30,499 - src.loaders.parameter_handler - DEBUG - File path does not exist: None\n",
      "2024-11-17 18:23:30,500 - src.nb_helpers.analyzers - DEBUG - Creating keyword tester with parameters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Finnish keyword analysis:\n",
      "--------------------------------------------------\n",
      "\n",
      "Keyword Analysis\n",
      "==================================================\n",
      "\n",
      "Detected Language: fi\n",
      "\n",
      "Input Text:\n",
      "--------------------\n",
      "Pilvipalveluihin siirtyminen paransi järjestelmän skaalautuvuutta ja vähensi kustannuksia.\n",
      "        Uudet DevOps-käytännöt tehostivat merkittävästi käyttöönottoprosessia.\n",
      "\n",
      "Analyzing...\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-17 18:23:31,833 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:192] - Initialized FileUtils with log level: INFO\n",
      "2024-11-17 18:23:31,833 - src.utils.FileUtils.file_utils - DEBUG - Initialized FileUtils with log level: INFO\n",
      "2024-11-17 18:23:31,836 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:198] - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n",
      "2024-11-17 18:23:31,836 - src.utils.FileUtils.file_utils - DEBUG - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n",
      "2024-11-17 18:23:31,840 - src.core.language_processing.factory - DEBUG - Using default configuration\n",
      "2024-11-17 18:23:31,843 - src.core.language_processing.factory - DEBUG - Creating text processor for language: None\n",
      "2024-11-17 18:23:31,845 - src.core.language_processing.factory - WARNING - Unsupported language: None, defaulting to English\n",
      "2024-11-17 18:23:31,847 - src.core.language_processing.factory - DEBUG - Creating en processor\n",
      "2024-11-17 18:23:31,849 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:192] - Initialized FileUtils with log level: INFO\n",
      "2024-11-17 18:23:31,849 - src.utils.FileUtils.file_utils - DEBUG - Initialized FileUtils with log level: INFO\n",
      "2024-11-17 18:23:31,852 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:198] - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n",
      "2024-11-17 18:23:31,852 - src.utils.FileUtils.file_utils - DEBUG - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n",
      "2024-11-17 18:23:31,859 - src.core.language_processing.english - DEBUG - Loaded 179 NLTK stopwords\n",
      "2024-11-17 18:23:31,862 - src.core.language_processing.english - DEBUG - Loaded 733 additional stopwords from file\n",
      "2024-11-17 18:23:31,864 - src.core.language_processing.english - DEBUG - Total English stopwords: 831\n",
      "2024-11-17 18:23:31,865 - src.core.language_processing.english - DEBUG - Initialized English processor with 831 stopwords\n",
      "2024-11-17 18:23:31,868 - src.analyzers.keyword_analyzer - DEBUG - Initialized with weights: {'statistical': 0.4, 'llm': 0.6, 'compound_bonus': 0.2, 'domain_bonus': 0.15, 'length_factor': 0.1, 'generic_penalty': 0.3}\n",
      "2024-11-17 18:23:31,869 - src.analyzers.keyword_analyzer - DEBUG - Initialized with config: {'general': {'max_keywords': 10, 'min_keyword_length': 3, 'language': 'fi', 'focus_on': 'general content analysis', 'include_compounds': True, 'max_themes': 3, 'min_confidence': 0.3, 'column_name_to_analyze': 'text'}, 'categories': {}, 'predefined_keywords': {}, 'excluded_keywords': set(), 'analysis_settings': {'theme_analysis': {'enabled': True, 'min_confidence': 0.5}, 'weights': {'statistical': 0.4, 'llm': 0.6}}, 'domain_context': {}}\n",
      "2024-11-17 18:23:31,870 - src.nb_helpers.analyzers - DEBUG - Running keyword analysis\n",
      "2024-11-17 18:23:31,872 - src.nb_helpers.testers - DEBUG - KeywordTester starting analysis\n",
      "2024-11-17 18:23:31,875 - src.analyzers.keyword_analyzer - DEBUG - Starting keyword analysis\n",
      "2024-11-17 18:23:31,876 - src.analyzers.keyword_analyzer - DEBUG - Analyzing text of length 183\n",
      "2024-11-17 18:23:31,877 - src.analyzers.keyword_analyzer - DEBUG - Starting internal analysis\n",
      "2024-11-17 18:23:35,066 - src.core.language_processing.english - DEBUG - Split hyphenated word 'devops-k' into: ['devops', 'k']\n",
      "2024-11-17 18:23:35,110 - src.core.language_processing.english - DEBUG - Split hyphenated word 'devops-k' into: ['devops', 'k']\n",
      "2024-11-17 18:23:38,496 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-17 18:23:38,525 - src.analyzers.keyword_analyzer - DEBUG - Processing compound words\n",
      "2024-11-17 18:23:38,546 - src.core.language_processing.english - DEBUG - Split hyphenated word 'DevOps-käytännöt' into: ['DevOps', 'käytännöt']\n",
      "2024-11-17 18:23:38,549 - src.analyzers.keyword_analyzer - DEBUG - Found compound word: DevOps-käytännöt -> ['DevOps', 'käytännöt']\n",
      "2024-11-17 18:23:38,555 - src.core.language_processing.english - DEBUG - Split hyphenated word 'devops-k' into: ['devops', 'k']\n",
      "2024-11-17 18:23:38,556 - src.analyzers.keyword_analyzer - DEBUG - Found compound word: devops-k -> ['devops', 'k']\n",
      "2024-11-17 18:23:38,570 - src.analyzers.keyword_analyzer - DEBUG - Calculating score for 'pilvipalveluihin' with base score 0.4\n",
      "2024-11-17 18:23:38,571 - src.analyzers.keyword_analyzer - DEBUG - Final score for 'pilvipalveluihin': 0.4\n",
      "2024-11-17 18:23:38,576 - src.analyzers.keyword_analyzer - DEBUG - Calculating score for 'siirtyminen' with base score 0.4\n",
      "2024-11-17 18:23:38,578 - src.analyzers.keyword_analyzer - DEBUG - Final score for 'siirtyminen': 0.4\n",
      "2024-11-17 18:23:38,581 - src.analyzers.keyword_analyzer - DEBUG - Calculating score for 'paransi' with base score 0.4\n",
      "2024-11-17 18:23:38,584 - src.analyzers.keyword_analyzer - DEBUG - Final score for 'paransi': 0.4\n",
      "2024-11-17 18:23:38,588 - src.analyzers.keyword_analyzer - DEBUG - Calculating score for 'rjestelm' with base score 0.4\n",
      "2024-11-17 18:23:38,589 - src.analyzers.keyword_analyzer - DEBUG - Final score for 'rjestelm': 0.4\n",
      "2024-11-17 18:23:38,603 - src.analyzers.keyword_analyzer - DEBUG - Calculating score for 'skaalautuvuutta' with base score 0.4\n",
      "2024-11-17 18:23:38,605 - src.analyzers.keyword_analyzer - DEBUG - Final score for 'skaalautuvuutta': 0.4\n",
      "2024-11-17 18:23:38,607 - src.analyzers.keyword_analyzer - DEBUG - Calculating score for 'hensi' with base score 0.4\n",
      "2024-11-17 18:23:38,608 - src.analyzers.keyword_analyzer - DEBUG - Final score for 'hensi': 0.4\n",
      "2024-11-17 18:23:38,618 - src.analyzers.keyword_analyzer - DEBUG - Calculating score for 'kustannuksia' with base score 0.4\n",
      "2024-11-17 18:23:38,620 - src.analyzers.keyword_analyzer - DEBUG - Final score for 'kustannuksia': 0.4\n",
      "2024-11-17 18:23:38,622 - src.analyzers.keyword_analyzer - DEBUG - Calculating score for 'uudet' with base score 0.4\n",
      "2024-11-17 18:23:38,623 - src.analyzers.keyword_analyzer - DEBUG - Final score for 'uudet': 0.4\n",
      "2024-11-17 18:23:38,624 - src.analyzers.keyword_analyzer - DEBUG - Processing compound word: devops-k\n",
      "2024-11-17 18:23:38,626 - src.core.language_processing.english - DEBUG - Split hyphenated word 'devops-k' into: ['devops', 'k']\n",
      "2024-11-17 18:23:38,626 - src.analyzers.keyword_analyzer - DEBUG - Found parts: ['devops', 'k']\n",
      "2024-11-17 18:23:38,631 - src.analyzers.keyword_analyzer - DEBUG - Calculating score for 'devops-k' with base score 0.4\n",
      "2024-11-17 18:23:38,634 - src.analyzers.keyword_analyzer - DEBUG - Applied compound boost (1.2) for 2 parts, new score: 0.48\n",
      "2024-11-17 18:23:38,636 - src.analyzers.keyword_analyzer - DEBUG - Final score for 'devops-k': 0.48\n",
      "2024-11-17 18:23:38,640 - src.analyzers.keyword_analyzer - DEBUG - Calculating score for 'tehostivat' with base score 0.4\n",
      "2024-11-17 18:23:38,641 - src.analyzers.keyword_analyzer - DEBUG - Final score for 'tehostivat': 0.4\n",
      "2024-11-17 18:23:38,655 - src.analyzers.keyword_analyzer - DEBUG - Calculating score for 'pilvipalveluihin' with base score 0.97\n",
      "2024-11-17 18:23:38,658 - src.analyzers.keyword_analyzer - DEBUG - Applied domain boost (1.15) for 'technical', new score: 1.1155\n",
      "2024-11-17 18:23:38,659 - src.analyzers.keyword_analyzer - DEBUG - Final score for 'pilvipalveluihin': 1.0\n",
      "2024-11-17 18:23:38,668 - src.analyzers.keyword_analyzer - DEBUG - Calculating score for 'siirtyminen' with base score 0.9400000000000001\n",
      "2024-11-17 18:23:38,669 - src.analyzers.keyword_analyzer - DEBUG - Applied domain boost (1.15) for 'technical', new score: 1.081\n",
      "2024-11-17 18:23:38,670 - src.analyzers.keyword_analyzer - DEBUG - Final score for 'siirtyminen': 1.0\n",
      "2024-11-17 18:23:38,674 - src.analyzers.keyword_analyzer - DEBUG - Calculating score for 'paransi' with base score 0.91\n",
      "2024-11-17 18:23:38,675 - src.analyzers.keyword_analyzer - DEBUG - Final score for 'paransi': 0.91\n",
      "2024-11-17 18:23:38,685 - src.analyzers.keyword_analyzer - DEBUG - Calculating score for 'järjestelmän' with base score 0.48\n",
      "2024-11-17 18:23:38,686 - src.analyzers.keyword_analyzer - DEBUG - Applied domain boost (1.15) for 'technical', new score: 0.5519999999999999\n",
      "2024-11-17 18:23:38,687 - src.analyzers.keyword_analyzer - DEBUG - Final score for 'järjestelmän': 0.5519999999999999\n",
      "2024-11-17 18:23:38,698 - src.analyzers.keyword_analyzer - DEBUG - Calculating score for 'skaalautuvuutta' with base score 0.9400000000000001\n",
      "2024-11-17 18:23:38,700 - src.analyzers.keyword_analyzer - DEBUG - Applied domain boost (1.15) for 'technical', new score: 1.081\n",
      "2024-11-17 18:23:38,701 - src.analyzers.keyword_analyzer - DEBUG - Final score for 'skaalautuvuutta': 1.0\n",
      "2024-11-17 18:23:38,702 - src.analyzers.keyword_analyzer - DEBUG - Processing compound word: DevOps-käytännöt\n",
      "2024-11-17 18:23:38,706 - src.core.language_processing.english - DEBUG - Split hyphenated word 'DevOps-käytännöt' into: ['DevOps', 'käytännöt']\n",
      "2024-11-17 18:23:38,708 - src.analyzers.keyword_analyzer - DEBUG - Found parts: ['DevOps', 'käytännöt']\n",
      "2024-11-17 18:23:38,712 - src.analyzers.keyword_analyzer - DEBUG - Calculating score for 'DevOps-käytännöt' with base score 0.57\n",
      "2024-11-17 18:23:38,714 - src.analyzers.keyword_analyzer - DEBUG - Applied domain boost (1.15) for 'technical', new score: 0.6554999999999999\n",
      "2024-11-17 18:23:38,716 - src.analyzers.keyword_analyzer - DEBUG - Applied compound boost (1.2) for 2 parts, new score: 0.7865999999999999\n",
      "2024-11-17 18:23:38,719 - src.analyzers.keyword_analyzer - DEBUG - Final score for 'DevOps-käytännöt': 0.7865999999999999\n",
      "2024-11-17 18:23:38,725 - src.analyzers.keyword_analyzer - DEBUG - Calculating score for 'tehostivat' with base score 0.88\n",
      "2024-11-17 18:23:38,727 - src.analyzers.keyword_analyzer - DEBUG - Final score for 'tehostivat': 0.88\n",
      "2024-11-17 18:23:38,742 - src.analyzers.keyword_analyzer - DEBUG - Calculating score for 'käyttöönottoprosessia' with base score 0.51\n",
      "2024-11-17 18:23:38,744 - src.analyzers.keyword_analyzer - DEBUG - Applied domain boost (1.15) for 'technical', new score: 0.5864999999999999\n",
      "2024-11-17 18:23:38,746 - src.analyzers.keyword_analyzer - DEBUG - Final score for 'käyttöönottoprosessia': 0.5864999999999999\n",
      "2024-11-17 18:23:38,797 - src.core.language_processing.english - DEBUG - Split hyphenated word 'devops-k' into: ['devops', 'k']\n",
      "2024-11-17 18:23:38,814 - src.core.language_processing.english - DEBUG - Split hyphenated word 'DevOps-käytännöt' into: ['DevOps', 'käytännöt']\n",
      "2024-11-17 18:23:38,847 - src.core.language_processing.english - DEBUG - Split hyphenated word 'devops-k' into: ['devops', 'k']\n",
      "2024-11-17 18:23:38,860 - src.core.language_processing.english - DEBUG - Split hyphenated word 'DevOps-käytännöt' into: ['DevOps', 'käytännöt']\n",
      "2024-11-17 18:23:38,883 - src.core.language_processing.english - DEBUG - Split hyphenated word 'devops-k' into: ['devops', 'k']\n",
      "2024-11-17 18:23:38,891 - src.core.language_processing.english - DEBUG - Split hyphenated word 'DevOps-käytännöt' into: ['DevOps', 'käytännöt']\n",
      "2024-11-17 18:23:38,912 - src.core.language_processing.english - DEBUG - Split hyphenated word 'devops-k' into: ['devops', 'k']\n",
      "2024-11-17 18:23:38,917 - src.core.language_processing.english - DEBUG - Split hyphenated word 'DevOps-käytännöt' into: ['DevOps', 'käytännöt']\n",
      "2024-11-17 18:23:38,942 - src.core.language_processing.english - DEBUG - Split hyphenated word 'devops-k' into: ['devops', 'k']\n",
      "2024-11-17 18:23:38,953 - src.core.language_processing.english - DEBUG - Split hyphenated word 'DevOps-käytännöt' into: ['DevOps', 'käytännöt']\n",
      "2024-11-17 18:23:38,967 - src.core.language_processing.english - DEBUG - Split hyphenated word 'devops-k' into: ['devops', 'k']\n",
      "2024-11-17 18:23:38,970 - src.core.language_processing.english - DEBUG - Split hyphenated word 'DevOps-käytännöt' into: ['DevOps', 'käytännöt']\n",
      "2024-11-17 18:23:38,984 - src.core.language_processing.english - DEBUG - Split hyphenated word 'devops-k' into: ['devops', 'k']\n",
      "2024-11-17 18:23:38,991 - src.core.language_processing.english - DEBUG - Split hyphenated word 'DevOps-käytännöt' into: ['DevOps', 'käytännöt']\n",
      "2024-11-17 18:23:39,003 - src.core.language_processing.english - DEBUG - Split hyphenated word 'devops-k' into: ['devops', 'k']\n",
      "2024-11-17 18:23:39,006 - src.core.language_processing.english - DEBUG - Split hyphenated word 'DevOps-käytännöt' into: ['DevOps', 'käytännöt']\n",
      "2024-11-17 18:23:39,014 - src.core.language_processing.english - DEBUG - Split hyphenated word 'devops-k' into: ['devops', 'k']\n",
      "2024-11-17 18:23:39,018 - src.core.language_processing.english - DEBUG - Split hyphenated word 'devops-k' into: ['devops', 'k']\n",
      "2024-11-17 18:23:39,037 - src.core.language_processing.english - DEBUG - Split hyphenated word 'DevOps-käytännöt' into: ['DevOps', 'käytännöt']\n",
      "2024-11-17 18:23:39,038 - src.analyzers.keyword_analyzer - DEBUG - Added compound parts for DevOps-käytännöt: ['DevOps', 'käytännöt']\n",
      "2024-11-17 18:23:39,040 - src.core.language_processing.english - DEBUG - Split hyphenated word 'devops-k' into: ['devops', 'k']\n",
      "2024-11-17 18:23:39,042 - src.analyzers.keyword_analyzer - DEBUG - Added compound parts for devops-k: ['devops', 'k']\n",
      "2024-11-17 18:23:39,043 - src.analyzers.keyword_analyzer - DEBUG - Found 2 compound words\n",
      "2024-11-17 18:23:39,044 - src.analyzers.keyword_analyzer - DEBUG - Analysis complete. Found 10 keywords\n",
      "2024-11-17 18:23:39,046 - src.nb_helpers.testers - DEBUG - KeywordTester analysis complete\n",
      "2024-11-17 18:23:39,048 - src.nb_helpers.analyzers - DEBUG - Keyword analysis completed\n",
      "2024-11-17 18:23:39,049 - src.nb_helpers.analyzers - DEBUG - Formatting results\n",
      "2024-11-17 18:23:39,050 - src.nb_helpers.analyzers - DEBUG - Displaying debug information\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Keywords Found:\n",
      "  • pilvipalveluihin     [████████████████████] (1.00)\n",
      "  • siirtyminen          [████████████████████] (1.00)\n",
      "  • paransi              [████████████████████] (1.00)\n",
      "  • skaalautuvuutta      [████████████████████] (1.00)\n",
      "  • tehostivat           [████████████████████] (1.00)\n",
      "  • DevOps-käytännöt     [██████████████████░░] (0.94)\n",
      "  • käyttöönottoprosessia [██████████████░░░░░░] (0.70)\n",
      "  • järjestelmän         [█████████████░░░░░░░] (0.66)\n",
      "  • devops-k             [██████████░░░░░░░░░░] (0.53)\n",
      "  • hensi                [█████████░░░░░░░░░░░] (0.48)\n",
      "\n",
      "Debug Information:\n",
      "--------------------\n",
      "{\n",
      "  \"keywords\": [\n",
      "    {\n",
      "      \"keyword\": \"pilvipalveluihin\",\n",
      "      \"score\": 1.0,\n",
      "      \"domain\": \"technical\",\n",
      "      \"compound_parts\": null\n",
      "    },\n",
      "    {\n",
      "      \"keyword\": \"siirtyminen\",\n",
      "      \"score\": 1.0,\n",
      "      \"domain\": \"technical\",\n",
      "      \"compound_parts\": null\n",
      "    },\n",
      "    {\n",
      "      \"keyword\": \"paransi\",\n",
      "      \"score\": 1.0,\n",
      "      \"domain\": \"general\",\n",
      "      \"compound_parts\": null\n",
      "    },\n",
      "    {\n",
      "      \"keyword\": \"skaalautuvuutta\",\n",
      "      \"score\": 1.0,\n",
      "      \"domain\": \"technical\",\n",
      "      \"compound_parts\": null\n",
      "    },\n",
      "    {\n",
      "      \"keyword\": \"tehostivat\",\n",
      "      \"score\": 1.0,\n",
      "      \"domain\": \"general\",\n",
      "      \"compound_parts\": null\n",
      "    },\n",
      "    {\n",
      "      \"keyword\": \"DevOps-k\\u00e4yt\\u00e4nn\\u00f6t\",\n",
      "      \"score\": 0.9439199999999998,\n",
      "      \"domain\": \"technical\",\n",
      "      \"compound_parts\": [\n",
      "        \"DevOps\",\n",
      "        \"k\\u00e4yt\\u00e4nn\\u00f6t\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"keyword\": \"k\\u00e4ytt\\u00f6\\u00f6nottoprosessia\",\n",
      "      \"score\": 0.7037999999999999,\n",
      "      \"domain\": \"technical\",\n",
      "      \"compound_parts\": null\n",
      "    },\n",
      "    {\n",
      "      \"keyword\": \"j\\u00e4rjestelm\\u00e4n\",\n",
      "      \"score\": 0.6623999999999999,\n",
      "      \"domain\": \"technical\",\n",
      "      \"compound_parts\": null\n",
      "    },\n",
      "    {\n",
      "      \"keyword\": \"devops-k\",\n",
      "      \"score\": 0.528,\n",
      "      \"domain\": null,\n",
      "      \"compound_parts\": [\n",
      "        \"devops\",\n",
      "        \"k\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"keyword\": \"hensi\",\n",
      "      \"score\": 0.48,\n",
      "      \"domain\": null,\n",
      "      \"compound_parts\": null\n",
      "    }\n",
      "  ],\n",
      "  \"compound_words\": [],\n",
      "  \"domain_keywords\": {\n",
      "    \"technical\": [\n",
      "      \"pilvipalveluihin\",\n",
      "      \"siirtyminen\",\n",
      "      \"skaalautuvuutta\",\n",
      "      \"DevOps-k\\u00e4yt\\u00e4nn\\u00f6t\",\n",
      "      \"k\\u00e4ytt\\u00f6\\u00f6nottoprosessia\",\n",
      "      \"j\\u00e4rjestelm\\u00e4n\"\n",
      "    ],\n",
      "    \"general\": [\n",
      "      \"paransi\",\n",
      "      \"tehostivat\"\n",
      "    ]\n",
      "  },\n",
      "  \"language\": \"en\",\n",
      "  \"success\": true,\n",
      "  \"error\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Test Finnish keyword analysis\n",
    "print(\"\\nTest Finnish keyword analysis:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "options_fi = AnalysisOptions(\n",
    "    show_confidence=True,\n",
    "    show_evidence=True,\n",
    "    debug_mode=True,\n",
    "    language=\"fi\"  # Auto-detect language\n",
    ")\n",
    "\n",
    "# Analyze Finnish text with auto-detection\n",
    "text = example_texts[\"Finnish Technical\"]\n",
    "# results_fi = await analyze_text(text, options_fi)\n",
    "keywords_fi = await analyze_keywords(text, options_fi)\n",
    "\n",
    "\n",
    "# # 4. Test batch analysis with mixed languages\n",
    "# print(\"\\nBatch Analysis with Mixed Languages:\")\n",
    "# print(\"-\" * 50)\n",
    "\n",
    "# batch_results = {}\n",
    "# for name, text in example_texts.items():\n",
    "#     print(f\"\\nAnalyzing {name}:\")\n",
    "#     results = await analyze_text(text, options_auto)\n",
    "#     batch_results[name] = results\n",
    "\n",
    "# # 5. Test Excel file processing\n",
    "# from src.nb_helpers.analyzers import analyze_excel_content\n",
    "\n",
    "# # Create a test DataFrame\n",
    "# import pandas as pd\n",
    "# df = pd.DataFrame({\n",
    "#     \"content\": example_texts.values(),\n",
    "#     \"type\": [name.split()[0] for name in example_texts.keys()],  # \"English\" or \"Finnish\"\n",
    "# })\n",
    "\n",
    "# # Save to temporary Excel file\n",
    "# temp_excel = \"temp_test_content.xlsx\"\n",
    "# df.to_excel(temp_excel, index=False)\n",
    "\n",
    "# print(\"\\nExcel File Analysis:\")\n",
    "# print(\"-\" * 50)\n",
    "\n",
    "# await analyze_excel_content(\n",
    "#     input_file=temp_excel,\n",
    "#     output_file=\"analysis_results\",\n",
    "#     content_column=\"content\",\n",
    "#     parameter_file=\"parameters_en.xlsx\",  # Use our parameter file\n",
    "#     language_column=\"type\"  # Use type column for language\n",
    "# )\n",
    "\n",
    "# # Clean up temporary file\n",
    "# os.remove(temp_excel)\n",
    "\n",
    "# # 6. Compare analysis results\n",
    "# print(\"\\nAnalysis Results Comparison:\")\n",
    "# print(\"-\" * 50)\n",
    "\n",
    "# def print_analysis_summary(results: dict, name: str):\n",
    "#     print(f\"\\n{name}:\")\n",
    "#     if \"keywords\" in results:\n",
    "#         keywords = results[\"keywords\"].get(\"keywords\", [])\n",
    "#         print(f\"Keywords found: {len(keywords)}\")\n",
    "#         for kw in keywords[:3]:  # Show top 3 keywords\n",
    "#             print(f\"- {kw.keyword}: {kw.score:.2f}\")\n",
    "    \n",
    "#     if \"themes\" in results:\n",
    "#         themes = results[\"themes\"].get(\"themes\", [])\n",
    "#         print(f\"Themes found: {len(themes)}\")\n",
    "#         for theme in themes[:2]:  # Show top 2 themes\n",
    "#             print(f\"- {theme.name}: {theme.confidence:.2f}\")\n",
    "            \n",
    "#     if \"categories\" in results:\n",
    "#         categories = results[\"categories\"].get(\"categories\", [])\n",
    "#         print(f\"Categories found: {len(categories)}\")\n",
    "#         for cat in categories[:2]:  # Show top 2 categories\n",
    "#             print(f\"- {cat.name}: {cat.confidence:.2f}\")\n",
    "\n",
    "# for name, results in batch_results.items():\n",
    "#     print_analysis_summary(results, name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run environment verification\n",
    "# from src.nb_helpers.environment import verify_environment\n",
    "# verify_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import other modules after logging is configured\n",
    "from src.nb_helpers.analyzers import (\n",
    "    analyze_keywords,\n",
    "    analyze_themes,\n",
    "    analyze_categories,\n",
    "    analyze_text,\n",
    "    AnalysisOptions\n",
    ")\n",
    "\n",
    "# Create analysis options with language support\n",
    "options = AnalysisOptions(\n",
    "    show_confidence=True,\n",
    "    show_evidence=True,\n",
    "    show_keywords=True,\n",
    "    show_raw_data=True,\n",
    "    debug_mode=True,\n",
    "    # Optional language override\n",
    "    language=None,  # Will auto-detect if not specified\n",
    "    # Optional parameter file\n",
    "    parameter_file=\"analysis_params.yaml\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test logging\n",
    "logger = logging.getLogger(\"src.analyzers.keyword_analyzer\")\n",
    "logger.debug(\"Testing keyword analyzer logging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- define language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example texts in different languages\n",
    "example_texts = {\n",
    "    \"English Technical\": \"\"\"\n",
    "        The cloud migration project improved system scalability while reducing costs.\n",
    "        New DevOps practices streamlined the deployment pipeline significantly.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Finnish Technical\": \"\"\"\n",
    "        Pilvipalveluihin siirtyminen paransi järjestelmän skaalautuvuutta ja vähensi kustannuksia.\n",
    "        Uudet DevOps-käytännöt tehostivat merkittävästi käyttöönottoprosessia.\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "# Analyze with automatic language detection\n",
    "for name, text in example_texts.items():\n",
    "    print(f\"\\nAnalyzing {name}:\")\n",
    "    results = await analyze_text(text, options)\n",
    "\n",
    "# Example with specific language and parameters\n",
    "fi_options = AnalysisOptions(\n",
    "    show_confidence=True,\n",
    "    show_evidence=True,\n",
    "    debug_mode=True,\n",
    "    language=\"fi\",\n",
    "    parameter_file=\"finnish_params.yaml\"\n",
    ")\n",
    "\n",
    "# Analyze Finnish text with specific parameters\n",
    "fi_results = await analyze_text(example_texts[\"Finnish Technical\"], fi_options)\n",
    "\n",
    "# Batch process Excel file with language detection\n",
    "await analyze_excel_content(\n",
    "    input_file=\"multilingual_texts.xlsx\",\n",
    "    output_file=\"analysis_results\",\n",
    "    content_column=\"content\",\n",
    "    parameter_file=\"analysis_params.yaml\",\n",
    "    language_column=\"language\"  # Optional column specifying language\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ## Analysis Functions\n",
    "\n",
    "### Single Analysis with Debug Output\n",
    "Run detailed analysis for a single text: -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_texts = {\n",
    "    \"Business Analysis\": \"\"\"\n",
    "        Q3 revenue increased by 15% with strong growth in enterprise sales.\n",
    "        Customer retention improved while acquisition costs decreased.\n",
    "        New market expansion initiatives are showing positive early results.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Technical Content\": \"\"\"\n",
    "        The application uses microservices architecture with containerized deployments.\n",
    "        Data processing pipeline incorporates machine learning models for prediction.\n",
    "        System monitoring ensures high availability and performance metrics.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Mixed Content\": \"\"\"\n",
    "        The IT department's cloud migration project reduced infrastructure costs by 25%.\n",
    "        DevOps implementation improved deployment frequency while maintaining quality.\n",
    "        Monthly recurring revenue from SaaS products grew steadily.\n",
    "    \"\"\",\n",
    "    \"koulutus\":\n",
    "    \"\"\"\n",
    "        Verkko-oppimisalusta sisältää interaktiivisia moduuleja ja oman tahdin edistymisen seurannan. \n",
    "        Virtuaaliluokat mahdollistavat reaaliaikaisen yhteistyön opiskelijoiden ja ohjaajien välillä. \n",
    "        Digitaaliset arviointityökalut antavat välitöntä palautetta oppimistuloksista.\n",
    "    \"\"\",\n",
    "    \"tekninen\":\n",
    "    \"\"\"\n",
    "        Koneoppimismalleja koulutetaan suurilla datajoukolla tunnistamaan kaavoja. \n",
    "        Neuroverkon arkkitehtuuri sisältää useita kerroksia piirteiden erottamiseen. \n",
    "        Datan esikäsittely ja piirteiden suunnittelu ovat keskeisiä vaiheita prosessissa.\n",
    "\n",
    "    \"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # New imports\n",
    "# from src.core.language_parameters import LanguageParameterManager\n",
    "\n",
    "# # Initialize parameter manager\n",
    "# param_manager = LanguageParameterManager()\n",
    "\n",
    "# # Example analysis with automatic language detection\n",
    "# text_en = \"Cloud computing enables scalable infrastructure deployment.\"\n",
    "# text_fi = \"Pilvipalvelut mahdollistavat skaalautuvan infrastruktuurin käyttöönoton.\"\n",
    "\n",
    "# # Analyze with automatic language detection and default parameters\n",
    "# async def analyze_text_with_language(text: str, parameter_file: Optional[str] = None):\n",
    "#     \"\"\"Analyze text with automatic language handling.\"\"\"\n",
    "#     # Get language-specific parameters\n",
    "#     params = param_manager.get_parameters(text, parameter_file)\n",
    "    \n",
    "#     # Create analyzers with parameters\n",
    "#     keyword_analyzer = KeywordAnalyzer(config=params.dict())\n",
    "#     theme_analyzer = ThemeAnalyzer(config=params.dict())\n",
    "#     category_analyzer = CategoryAnalyzer(config=params.dict())\n",
    "    \n",
    "#     # Run analysis\n",
    "#     results = {\n",
    "#         \"keywords\": await keyword_analyzer.analyze(text),\n",
    "#         \"themes\": await theme_analyzer.analyze(text),\n",
    "#         \"categories\": await category_analyzer.analyze(text)\n",
    "#     }\n",
    "    \n",
    "#     return results\n",
    "\n",
    "# # Example with Excel parameters\n",
    "# async def analyze_batch_with_excel_params(texts: List[str], excel_params: str):\n",
    "#     \"\"\"Analyze texts using parameters from Excel.\"\"\"\n",
    "#     # Load language-specific parameters\n",
    "#     params_by_lang = param_manager.load_excel_parameters(excel_params)\n",
    "    \n",
    "#     results = []\n",
    "#     for text in texts:\n",
    "#         # Detect language\n",
    "#         lang = param_manager.detect_language(text)\n",
    "#         # Get parameters for language\n",
    "#         params = params_by_lang.get(lang, param_manager.get_parameters(text))\n",
    "        \n",
    "#         # Create analyzer with language-specific parameters\n",
    "#         analyzer = KeywordAnalyzer(config=params.dict())\n",
    "#         result = await analyzer.analyze(text)\n",
    "#         results.append(result)\n",
    "    \n",
    "#     return results\n",
    "\n",
    "# # Example usage:\n",
    "# # With default parameters\n",
    "# results_en = await analyze_text_with_language(text_en)\n",
    "\n",
    "# # With parameter file\n",
    "# results_fi = await analyze_text_with_language(text_fi, \"finnish_params.yaml\")\n",
    "\n",
    "# # With Excel parameters\n",
    "# texts = [text_en, text_fi]\n",
    "# batch_results = await analyze_batch_with_excel_params(texts, \"analysis_params.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = example_texts[\"Mixed Content\"]\n",
    "# text = example_texts[\"koulutussisältö\"]\n",
    "# Debug specific analyzer\n",
    "\n",
    "# Example usage\n",
    "text = example_texts[\"Mixed Content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'options' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m analyze_keywords(text, options\u001b[38;5;241m=\u001b[39moptions)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'options' is not defined"
     ]
    }
   ],
   "source": [
    "await analyze_keywords(text, options=options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Debug Theme Analysis\n",
      "==================================================\n",
      "\n",
      "Input Text:\n",
      "--------------------\n",
      "The IT department's cloud migration project reduced infrastructure costs by 25%.\n",
      "        DevOps implementation improved deployment frequency while maintaining quality.\n",
      "        Monthly recurring revenue from SaaS products grew steadily.\n",
      "\n",
      "Running Analysis...\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "await analyze_themes(text, options=options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Debug Category Analysis\n",
      "==================================================\n",
      "\n",
      "Input Text:\n",
      "--------------------\n",
      "The IT department's cloud migration project reduced infrastructure costs by 25%.\n",
      "        DevOps implementation improved deployment frequency while maintaining quality.\n",
      "        Monthly recurring revenue from SaaS products grew steadily.\n",
      "\n",
      "Running Analysis...\n",
      "--------------------\n",
      "\n",
      "Categories Found:\n",
      "\n",
      "  • technical\n",
      "    Confidence: [█████████████████░░░] (0.85)\n",
      "    The text discusses a cloud migration project and DevOps implementation, both of which are technical processes related to software development and system management.\n",
      "    Evidence:\n",
      "      - cloud migration project\n",
      "      - DevOps implementation\n",
      "      - improved deployment frequency\n",
      "\n",
      "  • business\n",
      "    Confidence: [███████████████░░░░░] (0.75)\n",
      "    The text mentions reduced infrastructure costs and growth in monthly recurring revenue, which are key indicators of business performance and financial health.\n",
      "    Evidence:\n",
      "      - reduced infrastructure costs by 25%\n",
      "      - monthly recurring revenue from SaaS products grew steadily\n",
      "\n",
      "Debug Information:\n",
      "--------------------\n",
      "\n",
      "Confidence Statistics:\n",
      "  Average: 0.80\n",
      "  Max: 0.85\n",
      "  Min: 0.75\n",
      "\n",
      "Raw Analysis Data:\n",
      "{\n",
      "  \"language\": \"en\",\n",
      "  \"error\": null,\n",
      "  \"success\": true,\n",
      "  \"categories\": [\n",
      "    {\n",
      "      \"name\": \"technical\",\n",
      "      \"confidence\": 0.85,\n",
      "      \"explanation\": \"The text discusses a cloud migration project and DevOps implementation, both of which are technical processes related to software development and system management.\",\n",
      "      \"evidence\": [\n",
      "        \"cloud migration project\",\n",
      "        \"DevOps implementation\",\n",
      "        \"improved deployment frequency\"\n",
      "      ],\n",
      "      \"themes\": [\n",
      "        \"cloud computing\",\n",
      "        \"DevOps\",\n",
      "        \"infrastructure management\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"business\",\n",
      "      \"confidence\": 0.75,\n",
      "      \"explanation\": \"The text mentions reduced infrastructure costs and growth in monthly recurring revenue, which are key indicators of business performance and financial health.\",\n",
      "      \"evidence\": [\n",
      "        \"reduced infrastructure costs by 25%\",\n",
      "        \"monthly recurring revenue from SaaS products grew steadily\"\n",
      "      ],\n",
      "      \"themes\": [\n",
      "        \"cost reduction\",\n",
      "        \"revenue growth\",\n",
      "        \"SaaS business model\"\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"explanations\": {\n",
      "    \"technical\": \"The text discusses a cloud migration project and DevOps implementation, both of which are technical processes related to software development and system management.\",\n",
      "    \"business\": \"The text mentions reduced infrastructure costs and growth in monthly recurring revenue, which are key indicators of business performance and financial health.\"\n",
      "  },\n",
      "  \"evidence\": {\n",
      "    \"technical\": [\n",
      "      \"cloud migration project\",\n",
      "      \"DevOps implementation\",\n",
      "      \"improved deployment frequency\"\n",
      "    ],\n",
      "    \"business\": [\n",
      "      \"reduced infrastructure costs by 25%\",\n",
      "      \"monthly recurring revenue from SaaS products grew steadily\"\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CategoryOutput(language='en', error=None, success=True, categories=[CategoryInfo(name='technical', confidence=0.85, explanation='The text discusses a cloud migration project and DevOps implementation, both of which are technical processes related to software development and system management.', evidence=['cloud migration project', 'DevOps implementation', 'improved deployment frequency'], themes=['cloud computing', 'DevOps', 'infrastructure management']), CategoryInfo(name='business', confidence=0.75, explanation='The text mentions reduced infrastructure costs and growth in monthly recurring revenue, which are key indicators of business performance and financial health.', evidence=['reduced infrastructure costs by 25%', 'monthly recurring revenue from SaaS products grew steadily'], themes=['cost reduction', 'revenue growth', 'SaaS business model'])], explanations={'technical': 'The text discusses a cloud migration project and DevOps implementation, both of which are technical processes related to software development and system management.', 'business': 'The text mentions reduced infrastructure costs and growth in monthly recurring revenue, which are key indicators of business performance and financial health.'}, evidence={'technical': ['cloud migration project', 'DevOps implementation', 'improved deployment frequency'], 'business': ['reduced infrastructure costs by 25%', 'monthly recurring revenue from SaaS products grew steadily']})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await analyze_categories(text, options=options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or run full pipeline with debug info\n",
    "await debug_full_pipeline(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Processing from Excel\n",
    "Process multiple texts from Excel file:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await analyze_excel_content(\n",
    "    input_file=\"test_content.xlsx\",  # Input Excel file path\n",
    "    output_file=\"analysis_results\",  # Output filename (without extension)\n",
    "    content_column=\"content\"         # Column containing text to analyze\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "- Configure analyzers using parameter files\n",
    "- Control output detail with DebugOptions\n",
    "- Set logging level for verbosity control\n",
    "\n",
    "## Example Outputs\n",
    "The analysis provides:\n",
    "- Keywords with confidence scores\n",
    "- Theme identification and descriptions\n",
    "- Category classification with evidence\n",
    "- Confidence visualizations with Unicode bars\n",
    "\n",
    "## Notes\n",
    "- Set logging level to WARNING to minimize output\n",
    "- Use debug functions for detailed analysis inspection\n",
    "- Excel output combines all analysis types"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semantic-analyzer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
