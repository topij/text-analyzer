{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Analysis Pipeline\n",
    "\n",
    "This notebook demonstrates the semantic text analysis capabilities using our custom analyzers.\n",
    "\n",
    "## Setup\n",
    "Import required packages and configure the environment:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At start of notebook\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = str(Path().resolve().parent)\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 14:03:30,306 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:192] - Initialized FileUtils with log level: INFO\n",
      "2024-11-20 14:03:30,310 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:198] - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n",
      "2024-11-20 14:03:30,320 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:192] - Initialized FileUtils with log level: INFO\n",
      "2024-11-20 14:03:30,324 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:198] - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Check Results:\n",
      "==================================================\n",
      "\n",
      "Basic Setup:\n",
      "-----------\n",
      "✓ Project root in path\n",
      "✓ FileUtils initialized\n",
      "✓ .env file loaded\n",
      "\n",
      "Environment Variables:\n",
      "---------------------\n",
      "✓ OPENAI_API_KEY set\n",
      "✓ ANTHROPIC_API_KEY set\n",
      "\n",
      "Project Structure:\n",
      "-----------------\n",
      "✓ Raw data exists\n",
      "✓ Processed data exists\n",
      "✓ Configuration exists\n",
      "✓ Main config.yaml exists\n",
      "\n",
      "==================================================\n",
      "Environment Status: Ready ✓\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import logging\n",
    "# from src.nb_helpers.logging import configure_logging\n",
    "\n",
    "# Set up environment with DEBUG level\n",
    "from src.nb_helpers.environment import setup_notebook_env, verify_environment\n",
    "setup_notebook_env(log_level=\"DEBUG\")\n",
    "\n",
    "# Any verification needed will maintain DEBUG level\n",
    "verify_environment(log_level=\"DEBUG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary components\n",
    "from src.loaders.parameter_handler import ParameterHandler\n",
    "from src.nb_helpers.analyzers import (\n",
    "    analyze_keywords,\n",
    "    analyze_themes,\n",
    "    analyze_categories,\n",
    "    analyze_text,\n",
    "    AnalysisOptions\n",
    ")\n",
    "\n",
    "from scripts.migrate_parameters import create_example_parameters\n",
    "from src.nb_helpers.logging import configure_logging, verify_logging_setup_with_hierarchy, reset_debug_logging\n",
    "from src.loaders.parameter_handler import ParameterHandler, get_parameter_file_path, verify_parameter_file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 14:03:30,508 - src.nb_helpers.logging - DEBUG - Logging configured at DEBUG level\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logging Configuration:\n",
      "--------------------------------------------------\n",
      "\n",
      "Logger: root\n",
      "Set Level: DEBUG\n",
      "Effective Level: DEBUG\n",
      "Propagates to root: True\n",
      "Handlers:\n",
      "  Handler 1 level: DEBUG\n",
      "\n",
      "Logger: src.nb_helpers.analyzers\n",
      "Hierarchy:\n",
      "  src: NOTSET\n",
      "  src.nb_helpers: NOTSET\n",
      "  src.nb_helpers.analyzers: DEBUG\n",
      "Set Level: DEBUG\n",
      "Effective Level: DEBUG\n",
      "Propagates to root: True\n",
      "No handlers (uses root handlers)\n",
      "\n",
      "Logger: src.analyzers.keyword_analyzer\n",
      "Hierarchy:\n",
      "  src: NOTSET\n",
      "  src.analyzers: NOTSET\n",
      "  src.analyzers.keyword_analyzer: DEBUG\n",
      "Set Level: DEBUG\n",
      "Effective Level: DEBUG\n",
      "Propagates to root: True\n",
      "No handlers (uses root handlers)\n",
      "\n",
      "Logger: src.analyzers.theme_analyzer\n",
      "Hierarchy:\n",
      "  src: NOTSET\n",
      "  src.analyzers: NOTSET\n",
      "  src.analyzers.theme_analyzer: DEBUG\n",
      "Set Level: DEBUG\n",
      "Effective Level: DEBUG\n",
      "Propagates to root: True\n",
      "No handlers (uses root handlers)\n",
      "\n",
      "Logger: src.analyzers.category_analyzer\n",
      "Hierarchy:\n",
      "  src: NOTSET\n",
      "  src.analyzers: NOTSET\n",
      "  src.analyzers.category_analyzer: DEBUG\n",
      "Set Level: DEBUG\n",
      "Effective Level: DEBUG\n",
      "Propagates to root: True\n",
      "No handlers (uses root handlers)\n",
      "\n",
      "Logger: src.utils.FileUtils.file_utils\n",
      "Hierarchy:\n",
      "  src: NOTSET\n",
      "  src.utils: NOTSET\n",
      "  src.utils.FileUtils: NOTSET\n",
      "  src.utils.FileUtils.file_utils: DEBUG\n",
      "Set Level: DEBUG\n",
      "Effective Level: DEBUG\n",
      "Propagates to root: True\n",
      "No handlers (uses root handlers)\n",
      "\n",
      "Logger: httpx\n",
      "Hierarchy:\n",
      "  httpx: INFO\n",
      "Set Level: INFO\n",
      "Effective Level: INFO\n",
      "Propagates to root: True\n",
      "No handlers (uses root handlers)\n"
     ]
    }
   ],
   "source": [
    "# Set initial logging\n",
    "configure_logging(level=\"DEBUG\")\n",
    "# Keep HTTP loggers at INFO\n",
    "for name in [\"httpx\", \"httpcore\", \"openai\", \"anthropic\"]:\n",
    "    logging.getLogger(name).setLevel(logging.INFO)\n",
    "    \n",
    "verify_logging_setup_with_hierarchy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detailed_logging_info = True\n",
    "# if detailed_logging_info:\n",
    "#     from src.nb_helpers.logging import verify_logging_setup_with_hierarchy\n",
    "#     # Configure logging\n",
    "#     # configure_logging(level=\"DEBUG\")\n",
    "#     # Verify with detailed information\n",
    "#     verify_logging_setup_with_hierarchy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example texts in different languages\n",
    "example_texts = {\n",
    "    \"English Technical\": \"\"\"\n",
    "        The cloud migration project improved system scalability while reducing costs.\n",
    "        New DevOps practices streamlined the deployment pipeline significantly.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Finnish Technical\": \"\"\"\n",
    "        Pilvipalveluihin siirtyminen paransi järjestelmän skaalautuvuutta ja vähensi kustannuksia.\n",
    "        Uudet DevOps-käytännöt tehostivat merkittävästi käyttöönottoprosessia.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"English Business\": \"\"\"\n",
    "        Q3 financial results show 15% revenue growth and improved profit margins.\n",
    "        Customer acquisition costs decreased while retention rates increased.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Finnish Business\": \"\"\"\n",
    "        Q3 taloudelliset tulokset osoittavat 15% liikevaihdon kasvun ja parantuneet katteet.\n",
    "        Asiakashankinnan kustannukset laskivat ja asiakaspysyvyys parani.\n",
    "    \"\"\"\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 14:03:30,640 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:192] - Initialized FileUtils with log level: INFO\n",
      "2024-11-20 14:03:30,640 - src.utils.FileUtils.file_utils - DEBUG - Initialized FileUtils with log level: INFO\n",
      "2024-11-20 14:03:30,642 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:198] - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n",
      "2024-11-20 14:03:30,642 - src.utils.FileUtils.file_utils - DEBUG - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing parameter file at: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\parameters\\parameters_en.xlsx\n",
      "\n",
      "Parameter File Verification:\n",
      "Absolute path: C:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\parameters\\parameters_en.xlsx\n",
      "File exists: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 14:03:32,636 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:192] - Initialized FileUtils with log level: INFO\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found sheets:\n",
      "\n",
      "General Parameters:\n",
      "                parameter                           value  \\\n",
      "0            max_keywords                              10   \n",
      "1                focus_on  technical and business content   \n",
      "2  column_name_to_analyze                         content   \n",
      "3      min_keyword_length                               3   \n",
      "4       include_compounds                            True   \n",
      "\n",
      "                          description  \n",
      "0  Maximum keywords to extract (1-20)  \n",
      "1                 Analysis focus area  \n",
      "2          Name of the content column  \n",
      "3              Minimum keyword length  \n",
      "4              Include compound words  \n",
      "\n",
      "Categories:\n",
      "              category                                 description  \\\n",
      "0    technical_content  Technical and software development content   \n",
      "1     business_content              Business and financial content   \n",
      "2  educational_content            Educational and training content   \n",
      "\n",
      "                                            keywords  threshold  parent  \n",
      "0  software,development,api,programming,technical...        0.6     NaN  \n",
      "1     revenue,sales,market,growth,financial,business        0.6     NaN  \n",
      "2        learning,education,training,teaching,skills        0.5     NaN  \n",
      "\n",
      "Predefined Keywords:\n",
      "            keyword  importance     domain\n",
      "0  machine learning         1.0  technical\n",
      "1   cloud computing         0.9  technical\n",
      "2    revenue growth         0.9   business\n",
      "\n",
      "Excluded Keywords:\n",
      "  keyword       reason\n",
      "0     the  Common word\n",
      "1     and  Common word\n",
      "2     for  Common word\n",
      "\n",
      "Analysis Settings:\n",
      "                         setting  value  \\\n",
      "0  theme_analysis.min_confidence    0.5   \n",
      "1            weights.statistical    0.4   \n",
      "2                    weights.llm    0.6   \n",
      "\n",
      "                              description  \n",
      "0  Minimum confidence for theme detection  \n",
      "1         Weight for statistical analysis  \n",
      "2                 Weight for LLM analysis  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 14:03:32,636 - src.utils.FileUtils.file_utils - DEBUG - Initialized FileUtils with log level: INFO\n",
      "2024-11-20 14:03:32,639 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:198] - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n",
      "2024-11-20 14:03:32,639 - src.utils.FileUtils.file_utils - DEBUG - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n",
      "2024-11-20 14:03:32,639 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:198] - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n",
      "2024-11-20 14:03:32,639 - src.utils.FileUtils.file_utils - DEBUG - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n",
      "2024-11-20 14:03:32,652 - src.loaders.parameter_handler - DEBUG - Using parameter file: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\parameters\\parameters_en.xlsx\n",
      "2024-11-20 14:03:32,654 - src.loaders.parameter_handler - DEBUG - Loading Excel file from: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\parameters\\parameters_en.xlsx\n",
      "2024-11-20 14:03:32,707 - src.utils.FileUtils.file_utils - INFO [file_utils.py:1018] - Successfully loaded 5 sheets from c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\parameters\\parameters_en.xlsx\n",
      "2024-11-20 14:03:32,707 - src.utils.FileUtils.file_utils - INFO - Successfully loaded 5 sheets from c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\parameters\\parameters_en.xlsx\n",
      "2024-11-20 14:03:32,710 - src.loaders.parameter_handler - DEBUG - Found sheets: ['General Parameters', 'Categories', 'Predefined Keywords', 'Excluded Keywords', 'Analysis Settings']\n",
      "2024-11-20 14:03:32,713 - src.loaders.parameter_handler - DEBUG - Looking for general parameters in sheet: General Parameters\n",
      "2024-11-20 14:03:32,717 - src.loaders.parameter_handler - DEBUG - General sheet columns: ['parameter', 'value', 'description']\n",
      "2024-11-20 14:03:32,722 - src.loaders.parameter_handler - DEBUG - General sheet content:\n",
      "                parameter                           value  \\\n",
      "0            max_keywords                              10   \n",
      "1                focus_on  technical and business content   \n",
      "2  column_name_to_analyze                         content   \n",
      "3      min_keyword_length                               3   \n",
      "4       include_compounds                            True   \n",
      "5                language                              en   \n",
      "\n",
      "                          description  \n",
      "0  Maximum keywords to extract (1-20)  \n",
      "1                 Analysis focus area  \n",
      "2          Name of the content column  \n",
      "3              Minimum keyword length  \n",
      "4              Include compound words  \n",
      "5                    Content language  \n",
      "2024-11-20 14:03:32,724 - src.loaders.parameter_handler - DEBUG - Starting with default general parameters: {'max_keywords': 10, 'min_keyword_length': 3, 'language': 'en', 'focus_on': 'general content analysis', 'include_compounds': True, 'max_themes': 3, 'min_confidence': 0.3, 'column_name_to_analyze': 'text'}\n",
      "2024-11-20 14:03:32,727 - src.loaders.parameter_handler - DEBUG - Found parameter and value columns\n",
      "2024-11-20 14:03:32,734 - src.loaders.parameter_handler - DEBUG - Full dataframe:\n",
      "                parameter                           value  \\\n",
      "0            max_keywords                              10   \n",
      "1                focus_on  technical and business content   \n",
      "2  column_name_to_analyze                         content   \n",
      "3      min_keyword_length                               3   \n",
      "4       include_compounds                            True   \n",
      "5                language                              en   \n",
      "\n",
      "                          description  \n",
      "0  Maximum keywords to extract (1-20)  \n",
      "1                 Analysis focus area  \n",
      "2          Name of the content column  \n",
      "3              Minimum keyword length  \n",
      "4              Include compound words  \n",
      "5                    Content language  \n",
      "2024-11-20 14:03:32,736 - src.loaders.parameter_handler - DEBUG - Setting max_keywords = 10\n",
      "2024-11-20 14:03:32,738 - src.loaders.parameter_handler - DEBUG - Setting focus_on = technical and business content\n",
      "2024-11-20 14:03:32,740 - src.loaders.parameter_handler - DEBUG - Setting column_name_to_analyze = content\n",
      "2024-11-20 14:03:32,741 - src.loaders.parameter_handler - DEBUG - Setting min_keyword_length = 3\n",
      "2024-11-20 14:03:32,743 - src.loaders.parameter_handler - DEBUG - Setting include_compounds = True\n",
      "2024-11-20 14:03:32,745 - src.loaders.parameter_handler - DEBUG - Setting language = en\n",
      "2024-11-20 14:03:32,750 - src.loaders.parameter_handler - DEBUG - Final general parameters: {'max_keywords': 10, 'min_keyword_length': 3, 'language': 'en', 'focus_on': 'technical and business content', 'include_compounds': True, 'max_themes': 3, 'min_confidence': 0.3, 'column_name_to_analyze': 'content'}\n",
      "2024-11-20 14:03:32,751 - src.loaders.parameter_handler - DEBUG - Parsed general parameters: {'max_keywords': 10, 'min_keyword_length': 3, 'language': 'en', 'focus_on': 'technical and business content', 'include_compounds': True, 'max_themes': 3, 'min_confidence': 0.3, 'column_name_to_analyze': 'content'}\n",
      "2024-11-20 14:03:32,755 - src.loaders.parameter_handler - DEBUG - Created parameter set: {'general': {'max_keywords': 10, 'min_keyword_length': 3, 'language': 'en', 'focus_on': 'technical and business content', 'include_compounds': True, 'max_themes': 3, 'min_confidence': 0.3, 'column_name_to_analyze': 'content'}, 'categories': {'technical_content': {'description': 'Technical and software development content', 'keywords': ['software', 'development', 'api', 'programming', 'technical', 'code'], 'threshold': 0.6, 'parent': None}, 'business_content': {'description': 'Business and financial content', 'keywords': ['revenue', 'sales', 'market', 'growth', 'financial', 'business'], 'threshold': 0.6, 'parent': None}, 'educational_content': {'description': 'Educational and training content', 'keywords': ['learning', 'education', 'training', 'teaching', 'skills'], 'threshold': 0.5, 'parent': None}}, 'predefined_keywords': {'machine learning': {'importance': 1.0, 'domain': 'technical', 'compound_parts': []}, 'cloud computing': {'importance': 0.9, 'domain': 'technical', 'compound_parts': []}, 'revenue growth': {'importance': 0.9, 'domain': 'business', 'compound_parts': []}}, 'excluded_keywords': {'and', 'for', 'the'}, 'analysis_settings': {'theme_analysis': {'enabled': True, 'min_confidence': 0.5}, 'weights': {'statistical': 0.4, 'llm': 0.6}}, 'domain_context': {}}\n"
     ]
    }
   ],
   "source": [
    "# Create and load parameters\n",
    "params_file_name = \"parameters_en.xlsx\"\n",
    "\n",
    "# Get the full parameter file path\n",
    "params_file = get_parameter_file_path(params_file_name)\n",
    "\n",
    "# Create file if it doesn't exist\n",
    "if not params_file.exists():\n",
    "    params_file = create_example_parameters(params_file_name)\n",
    "    print(f\"Created parameter file at: {params_file}\")\n",
    "else:\n",
    "    print(f\"Using existing parameter file at: {params_file}\")\n",
    "\n",
    "# Verify the file\n",
    "verify_parameter_file(params_file)\n",
    "\n",
    "# Load parameters\n",
    "handler = ParameterHandler(params_file_name)  # Can now use just the file name\n",
    "params = handler.get_parameters()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded parameters:\n",
      "ParameterSet(\n",
      "  general=max_keywords=10 min_keyword_length=3 language='en' focus_on='technical and business content' include_compounds=True max_themes=3 min_confidence=0.3 column_name_to_analyze='content',\n",
      "  categories=3 items,\n",
      "  predefined_keywords=3 items,\n",
      "  excluded_keywords=3 items,\n",
      "  analysis_settings=theme_analysis=ThemeAnalysisSettings(enabled=True, min_confidence=0.5) weights=AnalysisWeights(statistical=0.4, llm=0.6),\n",
      "  domain_context=0 items\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLoaded parameters:\")\n",
    "# params.print()  # Uses the new print method\n",
    "\n",
    "# Or just\n",
    "print(params)  # Uses the new __str__ method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not creating new example files.\n"
     ]
    }
   ],
   "source": [
    "# Create example files\n",
    "\n",
    "create_new_example_files=False\n",
    "\n",
    "if create_new_example_files:\n",
    "    en_params = create_example_parameters(\"parameters_en.xlsx\", \"en\")\n",
    "    fi_params = create_example_parameters(\"parameters_fi.xlsx\", \"fi\")\n",
    "    print(f\"Created parameter files:\\n- {en_params}\\n- {fi_params}\")\n",
    "    print(f\"Created parameter files:\\n- {en_params}\\n- {fi_params}\")\n",
    "else:\n",
    "    print(\"Not creating new example files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter Validation:\n",
      "--------------------------------------------------\n",
      "\n",
      "Parameters validated successfully!\n"
     ]
    }
   ],
   "source": [
    "# Check parameter validation\n",
    "print(\"Parameter Validation:\")\n",
    "print(\"-\" * 50)\n",
    "is_valid, warnings, errors = handler.validate()\n",
    "if warnings:\n",
    "    print(\"\\nWarnings:\")\n",
    "    for warning in warnings:\n",
    "        print(f\"- {warning}\")\n",
    "if not is_valid:\n",
    "    print(\"\\nErrors:\")\n",
    "    for error in errors:\n",
    "        print(f\"- {error}\")\n",
    "else:\n",
    "    print(\"\\nParameters validated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases\n",
    "test_texts = {\n",
    "    \"Technical Compound Terms\": \"\"\"\n",
    "        The cloud migration project improved system scalability.\n",
    "        DevOps practices streamlined the deployment pipeline.\n",
    "        Our microservices architecture enables API integrations.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Mixed Domain Content\": \"\"\"\n",
    "        The IT department's infrastructure costs decreased by 25%\n",
    "        after implementing cloud-native solutions. Monthly recurring\n",
    "        revenue from SaaS products grew steadily while deployment\n",
    "        frequency improved.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Business Focus\": \"\"\"\n",
    "        Market analysis shows 15% revenue growth in Q3.\n",
    "        Customer acquisition costs decreased while retention rates\n",
    "        increased. Strategic partnerships drove innovation.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Multiple Compounds\": \"\"\"\n",
    "        Machine learning models process real-time data streams.\n",
    "        The CI/CD pipeline integrates automated testing workflows.\n",
    "        Cloud-based infrastructure supports multi-region deployments.\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "finnish_texts = {\n",
    "    \"technical_fi_1\":\"Pilvipalveluiden käyttöönotto tehosti järjestelmän skaalautuvuutta merkittävästi. DevOps-prosessit nopeuttivat julkaisusykliä ja automatisoivat laadunvarmistusta. Kuukausittainen tilaustuotto SaaS-ratkaisuista kasvoi 25%.\",\n",
    "    \"technical_fi_2\":\"Mikropalveluarkkitehtuuri mahdollisti järjestelmän modulaarisen kehityksen. Konttiteknologian avulla saavutettiin parempi resurssien käyttöaste ja joustavampi ylläpito. Rajapintojen dokumentointi helpotti integraatioiden toteuttamista.\",\n",
    "    \"technical_fi_3\":\"Tekoälypohjaiset ennusteet auttoivat optimoimaan kuormantasausta. Pilvinatiivi lähestymistapa vähensi infrastruktuurikustannuksia ja paransi vikasietoisuutta. Monitorointi tarjosi reaaliaikaista näkyvyyttä suorituskykyyn.\",\n",
    "    \"business_fi_1\":\"Liikevaihdon kasvu vahvistui kolmannella vuosineljänneksellä 15 prosenttiin. Asiakashankinnan kustannukset laskivat samalla kun asiakaspysyvyys parani. Markkinaosuus kasvoi erityisesti pilvipalveluiden segmentissä.\",\n",
    "    \"business_fi_2\":\"Analytiikkatyökalut paljastivat uusia käyttäytymismalleja asiakasrajapinnassa. Toistuvaislaskutuksen osuus kokonaistuotoista nousi 75 prosenttiin. Automaattinen raportointi tehosti päätöksentekoa.\",\n",
    "    \"business_fi_3\":\"Uudet tuotelanseeraukset vahvistivat kilpailuasemaa. Strategiset kumppanuudet mahdollistivat laajentumisen uusille markkina-alueille. Resurssien kohdentaminen tuotekehitykseen tuotti merkittävää kasvua.\"\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import KeywordAnalyzer, ThemeAnalyzer, CategoryAnalyzer, TextAnalyzer\n",
    "from src.core.language_processing import create_text_processor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theme analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create language processor\n",
    "processor = create_text_processor(language=\"fi\")\n",
    "\n",
    "# Initialize analyzer\n",
    "analyzer = ThemeAnalyzer(\n",
    "    language_processor=processor,\n",
    "    config={\n",
    "        \"max_themes\": 3,\n",
    "        \"min_confidence\": 0.3,\n",
    "        \"focus_on\": \"technical\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Analyze text\n",
    "results = await analyzer.analyze(text)\n",
    "\n",
    "# Display results\n",
    "analyzer.display_themes(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing function\n",
    "async def test_keyword_analyzer(text: str, show_debug: bool = True, language: str = \"en\"):\n",
    "    options = AnalysisOptions(\n",
    "        show_confidence=True,\n",
    "        show_evidence=True,\n",
    "        show_keywords=True,\n",
    "        show_raw_data=show_debug,\n",
    "        debug_mode=True,\n",
    "        language=language\n",
    "    )\n",
    "    \n",
    "    results = await analyze_keywords(text, options)\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 14:03:35,710 - src.core.language_processing.factory - DEBUG - Using default configuration\n",
      "2024-11-20 14:03:35,712 - src.core.language_processing.factory - DEBUG - Creating fi processor\n",
      "2024-11-20 14:03:35,716 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:192] - Initialized FileUtils with log level: INFO\n",
      "2024-11-20 14:03:35,716 - src.utils.FileUtils.file_utils - DEBUG - Initialized FileUtils with log level: INFO\n",
      "2024-11-20 14:03:35,719 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:198] - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n",
      "2024-11-20 14:03:35,719 - src.utils.FileUtils.file_utils - DEBUG - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n",
      "2024-11-20 14:03:35,746 - src.core.language_processing.finnish - INFO - Loaded 747 stopwords from c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\configurations\\stop_words\\fi.txt\n",
      "2024-11-20 14:03:35,747 - src.core.language_processing.finnish - DEBUG - Added 36 technical stopwords\n",
      "2024-11-20 14:03:35,749 - src.core.language_processing.finnish - DEBUG - Total Finnish stopwords: 783\n",
      "2024-11-20 14:03:35,750 - src.core.language_processing.finnish - INFO - Detected platform: win32\n",
      "2024-11-20 14:03:35,752 - src.core.language_processing.finnish - DEBUG - System library initialization failed: Could not find module 'libvoikko-1.dll' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "Exception ignored in: <function Voikko.__del__ at 0x000001D765AB59D0>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\tja\\AppData\\Local\\miniconda3\\envs\\semantic-analyzer\\lib\\site-packages\\libvoikko.py\", line 446, in __del__\n",
      "    self.terminate()\n",
      "  File \"c:\\Users\\tja\\AppData\\Local\\miniconda3\\envs\\semantic-analyzer\\lib\\site-packages\\libvoikko.py\", line 476, in terminate\n",
      "    if self.__handle:\n",
      "AttributeError: 'Voikko' object has no attribute '_Voikko__handle'\n",
      "2024-11-20 14:03:35,786 - src.core.language_processing.finnish - INFO - Added C:\\scripts\\Voikko to DLL search path\n",
      "2024-11-20 14:03:35,866 - src.core.language_processing.finnish - INFO - Successfully initialized Voikko with path: C:\\scripts\\Voikko\n",
      "2024-11-20 14:03:35,876 - src.core.language_processing.factory - DEBUG - Using default configuration\n",
      "2024-11-20 14:03:35,877 - src.core.language_processing.factory - DEBUG - Creating fi processor\n",
      "2024-11-20 14:03:35,882 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:192] - Initialized FileUtils with log level: INFO\n",
      "2024-11-20 14:03:35,882 - src.utils.FileUtils.file_utils - DEBUG - Initialized FileUtils with log level: INFO\n",
      "2024-11-20 14:03:35,885 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:198] - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n",
      "2024-11-20 14:03:35,885 - src.utils.FileUtils.file_utils - DEBUG - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n",
      "2024-11-20 14:03:35,895 - src.core.language_processing.finnish - INFO - Loaded 747 stopwords from c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\configurations\\stop_words\\fi.txt\n",
      "2024-11-20 14:03:35,900 - src.core.language_processing.finnish - DEBUG - Added 36 technical stopwords\n",
      "2024-11-20 14:03:35,902 - src.core.language_processing.finnish - DEBUG - Total Finnish stopwords: 783\n",
      "2024-11-20 14:03:35,904 - src.core.language_processing.finnish - INFO - Detected platform: win32\n",
      "2024-11-20 14:03:35,909 - src.core.language_processing.finnish - DEBUG - System library initialization failed: Initialization of Voikko failed: No valid dictionaries were found\n",
      "2024-11-20 14:03:35,911 - src.core.language_processing.finnish - INFO - Added C:\\scripts\\Voikko to DLL search path\n",
      "2024-11-20 14:03:35,917 - src.core.language_processing.finnish - INFO - Successfully initialized Voikko with path: C:\\scripts\\Voikko\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing word: 'para'\n",
      "==================================================\n",
      "Base form: parantua\n",
      "Is verb: True\n",
      "Should keep: False\n",
      "\n",
      "Voikko analysis:\n",
      "  BASEFORM: para\n",
      "  CLASS: nimisana\n",
      "  FSTOUTPUT: [Ln][Xp]para[X]par[Sn][Ny]a\n",
      "  NUMBER: singular\n",
      "  SIJAMUOTO: nimento\n",
      "  STRUCTURE: =pppp\n",
      "  WORDBASES: +para(para)\n",
      "--------------------------------------------------\n",
      "\n",
      "Analyzing word: 'parani'\n",
      "==================================================\n",
      "Base form: None\n",
      "Is verb: True\n",
      "Should keep: False\n",
      "\n",
      "Voikko analysis:\n",
      "  BASEFORM: para\n",
      "  CLASS: nimisana\n",
      "  FSTOUTPUT: [Ln][Xp]para[X]par[Sn][Ny]a[O1y]ni\n",
      "  NUMBER: singular\n",
      "  POSSESSIVE: 1s\n",
      "  SIJAMUOTO: nimento\n",
      "  STRUCTURE: =pppppp\n",
      "  WORDBASES: +para(para)\n",
      "--------------------------------------------------\n",
      "\n",
      "Analyzing word: 'parantua'\n",
      "==================================================\n",
      "Base form: None\n",
      "Is verb: True\n",
      "Should keep: False\n",
      "\n",
      "Voikko analysis:\n",
      "  BASEFORM: parantua\n",
      "  CLASS: teonsana\n",
      "  FSTOUTPUT: [Lt][Xp]parata[X]paran[Xj]tua[X]tu[Tn1][Eb]a\n",
      "  MOOD: A-infinitive\n",
      "  NEGATIVE: both\n",
      "  STRUCTURE: =pppppppp\n",
      "  WORDBASES: +paran(parata)+tua(+tua)\n",
      "--------------------------------------------------\n",
      "\n",
      "Analyzing word: 'kasvu'\n",
      "==================================================\n",
      "Base form: kasvu\n",
      "Is verb: False\n",
      "Should keep: True\n",
      "\n",
      "Voikko analysis:\n",
      "  BASEFORM: kasvu\n",
      "  CLASS: nimisana\n",
      "  FSTOUTPUT: [Ln][Xp]kasvu[X]kasvu[Sn][Ny]\n",
      "  NUMBER: singular\n",
      "  SIJAMUOTO: nimento\n",
      "  STRUCTURE: =ppppp\n",
      "  WORDBASES: +kasvu(kasvu)\n",
      "--------------------------------------------------\n",
      "\n",
      "Analyzing word: 'kasvaa'\n",
      "==================================================\n",
      "Base form: None\n",
      "Is verb: True\n",
      "Should keep: False\n",
      "\n",
      "Voikko analysis:\n",
      "  BASEFORM: kasvaa\n",
      "  CLASS: teonsana\n",
      "  FSTOUTPUT: [Lt][Xp]kasvaa[X]kasv[Tt][Ap][P3][Ny][Ef]aa\n",
      "  MOOD: indicative\n",
      "  NEGATIVE: false\n",
      "  NUMBER: singular\n",
      "  PERSON: 3\n",
      "  STRUCTURE: =pppppp\n",
      "  TENSE: present_simple\n",
      "  WORDBASES: +kasvaa(kasvaa)\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 14:03:38,629 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:192] - Initialized FileUtils with log level: INFO\n",
      "2024-11-20 14:03:38,629 - src.utils.FileUtils.file_utils - DEBUG - Initialized FileUtils with log level: INFO\n",
      "2024-11-20 14:03:38,633 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:198] - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n",
      "2024-11-20 14:03:38,633 - src.utils.FileUtils.file_utils - DEBUG - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n",
      "2024-11-20 14:03:38,629 - src.utils.FileUtils.file_utils - DEBUG - Initialized FileUtils with log level: INFO\n",
      "2024-11-20 14:03:38,633 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:198] - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n",
      "2024-11-20 14:03:38,633 - src.utils.FileUtils.file_utils - DEBUG - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n"
     ]
    }
   ],
   "source": [
    "# from src.nb_helpers.testers import analyze_problematic_words, KeywordTester\n",
    "# Method 1: Use directly with a processor\n",
    "# processor = create_text_processor(language=\"fi\")\n",
    "# problematic_words = [\"para\", \"parani\", \"parantua\", \"kasvu\", \"kasvaa\"]\n",
    "# analyze_problematic_words(processor, problematic_words)\n",
    "\n",
    "# # Method 2: Use through KeywordTester\n",
    "# tester = KeywordTester(language_processor=create_text_processor(language=\"fi\"))\n",
    "# tester.analyze_words([\"para\", \"parani\", \"parantua\", \"kasvu\", \"kasvaa\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.core.language_processing.finnish import FinnishTextProcessor,analyze_problematic_words\n",
    "# Usage example:\n",
    "problematic_words = [\"para\", \"parani\", \"parantua\", \"kasvu\", \"kasvaa\"]\n",
    "analyze_problematic_words(processor, problematic_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 14:04:37,736 - src.nb_helpers.analyzers - DEBUG - Starting keyword analysis\n",
      "2024-11-20 14:04:37,740 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:192] - Initialized FileUtils with log level: INFO\n",
      "2024-11-20 14:04:37,740 - src.utils.FileUtils.file_utils - DEBUG - Initialized FileUtils with log level: INFO\n",
      "2024-11-20 14:04:37,743 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:198] - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n",
      "2024-11-20 14:04:37,743 - src.utils.FileUtils.file_utils - DEBUG - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n",
      "2024-11-20 14:04:37,750 - src.nb_helpers.analyzers - DEBUG - Initialized TextAnalyzer with options: AnalysisOptions(show_confidence=True, show_evidence=True, show_keywords=True, show_raw_data=True, debug_mode=True, language='fi', parameter_file=None)\n",
      "2024-11-20 14:04:37,752 - src.core.language_processing.factory - DEBUG - Using default configuration\n",
      "2024-11-20 14:04:37,753 - src.core.language_processing.factory - DEBUG - Creating fi processor\n",
      "2024-11-20 14:04:37,757 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:192] - Initialized FileUtils with log level: INFO\n",
      "2024-11-20 14:04:37,757 - src.utils.FileUtils.file_utils - DEBUG - Initialized FileUtils with log level: INFO\n",
      "2024-11-20 14:04:37,759 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:198] - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n",
      "2024-11-20 14:04:37,759 - src.utils.FileUtils.file_utils - DEBUG - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n",
      "2024-11-20 14:04:37,770 - src.core.language_processing.finnish - INFO - Loaded 747 stopwords from c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\configurations\\stop_words\\fi.txt\n",
      "2024-11-20 14:04:37,771 - src.core.language_processing.finnish - DEBUG - Added 36 technical stopwords\n",
      "2024-11-20 14:04:37,772 - src.core.language_processing.finnish - DEBUG - Total Finnish stopwords: 783\n",
      "2024-11-20 14:04:37,774 - src.core.language_processing.finnish - INFO - Detected platform: win32\n",
      "2024-11-20 14:04:37,778 - src.core.language_processing.finnish - DEBUG - System library initialization failed: Initialization of Voikko failed: No valid dictionaries were found\n",
      "2024-11-20 14:04:37,780 - src.core.language_processing.finnish - INFO - Added C:\\scripts\\Voikko to DLL search path\n",
      "2024-11-20 14:04:37,786 - src.core.language_processing.finnish - INFO - Successfully initialized Voikko with path: C:\\scripts\\Voikko\n",
      "2024-11-20 14:04:40,063 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:192] - Initialized FileUtils with log level: INFO\n",
      "2024-11-20 14:04:40,063 - src.utils.FileUtils.file_utils - DEBUG - Initialized FileUtils with log level: INFO\n",
      "2024-11-20 14:04:40,066 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:198] - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n",
      "2024-11-20 14:04:40,066 - src.utils.FileUtils.file_utils - DEBUG - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n",
      "2024-11-20 14:04:40,076 - src.nb_helpers.analyzers - DEBUG - Running keyword analysis\n",
      "2024-11-20 14:04:40,077 - src.nb_helpers.testers - DEBUG - KeywordTester starting analysis\n",
      "2024-11-20 14:04:40,078 - src.analyzers.keyword_analyzer - DEBUG - Starting keyword analysis\n",
      "2024-11-20 14:04:40,087 - src.analyzers.keyword_analyzer - DEBUG - Applied compound bonus to liikevaihto\n",
      "2024-11-20 14:04:40,089 - src.analyzers.keyword_analyzer - DEBUG - Applied technical term boost to liikevaihto\n",
      "2024-11-20 14:04:40,093 - src.analyzers.keyword_analyzer - DEBUG - Applied compound bonus to vuosineljännes\n",
      "2024-11-20 14:04:40,098 - src.analyzers.keyword_analyzer - DEBUG - Applied compound bonus to markkinaosuus\n",
      "2024-11-20 14:04:40,101 - src.analyzers.keyword_analyzer - DEBUG - Applied technical term boost to markkinaosuus\n",
      "2024-11-20 14:04:40,205 - src.analyzers.keyword_analyzer - DEBUG - Applied compound bonus to liikevaihto\n",
      "2024-11-20 14:04:40,210 - src.analyzers.keyword_analyzer - DEBUG - Applied technical term boost to liikevaihto\n",
      "2024-11-20 14:04:40,215 - src.analyzers.keyword_analyzer - DEBUG - Applied compound bonus to vuosineljännes\n",
      "2024-11-20 14:04:40,221 - src.analyzers.keyword_analyzer - DEBUG - Applied compound bonus to markkinaosuus\n",
      "2024-11-20 14:04:40,225 - src.analyzers.keyword_analyzer - DEBUG - Applied technical term boost to markkinaosuus\n",
      "2024-11-20 14:04:45,923 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-20 14:04:45,969 - src.analyzers.keyword_analyzer - DEBUG - Got LLM response for keyword analysis\n",
      "2024-11-20 14:04:45,971 - src.analyzers.keyword_analyzer - DEBUG - Processed LLM keyword: liikevaihto\n",
      "2024-11-20 14:04:45,972 - src.analyzers.keyword_analyzer - DEBUG - Processed LLM keyword: kasvu\n",
      "2024-11-20 14:04:45,974 - src.analyzers.keyword_analyzer - DEBUG - Processed LLM keyword: asiakas\n",
      "2024-11-20 14:04:45,975 - src.analyzers.keyword_analyzer - DEBUG - Processed LLM keyword: markkinaosuus\n",
      "2024-11-20 14:04:45,976 - src.analyzers.keyword_analyzer - DEBUG - Processed LLM keyword: pilvipalvelut\n",
      "2024-11-20 14:04:45,981 - src.analyzers.keyword_analyzer - DEBUG - Applied compound bonus to markkinaosuus\n",
      "2024-11-20 14:04:45,985 - src.analyzers.keyword_analyzer - DEBUG - Applied technical term boost to markkinaosuus\n",
      "2024-11-20 14:04:45,988 - src.analyzers.keyword_analyzer - DEBUG - Applied compound bonus to liikevaihto\n",
      "2024-11-20 14:04:45,990 - src.analyzers.keyword_analyzer - DEBUG - Applied technical term boost to liikevaihto\n",
      "2024-11-20 14:04:45,993 - src.analyzers.keyword_analyzer - DEBUG - Applied compound bonus to vuosineljännes\n",
      "2024-11-20 14:04:45,998 - src.analyzers.keyword_analyzer - DEBUG - Applied domain boost (1.15) to liikevaihto\n",
      "2024-11-20 14:04:46,001 - src.analyzers.keyword_analyzer - DEBUG - Applied compound bonus to liikevaihto\n",
      "2024-11-20 14:04:46,003 - src.analyzers.keyword_analyzer - DEBUG - Applied technical term boost to liikevaihto\n",
      "2024-11-20 14:04:46,007 - src.analyzers.keyword_analyzer - DEBUG - Applied domain boost (1.15) to markkinaosuus\n",
      "2024-11-20 14:04:46,010 - src.analyzers.keyword_analyzer - DEBUG - Applied compound bonus to markkinaosuus\n",
      "2024-11-20 14:04:46,012 - src.analyzers.keyword_analyzer - DEBUG - Applied technical term boost to markkinaosuus\n",
      "2024-11-20 14:04:46,014 - src.analyzers.keyword_analyzer - DEBUG - Applied compound bonus to pilvipalvelut\n",
      "2024-11-20 14:04:46,060 - src.analyzers.keyword_analyzer - DEBUG - Analysis complete. Found 7 keywords\n",
      "2024-11-20 14:04:46,062 - src.nb_helpers.testers - DEBUG - KeywordTester analysis complete\n",
      "2024-11-20 14:04:46,063 - src.nb_helpers.analyzers - DEBUG - Keyword analysis completed\n",
      "2024-11-20 14:04:46,066 - src.nb_helpers.analyzers - DEBUG - Formatting results\n",
      "2024-11-20 14:04:46,068 - src.nb_helpers.analyzers - DEBUG - Displaying debug information\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Keywords Found:\n",
      "  • markkinaosuus        [████████████████████] (1.00)\n",
      "  • liikevaihto          [████████████████████] (1.00)\n",
      "  • pilvipalvelut        [████████████████████] (1.00)\n",
      "  • asiakas              [███████████████░░░░░] (0.75)\n",
      "  • kasvu                [███████████████░░░░░] (0.75)\n",
      "  • vuosineljännes       [████████░░░░░░░░░░░░] (0.43)\n",
      "  • segmentti            [█████░░░░░░░░░░░░░░░] (0.26)\n",
      "\n",
      "Debug Information:\n",
      "--------------------\n",
      "{\n",
      "  \"keywords\": [\n",
      "    {\n",
      "      \"keyword\": \"markkinaosuus\",\n",
      "      \"score\": 1.0,\n",
      "      \"domain\": \"business\",\n",
      "      \"compound_parts\": [\n",
      "        \"markkina\",\n",
      "        \"osuus\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"keyword\": \"liikevaihto\",\n",
      "      \"score\": 1.0,\n",
      "      \"domain\": \"business\",\n",
      "      \"compound_parts\": [\n",
      "        \"liike\",\n",
      "        \"vaihto\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"keyword\": \"pilvipalvelut\",\n",
      "      \"score\": 1.0,\n",
      "      \"domain\": \"technical\",\n",
      "      \"compound_parts\": [\n",
      "        \"pilvi\",\n",
      "        \"palvella\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"keyword\": \"asiakas\",\n",
      "      \"score\": 0.7524,\n",
      "      \"domain\": \"business\",\n",
      "      \"compound_parts\": null\n",
      "    },\n",
      "    {\n",
      "      \"keyword\": \"kasvu\",\n",
      "      \"score\": 0.7524,\n",
      "      \"domain\": \"business\",\n",
      "      \"compound_parts\": null\n",
      "    },\n",
      "    {\n",
      "      \"keyword\": \"vuosinelj\\u00e4nnes\",\n",
      "      \"score\": 0.4262860800000001,\n",
      "      \"domain\": null,\n",
      "      \"compound_parts\": [\n",
      "        \"vuosi\",\n",
      "        \"nelj\\u00e4nnes\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"keyword\": \"segmentti\",\n",
      "      \"score\": 0.264,\n",
      "      \"domain\": null,\n",
      "      \"compound_parts\": null\n",
      "    }\n",
      "  ],\n",
      "  \"compound_words\": [\n",
      "    \"markkinaosuus\",\n",
      "    \"liikevaihto\",\n",
      "    \"pilvipalvelut\",\n",
      "    \"vuosinelj\\u00e4nnes\"\n",
      "  ],\n",
      "  \"domain_keywords\": {\n",
      "    \"business\": [\n",
      "      \"markkinaosuus\",\n",
      "      \"liikevaihto\",\n",
      "      \"asiakas\",\n",
      "      \"kasvu\"\n",
      "    ],\n",
      "    \"technical\": [\n",
      "      \"pilvipalvelut\"\n",
      "    ]\n",
      "  },\n",
      "  \"language\": \"fi\",\n",
      "  \"success\": true,\n",
      "  \"error\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# test_case = \"technical_fi_1\"\n",
    "test_case = \"business_fi_1\"\n",
    "\n",
    "results = await test_keyword_analyzer(finnish_texts[test_case], language=\"fi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_case = \"Mixed Domain Content\"\n",
    "results = await test_keyword_analyzer(test_texts[test_case])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run tests\n",
    "for case_name, text in test_texts.items():\n",
    "    print(f\"\\nTesting: {case_name}\")\n",
    "    print(\"=\" * 50)\n",
    "    results = await test_keyword_analyzer(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "previous examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2. Test single language analysis\n",
    "# print(\"\\nSingle Language Analysis:\")\n",
    "# print(\"-\" * 50)\n",
    "\n",
    "# options_en = AnalysisOptions(\n",
    "#     show_confidence=True,\n",
    "#     show_evidence=True,\n",
    "#     show_keywords=True,\n",
    "#     show_raw_data=True,\n",
    "#     debug_mode=True,\n",
    "#     language=\"en\"  # Explicitly set language\n",
    "# )\n",
    "\n",
    "# # Analyze English text\n",
    "# text = example_texts[\"English Technical\"]\n",
    "# results_en = await analyze_keywords(text, options_en)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test Finnish keyword analysis\n",
    "# print(\"\\nTest Finnish keyword analysis:\")\n",
    "# print(\"-\" * 50)\n",
    "\n",
    "# options_fi = AnalysisOptions(\n",
    "#     show_confidence=True,\n",
    "#     show_evidence=True,\n",
    "#     debug_mode=True,\n",
    "#     language=\"fi\"  # Explicitly set language\n",
    "# )\n",
    "\n",
    "# # Analyze Finnish text with auto-detection\n",
    "# text = example_texts[\"Finnish Technical\"]\n",
    "# # results_fi = await analyze_text(text, options_fi)\n",
    "# keywords_fi = await analyze_keywords(text, options_fi)\n",
    "\n",
    "\n",
    "# # # 4. Test batch analysis with mixed languages\n",
    "# # print(\"\\nBatch Analysis with Mixed Languages:\")\n",
    "# # print(\"-\" * 50)\n",
    "\n",
    "# # batch_results = {}\n",
    "# # for name, text in example_texts.items():\n",
    "# #     print(f\"\\nAnalyzing {name}:\")\n",
    "# #     results = await analyze_text(text, options_auto)\n",
    "# #     batch_results[name] = results\n",
    "\n",
    "# # # 5. Test Excel file processing\n",
    "# # from src.nb_helpers.analyzers import analyze_excel_content\n",
    "\n",
    "# # # Create a test DataFrame\n",
    "# # import pandas as pd\n",
    "# # df = pd.DataFrame({\n",
    "# #     \"content\": example_texts.values(),\n",
    "# #     \"type\": [name.split()[0] for name in example_texts.keys()],  # \"English\" or \"Finnish\"\n",
    "# # })\n",
    "\n",
    "# # # Save to temporary Excel file\n",
    "# # temp_excel = \"temp_test_content.xlsx\"\n",
    "# # df.to_excel(temp_excel, index=False)\n",
    "\n",
    "# # print(\"\\nExcel File Analysis:\")\n",
    "# # print(\"-\" * 50)\n",
    "\n",
    "# # await analyze_excel_content(\n",
    "# #     input_file=temp_excel,\n",
    "# #     output_file=\"analysis_results\",\n",
    "# #     content_column=\"content\",\n",
    "# #     parameter_file=\"parameters_en.xlsx\",  # Use our parameter file\n",
    "# #     language_column=\"type\"  # Use type column for language\n",
    "# # )\n",
    "\n",
    "# # # Clean up temporary file\n",
    "# # os.remove(temp_excel)\n",
    "\n",
    "# # # 6. Compare analysis results\n",
    "# # print(\"\\nAnalysis Results Comparison:\")\n",
    "# # print(\"-\" * 50)\n",
    "\n",
    "# # def print_analysis_summary(results: dict, name: str):\n",
    "# #     print(f\"\\n{name}:\")\n",
    "# #     if \"keywords\" in results:\n",
    "# #         keywords = results[\"keywords\"].get(\"keywords\", [])\n",
    "# #         print(f\"Keywords found: {len(keywords)}\")\n",
    "# #         for kw in keywords[:3]:  # Show top 3 keywords\n",
    "# #             print(f\"- {kw.keyword}: {kw.score:.2f}\")\n",
    "    \n",
    "# #     if \"themes\" in results:\n",
    "# #         themes = results[\"themes\"].get(\"themes\", [])\n",
    "# #         print(f\"Themes found: {len(themes)}\")\n",
    "# #         for theme in themes[:2]:  # Show top 2 themes\n",
    "# #             print(f\"- {theme.name}: {theme.confidence:.2f}\")\n",
    "            \n",
    "# #     if \"categories\" in results:\n",
    "# #         categories = results[\"categories\"].get(\"categories\", [])\n",
    "# #         print(f\"Categories found: {len(categories)}\")\n",
    "# #         for cat in categories[:2]:  # Show top 2 categories\n",
    "# #             print(f\"- {cat.name}: {cat.confidence:.2f}\")\n",
    "\n",
    "# # for name, results in batch_results.items():\n",
    "# #     print_analysis_summary(results, name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run environment verification\n",
    "# from src.nb_helpers.environment import verify_environment\n",
    "# verify_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test logging\n",
    "logger = logging.getLogger(\"src.analyzers.keyword_analyzer\")\n",
    "logger.debug(\"Testing keyword analyzer logging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: following are not working with the new parameter handling model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example texts in different languages\n",
    "example_texts = {\n",
    "    \"English Technical\": \"\"\"\n",
    "        The cloud migration project improved system scalability while reducing costs.\n",
    "        New DevOps practices streamlined the deployment pipeline significantly.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Finnish Technical\": \"\"\"\n",
    "        Pilvipalveluihin siirtyminen paransi järjestelmän skaalautuvuutta ja vähensi kustannuksia.\n",
    "        Uudet DevOps-käytännöt tehostivat merkittävästi käyttöönottoprosessia.\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "# Analyze with automatic language detection\n",
    "for name, text in example_texts.items():\n",
    "    print(f\"\\nAnalyzing {name}:\")\n",
    "    results = await analyze_text(text, options)\n",
    "\n",
    "# Example with specific language and parameters\n",
    "fi_options = AnalysisOptions(\n",
    "    show_confidence=True,\n",
    "    show_evidence=True,\n",
    "    debug_mode=True,\n",
    "    language=\"fi\",\n",
    "    parameter_file=\"finnish_params.yaml\"\n",
    ")\n",
    "\n",
    "# Analyze Finnish text with specific parameters\n",
    "fi_results = await analyze_text(example_texts[\"Finnish Technical\"], fi_options)\n",
    "\n",
    "# Batch process Excel file with language detection\n",
    "await analyze_excel_content(\n",
    "    input_file=\"multilingual_texts.xlsx\",\n",
    "    output_file=\"analysis_results\",\n",
    "    content_column=\"content\",\n",
    "    parameter_file=\"analysis_params.yaml\",\n",
    "    language_column=\"language\"  # Optional column specifying language\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ## Analysis Functions\n",
    "\n",
    "### Single Analysis with Debug Output\n",
    "Run detailed analysis for a single text: -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_texts = {\n",
    "    \"Business Analysis\": \"\"\"\n",
    "        Q3 revenue increased by 15% with strong growth in enterprise sales.\n",
    "        Customer retention improved while acquisition costs decreased.\n",
    "        New market expansion initiatives are showing positive early results.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Technical Content\": \"\"\"\n",
    "        The application uses microservices architecture with containerized deployments.\n",
    "        Data processing pipeline incorporates machine learning models for prediction.\n",
    "        System monitoring ensures high availability and performance metrics.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Mixed Content\": \"\"\"\n",
    "        The IT department's cloud migration project reduced infrastructure costs by 25%.\n",
    "        DevOps implementation improved deployment frequency while maintaining quality.\n",
    "        Monthly recurring revenue from SaaS products grew steadily.\n",
    "    \"\"\",\n",
    "    \"koulutus\":\n",
    "    \"\"\"\n",
    "        Verkko-oppimisalusta sisältää interaktiivisia moduuleja ja oman tahdin edistymisen seurannan. \n",
    "        Virtuaaliluokat mahdollistavat reaaliaikaisen yhteistyön opiskelijoiden ja ohjaajien välillä. \n",
    "        Digitaaliset arviointityökalut antavat välitöntä palautetta oppimistuloksista.\n",
    "    \"\"\",\n",
    "    \"tekninen\":\n",
    "    \"\"\"\n",
    "        Koneoppimismalleja koulutetaan suurilla datajoukolla tunnistamaan kaavoja. \n",
    "        Neuroverkon arkkitehtuuri sisältää useita kerroksia piirteiden erottamiseen. \n",
    "        Datan esikäsittely ja piirteiden suunnittelu ovat keskeisiä vaiheita prosessissa.\n",
    "\n",
    "    \"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # New imports\n",
    "# from src.core.language_parameters import LanguageParameterManager\n",
    "\n",
    "# # Initialize parameter manager\n",
    "# param_manager = LanguageParameterManager()\n",
    "\n",
    "# # Example analysis with automatic language detection\n",
    "# text_en = \"Cloud computing enables scalable infrastructure deployment.\"\n",
    "# text_fi = \"Pilvipalvelut mahdollistavat skaalautuvan infrastruktuurin käyttöönoton.\"\n",
    "\n",
    "# # Analyze with automatic language detection and default parameters\n",
    "# async def analyze_text_with_language(text: str, parameter_file: Optional[str] = None):\n",
    "#     \"\"\"Analyze text with automatic language handling.\"\"\"\n",
    "#     # Get language-specific parameters\n",
    "#     params = param_manager.get_parameters(text, parameter_file)\n",
    "    \n",
    "#     # Create analyzers with parameters\n",
    "#     keyword_analyzer = KeywordAnalyzer(config=params.dict())\n",
    "#     theme_analyzer = ThemeAnalyzer(config=params.dict())\n",
    "#     category_analyzer = CategoryAnalyzer(config=params.dict())\n",
    "    \n",
    "#     # Run analysis\n",
    "#     results = {\n",
    "#         \"keywords\": await keyword_analyzer.analyze(text),\n",
    "#         \"themes\": await theme_analyzer.analyze(text),\n",
    "#         \"categories\": await category_analyzer.analyze(text)\n",
    "#     }\n",
    "    \n",
    "#     return results\n",
    "\n",
    "# # Example with Excel parameters\n",
    "# async def analyze_batch_with_excel_params(texts: List[str], excel_params: str):\n",
    "#     \"\"\"Analyze texts using parameters from Excel.\"\"\"\n",
    "#     # Load language-specific parameters\n",
    "#     params_by_lang = param_manager.load_excel_parameters(excel_params)\n",
    "    \n",
    "#     results = []\n",
    "#     for text in texts:\n",
    "#         # Detect language\n",
    "#         lang = param_manager.detect_language(text)\n",
    "#         # Get parameters for language\n",
    "#         params = params_by_lang.get(lang, param_manager.get_parameters(text))\n",
    "        \n",
    "#         # Create analyzer with language-specific parameters\n",
    "#         analyzer = KeywordAnalyzer(config=params.dict())\n",
    "#         result = await analyzer.analyze(text)\n",
    "#         results.append(result)\n",
    "    \n",
    "#     return results\n",
    "\n",
    "# # Example usage:\n",
    "# # With default parameters\n",
    "# results_en = await analyze_text_with_language(text_en)\n",
    "\n",
    "# # With parameter file\n",
    "# results_fi = await analyze_text_with_language(text_fi, \"finnish_params.yaml\")\n",
    "\n",
    "# # With Excel parameters\n",
    "# texts = [text_en, text_fi]\n",
    "# batch_results = await analyze_batch_with_excel_params(texts, \"analysis_params.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = example_texts[\"Mixed Content\"]\n",
    "# text = example_texts[\"koulutussisältö\"]\n",
    "# Debug specific analyzer\n",
    "\n",
    "# Example usage\n",
    "text = example_texts[\"Mixed Content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await analyze_keywords(text, options=options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await analyze_themes(text, options=options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await analyze_categories(text, options=options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or run full pipeline with debug info\n",
    "await debug_full_pipeline(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Processing from Excel\n",
    "Process multiple texts from Excel file:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await analyze_excel_content(\n",
    "    input_file=\"test_content.xlsx\",  # Input Excel file path\n",
    "    output_file=\"analysis_results\",  # Output filename (without extension)\n",
    "    content_column=\"content\"         # Column containing text to analyze\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "- Configure analyzers using parameter files\n",
    "- Control output detail with DebugOptions\n",
    "- Set logging level for verbosity control\n",
    "\n",
    "## Example Outputs\n",
    "The analysis provides:\n",
    "- Keywords with confidence scores\n",
    "- Theme identification and descriptions\n",
    "- Category classification with evidence\n",
    "- Confidence visualizations with Unicode bars\n",
    "\n",
    "## Notes\n",
    "- Set logging level to WARNING to minimize output\n",
    "- Use debug functions for detailed analysis inspection\n",
    "- Excel output combines all analysis types"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semantic-analyzer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
