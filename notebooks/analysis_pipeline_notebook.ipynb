{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Analysis Pipeline\n",
    "\n",
    "This notebook demonstrates the semantic text analysis capabilities using our custom analyzers.\n",
    "\n",
    "## Setup\n",
    "Import required packages and configure the environment:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At start of notebook\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = str(Path().resolve().parent)\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 15:51:54,790 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:192] - Initialized FileUtils with log level: INFO\n",
      "2024-11-21 15:51:54,791 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:198] - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n",
      "2024-11-21 15:51:54,798 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:192] - Initialized FileUtils with log level: INFO\n",
      "2024-11-21 15:51:54,799 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:198] - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Check Results:\n",
      "==================================================\n",
      "\n",
      "Basic Setup:\n",
      "-----------\n",
      "✓ Project root in path\n",
      "✓ FileUtils initialized\n",
      "✓ .env file loaded\n",
      "\n",
      "Environment Variables:\n",
      "---------------------\n",
      "✓ OPENAI_API_KEY set\n",
      "✓ ANTHROPIC_API_KEY set\n",
      "\n",
      "Project Structure:\n",
      "-----------------\n",
      "✓ Raw data exists\n",
      "✓ Processed data exists\n",
      "✓ Configuration exists\n",
      "✓ Main config.yaml exists\n",
      "\n",
      "==================================================\n",
      "Environment Status: Ready ✓\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import logging\n",
    "# from src.nb_helpers.logging import configure_logging\n",
    "\n",
    "# Set up environment with DEBUG level\n",
    "from src.nb_helpers.environment import setup_notebook_env, verify_environment\n",
    "setup_notebook_env(log_level=\"DEBUG\")\n",
    "\n",
    "# Any verification needed will maintain DEBUG level\n",
    "verify_environment(log_level=\"DEBUG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary components\n",
    "from src.loaders.parameter_handler import ParameterHandler\n",
    "from src.nb_helpers.analyzers import (\n",
    "    analyze_keywords,\n",
    "    analyze_themes,\n",
    "    analyze_categories,\n",
    "    analyze_text,\n",
    "    AnalysisOptions\n",
    ")\n",
    "\n",
    "from scripts.migrate_parameters import create_example_parameters\n",
    "from src.nb_helpers.logging import configure_logging, verify_logging_setup_with_hierarchy, reset_debug_logging\n",
    "from src.loaders.parameter_handler import ParameterHandler, get_parameter_file_path, verify_parameter_file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 15:51:54,912 - src.nb_helpers.logging - DEBUG - Logging configured at DEBUG level\n"
     ]
    }
   ],
   "source": [
    "# Set initial logging\n",
    "configure_logging(level=\"DEBUG\")\n",
    "# Keep HTTP loggers at INFO\n",
    "for name in [\"httpx\", \"httpcore\", \"openai\", \"anthropic\"]:\n",
    "    logging.getLogger(name).setLevel(logging.INFO)\n",
    "    \n",
    "# verify_logging_setup_with_hierarchy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detailed_logging_info = True\n",
    "# if detailed_logging_info:\n",
    "#     from src.nb_helpers.logging import verify_logging_setup_with_hierarchy\n",
    "#     # Configure logging\n",
    "#     # configure_logging(level=\"DEBUG\")\n",
    "#     # Verify with detailed information\n",
    "#     verify_logging_setup_with_hierarchy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example texts in different languages\n",
    "example_texts = {\n",
    "    \"English Technical\": \"\"\"\n",
    "        The cloud migration project improved system scalability while reducing costs.\n",
    "        New DevOps practices streamlined the deployment pipeline significantly.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Finnish Technical\": \"\"\"\n",
    "        Pilvipalveluihin siirtyminen paransi järjestelmän skaalautuvuutta ja vähensi kustannuksia.\n",
    "        Uudet DevOps-käytännöt tehostivat merkittävästi käyttöönottoprosessia.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"English Business\": \"\"\"\n",
    "        Q3 financial results show 15% revenue growth and improved profit margins.\n",
    "        Customer acquisition costs decreased while retention rates increased.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Finnish Business\": \"\"\"\n",
    "        Q3 taloudelliset tulokset osoittavat 15% liikevaihdon kasvun ja parantuneet katteet.\n",
    "        Asiakashankinnan kustannukset laskivat ja asiakaspysyvyys parani.\n",
    "    \"\"\"\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and load parameters\n",
    "params_file_name = \"parameters_en.xlsx\"\n",
    "\n",
    "# Get the full parameter file path\n",
    "params_file = get_parameter_file_path(params_file_name)\n",
    "\n",
    "# Create file if it doesn't exist\n",
    "if not params_file.exists():\n",
    "    params_file = create_example_parameters(params_file_name)\n",
    "    print(f\"Created parameter file at: {params_file}\")\n",
    "else:\n",
    "    print(f\"Using existing parameter file at: {params_file}\")\n",
    "\n",
    "# Verify the file\n",
    "verify_parameter_file(params_file)\n",
    "\n",
    "# Load parameters\n",
    "handler = ParameterHandler(params_file_name)  # Can now use just the file name\n",
    "params = handler.get_parameters()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLoaded parameters:\")\n",
    "# params.print()  # Uses the new print method\n",
    "\n",
    "# Or just\n",
    "print(params)  # Uses the new __str__ method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create example files\n",
    "\n",
    "create_new_example_files=False\n",
    "\n",
    "if create_new_example_files:\n",
    "    en_params = create_example_parameters(\"parameters_en.xlsx\", \"en\")\n",
    "    fi_params = create_example_parameters(\"parameters_fi.xlsx\", \"fi\")\n",
    "    print(f\"Created parameter files:\\n- {en_params}\\n- {fi_params}\")\n",
    "    print(f\"Created parameter files:\\n- {en_params}\\n- {fi_params}\")\n",
    "else:\n",
    "    print(\"Not creating new example files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check parameter validation\n",
    "print(\"Parameter Validation:\")\n",
    "print(\"-\" * 50)\n",
    "is_valid, warnings, errors = handler.validate()\n",
    "if warnings:\n",
    "    print(\"\\nWarnings:\")\n",
    "    for warning in warnings:\n",
    "        print(f\"- {warning}\")\n",
    "if not is_valid:\n",
    "    print(\"\\nErrors:\")\n",
    "    for error in errors:\n",
    "        print(f\"- {error}\")\n",
    "else:\n",
    "    print(\"\\nParameters validated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases\n",
    "test_texts = {\n",
    "    \"Technical Compound Terms\": \"\"\"\n",
    "        The cloud migration project improved system scalability.\n",
    "        DevOps practices streamlined the deployment pipeline.\n",
    "        Our microservices architecture enables API integrations.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Mixed Domain Content\": \"\"\"\n",
    "        The IT department's infrastructure costs decreased by 25%\n",
    "        after implementing cloud-native solutions. Monthly recurring\n",
    "        revenue from SaaS products grew steadily while deployment\n",
    "        frequency improved.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Business Focus\": \"\"\"\n",
    "        Market analysis shows 15% revenue growth in Q3.\n",
    "        Customer acquisition costs decreased while retention rates\n",
    "        increased. Strategic partnerships drove innovation.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Multiple Compounds\": \"\"\"\n",
    "        Machine learning models process real-time data streams.\n",
    "        The CI/CD pipeline integrates automated testing workflows.\n",
    "        Cloud-based infrastructure supports multi-region deployments.\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "finnish_texts = {\n",
    "    \"technical_fi_1\":\"Pilvipalveluiden käyttöönotto tehosti järjestelmän skaalautuvuutta merkittävästi. DevOps-prosessit nopeuttivat julkaisusykliä ja automatisoivat laadunvarmistusta. Kuukausittainen tilaustuotto SaaS-ratkaisuista kasvoi 25%.\",\n",
    "    \"technical_fi_2\":\"Mikropalveluarkkitehtuuri mahdollisti järjestelmän modulaarisen kehityksen. Konttiteknologian avulla saavutettiin parempi resurssien käyttöaste ja joustavampi ylläpito. Rajapintojen dokumentointi helpotti integraatioiden toteuttamista.\",\n",
    "    \"technical_fi_3\":\"Tekoälypohjaiset ennusteet auttoivat optimoimaan kuormantasausta. Pilvinatiivi lähestymistapa vähensi infrastruktuurikustannuksia ja paransi vikasietoisuutta. Monitorointi tarjosi reaaliaikaista näkyvyyttä suorituskykyyn.\",\n",
    "    \"business_fi_1\":\"Liikevaihdon kasvu vahvistui kolmannella vuosineljänneksellä 15 prosenttiin. Asiakashankinnan kustannukset laskivat samalla kun asiakaspysyvyys parani. Markkinaosuus kasvoi erityisesti pilvipalveluiden segmentissä.\",\n",
    "    \"business_fi_2\":\"Analytiikkatyökalut paljastivat uusia käyttäytymismalleja asiakasrajapinnassa. Toistuvaislaskutuksen osuus kokonaistuotoista nousi 75 prosenttiin. Automaattinen raportointi tehosti päätöksentekoa.\",\n",
    "    \"business_fi_3\":\"Uudet tuotelanseeraukset vahvistivat kilpailuasemaa. Strategiset kumppanuudet mahdollistivat laajentumisen uusille markkina-alueille. Resurssien kohdentaminen tuotekehitykseen tuotti merkittävää kasvua.\"\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.analyzers import KeywordAnalyzer, ThemeAnalyzer, CategoryAnalyzer #, TextAnalyzer\n",
    "from src.core.language_processing import create_text_processor\n",
    "\n",
    "# from src.analyzers import CategoryAnalyzer\n",
    "# from src.core.language_processing import create_text_processor\n",
    "from src.loaders.models import CategoryConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Category Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create language processor\n",
    "processor = create_text_processor(language=\"fi\")\n",
    "\n",
    "# Define categories\n",
    "categories = {\n",
    "    \"technical_infrastructure\": CategoryConfig(\n",
    "        description=\"Technical infrastructure and cloud solutions\",\n",
    "        keywords=[\"pilvipalvelu\", \"infrastruktuuri\", \"skaalautuvuus\"],\n",
    "        threshold=0.5\n",
    "    ),\n",
    "    \"business_performance\": CategoryConfig(\n",
    "        description=\"Business performance metrics\",\n",
    "        keywords=[\"liikevaihto\", \"kasvu\", \"markkinaosuus\"],\n",
    "        threshold=0.5\n",
    "    )\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_case = \"technical_fi_1\"\n",
    "test_case = \"business_fi_1\"\n",
    "text= finnish_texts[test_case]\n",
    "\n",
    "\n",
    "\n",
    "# test_case = \"technical_fi_1\"\n",
    "# test_case = \"Mixed Domain Content\"\n",
    "# text= test_texts[test_case]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize analyzer\n",
    "analyzer = CategoryAnalyzer(\n",
    "    categories=categories,\n",
    "    language_processor=processor,\n",
    "    config={\"min_confidence\": 0.3}\n",
    ")\n",
    "\n",
    "# Analyze text\n",
    "results = await analyzer.analyze(text)\n",
    "\n",
    "# Display results\n",
    "analyzer.display_categories(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theme analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "technical_text = \"\"\"\n",
    "Pilvipalveluiden käyttöönotto tehosti järjestelmän skaalautuvuutta merkittävästi. \n",
    "DevOps-prosessit nopeuttivat julkaisusykliä ja automatisoivat laadunvarmistusta. \n",
    "Kuukausittainen tilaustuotto SaaS-ratkaisuista kasvoi 25%.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "technical_text_2 = \"Koneoppimismalleja koulutetaan suurilla datajoukolla tunnistamaan kaavoja. Neuroverkon arkkitehtuuri sisältää useita kerroksia piirteiden erottamiseen. Datan esikäsittely ja piirteiden suunnittelu ovat keskeisiä vaiheita prosessissa.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create language processor\n",
    "processor = create_text_processor(language=\"en\")\n",
    "\n",
    "# Initialize analyzer\n",
    "analyzer = ThemeAnalyzer(\n",
    "    language_processor=processor,\n",
    "    config={\n",
    "        \"max_themes\": 3,\n",
    "        \"min_confidence\": 0.3,\n",
    "        \"focus_on\": \"technical\"\n",
    "    }\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_case = \"technical_fi_1\"\n",
    "test_case = \"business_fi_1\"\n",
    "text= finnish_texts[test_case]\n",
    "\n",
    "\n",
    "\n",
    "# test_case = \"technical_fi_1\"\n",
    "# test_case = \"Mixed Domain Content\"\n",
    "# text= test_texts[test_case]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze text\n",
    "# results = await analyzer.analyze(technical_text)\n",
    "results = await analyzer.analyze(text)\n",
    "\n",
    "# Display results\n",
    "analyzer.display_themes(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing function\n",
    "async def test_keyword_analyzer(text: str, show_debug: bool = True, language: str = \"en\"):\n",
    "    options = AnalysisOptions(\n",
    "        show_confidence=True,\n",
    "        show_evidence=True,\n",
    "        show_keywords=True,\n",
    "        show_raw_data=show_debug,\n",
    "        debug_mode=True,\n",
    "        language=language\n",
    "    )\n",
    "    \n",
    "    results = await analyze_keywords(text, options)\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 15:55:45,203 - src.nb_helpers.analyzers - DEBUG - Starting keyword analysis\n",
      "2024-11-21 15:55:45,207 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:192] - Initialized FileUtils with log level: INFO\n",
      "2024-11-21 15:55:45,207 - src.utils.FileUtils.file_utils - DEBUG - Initialized FileUtils with log level: INFO\n",
      "2024-11-21 15:55:45,210 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:198] - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n",
      "2024-11-21 15:55:45,210 - src.utils.FileUtils.file_utils - DEBUG - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n",
      "2024-11-21 15:55:45,217 - src.nb_helpers.analyzers - DEBUG - Initialized TextAnalyzer with options: AnalysisOptions(show_confidence=True, show_evidence=True, show_keywords=True, show_raw_data=True, debug_mode=True, language='fi', parameter_file=None)\n",
      "2024-11-21 15:55:45,218 - src.core.language_processing.factory - DEBUG - Using default configuration\n",
      "2024-11-21 15:55:45,220 - src.core.language_processing.factory - DEBUG - Creating text processor for language: fi\n",
      "2024-11-21 15:55:45,222 - src.core.language_processing.factory - DEBUG - Creating fi processor\n",
      "2024-11-21 15:55:45,225 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:192] - Initialized FileUtils with log level: INFO\n",
      "2024-11-21 15:55:45,225 - src.utils.FileUtils.file_utils - DEBUG - Initialized FileUtils with log level: INFO\n",
      "2024-11-21 15:55:45,227 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:198] - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n",
      "2024-11-21 15:55:45,227 - src.utils.FileUtils.file_utils - DEBUG - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n",
      "2024-11-21 15:55:45,236 - src.core.language_processing.finnish - INFO - Loaded 747 stopwords from c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\configurations\\stop_words\\fi.txt\n",
      "2024-11-21 15:55:45,237 - src.core.language_processing.finnish - DEBUG - Added 36 technical stopwords\n",
      "2024-11-21 15:55:45,239 - src.core.language_processing.finnish - DEBUG - Total Finnish stopwords: 783\n",
      "2024-11-21 15:55:45,241 - src.core.language_processing.finnish - INFO - Detected platform: win32\n",
      "2024-11-21 15:55:45,245 - src.core.language_processing.finnish - DEBUG - System library initialization failed: Initialization of Voikko failed: No valid dictionaries were found\n",
      "2024-11-21 15:55:45,247 - src.core.language_processing.finnish - INFO - Added C:\\scripts\\Voikko to DLL search path\n",
      "2024-11-21 15:55:45,252 - src.core.language_processing.finnish - INFO - Successfully initialized Voikko with path: C:\\scripts\\Voikko\n",
      "2024-11-21 15:55:46,840 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:192] - Initialized FileUtils with log level: INFO\n",
      "2024-11-21 15:55:46,840 - src.utils.FileUtils.file_utils - DEBUG - Initialized FileUtils with log level: INFO\n",
      "2024-11-21 15:55:46,842 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:198] - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n",
      "2024-11-21 15:55:46,842 - src.utils.FileUtils.file_utils - DEBUG - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n",
      "2024-11-21 15:55:46,854 - src.nb_helpers.analyzers - DEBUG - Running keyword analysis\n",
      "2024-11-21 15:55:46,856 - src.nb_helpers.testers - DEBUG - KeywordTester starting analysis\n",
      "2024-11-21 15:55:46,857 - src.analyzers.keyword_analyzer - DEBUG - Starting keyword analysis\n",
      "2024-11-21 15:55:46,875 - src.analyzers.keyword_analyzer - DEBUG - Applied compound bonus to koneoppimismalli\n",
      "2024-11-21 15:55:46,878 - src.analyzers.keyword_analyzer - DEBUG - Applied technical term boost to koneoppimismalli\n",
      "2024-11-21 15:55:46,880 - src.analyzers.keyword_analyzer - DEBUG - Applied compound bonus to datajoukko\n",
      "2024-11-21 15:55:46,882 - src.analyzers.keyword_analyzer - DEBUG - Applied technical term boost to datajoukko\n",
      "2024-11-21 15:55:46,893 - src.analyzers.keyword_analyzer - DEBUG - Applied technical term boost to data\n",
      "2024-11-21 15:55:46,920 - src.analyzers.keyword_analyzer - DEBUG - Applied compound bonus to koneoppimismalli\n",
      "2024-11-21 15:55:46,926 - src.analyzers.keyword_analyzer - DEBUG - Applied technical term boost to koneoppimismalli\n",
      "2024-11-21 15:55:46,929 - src.analyzers.keyword_analyzer - DEBUG - Applied compound bonus to datajoukko\n",
      "2024-11-21 15:55:46,932 - src.analyzers.keyword_analyzer - DEBUG - Applied technical term boost to datajoukko\n",
      "2024-11-21 15:55:46,941 - src.analyzers.keyword_analyzer - DEBUG - Applied technical term boost to data\n",
      "2024-11-21 15:55:52,746 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-21 15:55:52,754 - src.analyzers.keyword_analyzer - DEBUG - Got LLM response for keyword analysis\n",
      "2024-11-21 15:55:52,755 - src.analyzers.keyword_analyzer - DEBUG - Processed LLM keyword: koneoppimismalli\n",
      "2024-11-21 15:55:52,756 - src.analyzers.keyword_analyzer - DEBUG - Processed LLM keyword: datajoukko\n",
      "2024-11-21 15:55:52,758 - src.analyzers.keyword_analyzer - DEBUG - Processed LLM keyword: neuroverkon arkkitehtuuri\n",
      "2024-11-21 15:55:52,759 - src.analyzers.keyword_analyzer - DEBUG - Processed LLM keyword: datan esikäsittely\n",
      "2024-11-21 15:55:52,761 - src.analyzers.keyword_analyzer - DEBUG - Processed LLM keyword: piirteiden suunnittelu\n",
      "2024-11-21 15:55:52,764 - src.analyzers.keyword_analyzer - DEBUG - Applied technical term boost to data\n",
      "2024-11-21 15:55:52,767 - src.analyzers.keyword_analyzer - DEBUG - Applied compound bonus to koneoppimismalli\n",
      "2024-11-21 15:55:52,769 - src.analyzers.keyword_analyzer - DEBUG - Applied technical term boost to koneoppimismalli\n",
      "2024-11-21 15:55:52,772 - src.analyzers.keyword_analyzer - DEBUG - Applied compound bonus to datajoukko\n",
      "2024-11-21 15:55:52,775 - src.analyzers.keyword_analyzer - DEBUG - Applied technical term boost to datajoukko\n",
      "2024-11-21 15:55:52,785 - src.analyzers.keyword_analyzer - DEBUG - Applied compound bonus to koneoppimismalli\n",
      "2024-11-21 15:55:52,788 - src.analyzers.keyword_analyzer - DEBUG - Applied technical term boost to koneoppimismalli\n",
      "2024-11-21 15:55:52,791 - src.analyzers.keyword_analyzer - DEBUG - Applied compound bonus to datajoukko\n",
      "2024-11-21 15:55:52,793 - src.analyzers.keyword_analyzer - DEBUG - Applied technical term boost to datajoukko\n",
      "2024-11-21 15:55:52,929 - src.analyzers.keyword_analyzer - DEBUG - Analysis complete. Found 10 keywords\n",
      "2024-11-21 15:55:52,931 - src.nb_helpers.testers - DEBUG - KeywordTester analysis complete\n",
      "2024-11-21 15:55:52,932 - src.nb_helpers.analyzers - DEBUG - Keyword analysis completed\n",
      "2024-11-21 15:55:52,933 - src.nb_helpers.analyzers - DEBUG - Formatting results\n",
      "2024-11-21 15:55:52,940 - src.nb_helpers.analyzers - DEBUG - Displaying debug information\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Keywords Found:\n",
      "  • koneoppimismalli     [████████████████████] (1.00)\n",
      "  • datajoukko           [████████████████████] (1.00)\n",
      "  • neuroverkon arkkitehtuuri [███████████████░░░░░] (0.75)\n",
      "  • datan esikäsittely   [███████████████░░░░░] (0.75)\n",
      "  • piirteiden suunnittelu [███████████████░░░░░] (0.75)\n",
      "  • piirre               [██████████░░░░░░░░░░] (0.53)\n",
      "  • data                 [███████░░░░░░░░░░░░░] (0.38)\n",
      "  • arkkitehtuuri        [██████░░░░░░░░░░░░░░] (0.32)\n",
      "  • esikäsittely         [██████░░░░░░░░░░░░░░] (0.32)\n",
      "  • suunnittelu          [██████░░░░░░░░░░░░░░] (0.32)\n",
      "\n",
      "Debug Information:\n",
      "--------------------\n",
      "{\n",
      "  \"keywords\": [\n",
      "    {\n",
      "      \"keyword\": \"koneoppimismalli\",\n",
      "      \"score\": 1.0,\n",
      "      \"domain\": \"technical\",\n",
      "      \"compound_parts\": [\n",
      "        \"kone\",\n",
      "        \"oppia\",\n",
      "        \"malli\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"keyword\": \"datajoukko\",\n",
      "      \"score\": 1.0,\n",
      "      \"domain\": \"technical\",\n",
      "      \"compound_parts\": [\n",
      "        \"data\",\n",
      "        \"joukko\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"keyword\": \"neuroverkon arkkitehtuuri\",\n",
      "      \"score\": 0.7524,\n",
      "      \"domain\": \"technical\",\n",
      "      \"compound_parts\": null\n",
      "    },\n",
      "    {\n",
      "      \"keyword\": \"datan esik\\u00e4sittely\",\n",
      "      \"score\": 0.7524,\n",
      "      \"domain\": \"technical\",\n",
      "      \"compound_parts\": null\n",
      "    },\n",
      "    {\n",
      "      \"keyword\": \"piirteiden suunnittelu\",\n",
      "      \"score\": 0.7524,\n",
      "      \"domain\": \"technical\",\n",
      "      \"compound_parts\": null\n",
      "    },\n",
      "    {\n",
      "      \"keyword\": \"piirre\",\n",
      "      \"score\": 0.528,\n",
      "      \"domain\": null,\n",
      "      \"compound_parts\": null\n",
      "    },\n",
      "    {\n",
      "      \"keyword\": \"data\",\n",
      "      \"score\": 0.3833280000000001,\n",
      "      \"domain\": null,\n",
      "      \"compound_parts\": null\n",
      "    },\n",
      "    {\n",
      "      \"keyword\": \"arkkitehtuuri\",\n",
      "      \"score\": 0.3168,\n",
      "      \"domain\": null,\n",
      "      \"compound_parts\": null\n",
      "    },\n",
      "    {\n",
      "      \"keyword\": \"esik\\u00e4sittely\",\n",
      "      \"score\": 0.3168,\n",
      "      \"domain\": null,\n",
      "      \"compound_parts\": null\n",
      "    },\n",
      "    {\n",
      "      \"keyword\": \"suunnittelu\",\n",
      "      \"score\": 0.3168,\n",
      "      \"domain\": null,\n",
      "      \"compound_parts\": null\n",
      "    }\n",
      "  ],\n",
      "  \"compound_words\": [\n",
      "    \"koneoppimismalli\",\n",
      "    \"datajoukko\"\n",
      "  ],\n",
      "  \"domain_keywords\": {\n",
      "    \"technical\": [\n",
      "      \"koneoppimismalli\",\n",
      "      \"datajoukko\",\n",
      "      \"neuroverkon arkkitehtuuri\",\n",
      "      \"datan esik\\u00e4sittely\",\n",
      "      \"piirteiden suunnittelu\"\n",
      "    ]\n",
      "  },\n",
      "  \"language\": \"fi\",\n",
      "  \"success\": true,\n",
      "  \"error\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "results = await test_keyword_analyzer(technical_text_2, language=\"fi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 15:52:31,403 - src.nb_helpers.analyzers - DEBUG - Starting keyword analysis\n",
      "2024-11-21 15:52:31,407 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:192] - Initialized FileUtils with log level: INFO\n",
      "2024-11-21 15:52:31,407 - src.utils.FileUtils.file_utils - DEBUG - Initialized FileUtils with log level: INFO\n",
      "2024-11-21 15:52:31,409 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:198] - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n",
      "2024-11-21 15:52:31,409 - src.utils.FileUtils.file_utils - DEBUG - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n",
      "2024-11-21 15:52:31,416 - src.nb_helpers.analyzers - DEBUG - Initialized TextAnalyzer with options: AnalysisOptions(show_confidence=True, show_evidence=True, show_keywords=True, show_raw_data=True, debug_mode=True, language='fi', parameter_file=None)\n",
      "2024-11-21 15:52:31,419 - src.core.language_processing.factory - DEBUG - Using default configuration\n",
      "2024-11-21 15:52:31,420 - src.core.language_processing.factory - DEBUG - Creating text processor for language: fi\n",
      "2024-11-21 15:52:31,421 - src.core.language_processing.factory - DEBUG - Creating fi processor\n",
      "2024-11-21 15:52:31,424 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:192] - Initialized FileUtils with log level: INFO\n",
      "2024-11-21 15:52:31,424 - src.utils.FileUtils.file_utils - DEBUG - Initialized FileUtils with log level: INFO\n",
      "2024-11-21 15:52:31,426 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:198] - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n",
      "2024-11-21 15:52:31,426 - src.utils.FileUtils.file_utils - DEBUG - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n",
      "2024-11-21 15:52:31,453 - src.core.language_processing.finnish - INFO - Loaded 747 stopwords from c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\configurations\\stop_words\\fi.txt\n",
      "2024-11-21 15:52:31,455 - src.core.language_processing.finnish - DEBUG - Added 36 technical stopwords\n",
      "2024-11-21 15:52:31,456 - src.core.language_processing.finnish - DEBUG - Total Finnish stopwords: 783\n",
      "2024-11-21 15:52:31,458 - src.core.language_processing.finnish - INFO - Detected platform: win32\n",
      "2024-11-21 15:52:31,460 - src.core.language_processing.finnish - DEBUG - System library initialization failed: Could not find module 'libvoikko-1.dll' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "Exception ignored in: <function Voikko.__del__ at 0x00000245D564F0D0>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\tja\\AppData\\Local\\miniconda3\\envs\\semantic-analyzer\\lib\\site-packages\\libvoikko.py\", line 446, in __del__\n",
      "    self.terminate()\n",
      "  File \"c:\\Users\\tja\\AppData\\Local\\miniconda3\\envs\\semantic-analyzer\\lib\\site-packages\\libvoikko.py\", line 476, in terminate\n",
      "    if self.__handle:\n",
      "AttributeError: 'Voikko' object has no attribute '_Voikko__handle'\n",
      "2024-11-21 15:52:31,491 - src.core.language_processing.finnish - INFO - Added C:\\scripts\\Voikko to DLL search path\n",
      "2024-11-21 15:52:32,058 - src.core.language_processing.finnish - INFO - Successfully initialized Voikko with path: C:\\scripts\\Voikko\n",
      "2024-11-21 15:52:33,586 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:192] - Initialized FileUtils with log level: INFO\n",
      "2024-11-21 15:52:33,586 - src.utils.FileUtils.file_utils - DEBUG - Initialized FileUtils with log level: INFO\n",
      "2024-11-21 15:52:33,590 - src.utils.FileUtils.file_utils - DEBUG [file_utils.py:198] - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n",
      "2024-11-21 15:52:33,590 - src.utils.FileUtils.file_utils - DEBUG - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n",
      "2024-11-21 15:52:33,602 - src.nb_helpers.analyzers - DEBUG - Running keyword analysis\n",
      "2024-11-21 15:52:33,604 - src.nb_helpers.testers - DEBUG - KeywordTester starting analysis\n",
      "2024-11-21 15:52:33,606 - src.analyzers.keyword_analyzer - DEBUG - Starting keyword analysis\n",
      "2024-11-21 15:52:33,638 - src.analyzers.keyword_analyzer - DEBUG - Applied compound bonus to liikevaihto\n",
      "2024-11-21 15:52:33,640 - src.analyzers.keyword_analyzer - DEBUG - Applied technical term boost to liikevaihto\n",
      "2024-11-21 15:52:33,644 - src.analyzers.keyword_analyzer - DEBUG - Applied compound bonus to vuosineljännes\n",
      "2024-11-21 15:52:33,648 - src.analyzers.keyword_analyzer - DEBUG - Applied compound bonus to asiakashankinta\n",
      "2024-11-21 15:52:33,651 - src.analyzers.keyword_analyzer - DEBUG - Applied technical term boost to asiakashankinta\n",
      "2024-11-21 15:52:33,654 - src.analyzers.keyword_analyzer - DEBUG - Applied compound bonus to asiakaspysyvyys\n",
      "2024-11-21 15:52:33,656 - src.analyzers.keyword_analyzer - DEBUG - Applied technical term boost to asiakaspysyvyys\n",
      "2024-11-21 15:52:33,659 - src.analyzers.keyword_analyzer - DEBUG - Applied compound bonus to markkinaosuus\n",
      "2024-11-21 15:52:33,661 - src.analyzers.keyword_analyzer - DEBUG - Applied technical term boost to markkinaosuus\n",
      "2024-11-21 15:52:33,665 - src.analyzers.keyword_analyzer - DEBUG - Applied compound bonus to pilvipalvelu\n",
      "2024-11-21 15:52:33,703 - src.analyzers.keyword_analyzer - DEBUG - Applied compound bonus to liikevaihto\n",
      "2024-11-21 15:52:33,708 - src.analyzers.keyword_analyzer - DEBUG - Applied technical term boost to liikevaihto\n",
      "2024-11-21 15:52:33,712 - src.analyzers.keyword_analyzer - DEBUG - Applied compound bonus to vuosineljännes\n",
      "2024-11-21 15:52:33,715 - src.analyzers.keyword_analyzer - DEBUG - Applied compound bonus to asiakashankinta\n",
      "2024-11-21 15:52:33,718 - src.analyzers.keyword_analyzer - DEBUG - Applied technical term boost to asiakashankinta\n",
      "2024-11-21 15:52:33,722 - src.analyzers.keyword_analyzer - DEBUG - Applied compound bonus to asiakaspysyvyys\n",
      "2024-11-21 15:52:33,724 - src.analyzers.keyword_analyzer - DEBUG - Applied technical term boost to asiakaspysyvyys\n",
      "2024-11-21 15:52:33,727 - src.analyzers.keyword_analyzer - DEBUG - Applied compound bonus to markkinaosuus\n",
      "2024-11-21 15:52:33,730 - src.analyzers.keyword_analyzer - DEBUG - Applied technical term boost to markkinaosuus\n",
      "2024-11-21 15:52:33,732 - src.analyzers.keyword_analyzer - DEBUG - Applied compound bonus to pilvipalvelu\n",
      "2024-11-21 15:52:38,782 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-21 15:52:38,909 - src.analyzers.keyword_analyzer - DEBUG - Got LLM response for keyword analysis\n",
      "2024-11-21 15:52:38,910 - src.analyzers.keyword_analyzer - DEBUG - Processed LLM keyword: liikevaihto\n",
      "2024-11-21 15:52:38,912 - src.analyzers.keyword_analyzer - DEBUG - Processed LLM keyword: kasvu\n",
      "2024-11-21 15:52:38,913 - src.analyzers.keyword_analyzer - DEBUG - Processed LLM keyword: vahvistua\n",
      "2024-11-21 15:52:38,914 - src.analyzers.keyword_analyzer - DEBUG - Processed LLM keyword: vuosineljännes\n",
      "2024-11-21 15:52:38,916 - src.analyzers.keyword_analyzer - DEBUG - Processed LLM keyword: asiakashankinta\n",
      "2024-11-21 15:52:38,920 - src.analyzers.keyword_analyzer - DEBUG - Applied compound bonus to liikevaihto\n",
      "2024-11-21 15:52:38,927 - src.analyzers.keyword_analyzer - DEBUG - Applied technical term boost to liikevaihto\n",
      "2024-11-21 15:52:38,942 - src.analyzers.keyword_analyzer - DEBUG - Applied compound bonus to vuosineljännes\n",
      "2024-11-21 15:52:38,947 - src.analyzers.keyword_analyzer - DEBUG - Applied compound bonus to asiakashankinta\n",
      "2024-11-21 15:52:38,950 - src.analyzers.keyword_analyzer - DEBUG - Applied technical term boost to asiakashankinta\n",
      "2024-11-21 15:52:38,954 - src.analyzers.keyword_analyzer - DEBUG - Applied compound bonus to asiakaspysyvyys\n",
      "2024-11-21 15:52:38,957 - src.analyzers.keyword_analyzer - DEBUG - Applied technical term boost to asiakaspysyvyys\n",
      "2024-11-21 15:52:38,960 - src.analyzers.keyword_analyzer - DEBUG - Applied compound bonus to markkinaosuus\n",
      "2024-11-21 15:52:38,962 - src.analyzers.keyword_analyzer - DEBUG - Applied technical term boost to markkinaosuus\n",
      "2024-11-21 15:52:38,964 - src.analyzers.keyword_analyzer - DEBUG - Applied compound bonus to pilvipalvelu\n",
      "2024-11-21 15:52:38,968 - src.analyzers.keyword_analyzer - DEBUG - Applied domain boost (1.15) to liikevaihto\n",
      "2024-11-21 15:52:38,971 - src.analyzers.keyword_analyzer - DEBUG - Applied compound bonus to liikevaihto\n",
      "2024-11-21 15:52:38,973 - src.analyzers.keyword_analyzer - DEBUG - Applied technical term boost to liikevaihto\n",
      "2024-11-21 15:52:38,977 - src.analyzers.keyword_analyzer - DEBUG - Applied domain boost (1.15) to vuosineljännes\n",
      "2024-11-21 15:52:38,979 - src.analyzers.keyword_analyzer - DEBUG - Applied compound bonus to vuosineljännes\n",
      "2024-11-21 15:52:38,981 - src.analyzers.keyword_analyzer - DEBUG - Applied domain boost (1.15) to asiakashankinta\n",
      "2024-11-21 15:52:38,982 - src.analyzers.keyword_analyzer - DEBUG - Applied compound bonus to asiakashankinta\n",
      "2024-11-21 15:52:38,984 - src.analyzers.keyword_analyzer - DEBUG - Applied technical term boost to asiakashankinta\n",
      "2024-11-21 15:52:39,013 - src.analyzers.keyword_analyzer - DEBUG - Analysis complete. Found 9 keywords\n",
      "2024-11-21 15:52:39,014 - src.nb_helpers.testers - DEBUG - KeywordTester analysis complete\n",
      "2024-11-21 15:52:39,015 - src.nb_helpers.analyzers - DEBUG - Keyword analysis completed\n",
      "2024-11-21 15:52:39,016 - src.nb_helpers.analyzers - DEBUG - Formatting results\n",
      "2024-11-21 15:52:39,018 - src.nb_helpers.analyzers - DEBUG - Displaying debug information\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Keywords Found:\n",
      "  • liikevaihto          [████████████████████] (1.00)\n",
      "  • asiakashankinta      [████████████████████] (1.00)\n",
      "  • vuosineljännes       [████████████████████] (1.00)\n",
      "  • markkinaosuus        [████████████████░░░░] (0.81)\n",
      "  • kasvu                [███████████████░░░░░] (0.75)\n",
      "  • pilvipalvelu         [██████████████░░░░░░] (0.73)\n",
      "  • asiakaspysyvyys      [█████████████░░░░░░░] (0.67)\n",
      "  • kustannus            [██████████░░░░░░░░░░] (0.53)\n",
      "  • segmentti            [██████████░░░░░░░░░░] (0.53)\n",
      "\n",
      "Debug Information:\n",
      "--------------------\n",
      "{\n",
      "  \"keywords\": [\n",
      "    {\n",
      "      \"keyword\": \"liikevaihto\",\n",
      "      \"score\": 1.0,\n",
      "      \"domain\": \"business\",\n",
      "      \"compound_parts\": [\n",
      "        \"liike\",\n",
      "        \"vaihto\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"keyword\": \"asiakashankinta\",\n",
      "      \"score\": 1.0,\n",
      "      \"domain\": \"business\",\n",
      "      \"compound_parts\": [\n",
      "        \"asiakas\",\n",
      "        \"hankkia\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"keyword\": \"vuosinelj\\u00e4nnes\",\n",
      "      \"score\": 1.0,\n",
      "      \"domain\": \"business\",\n",
      "      \"compound_parts\": [\n",
      "        \"vuosi\",\n",
      "        \"nelj\\u00e4nnes\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"keyword\": \"markkinaosuus\",\n",
      "      \"score\": 0.8084736000000001,\n",
      "      \"domain\": null,\n",
      "      \"compound_parts\": [\n",
      "        \"markkina\",\n",
      "        \"osuus\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"keyword\": \"kasvu\",\n",
      "      \"score\": 0.7524,\n",
      "      \"domain\": \"business\",\n",
      "      \"compound_parts\": null\n",
      "    },\n",
      "    {\n",
      "      \"keyword\": \"pilvipalvelu\",\n",
      "      \"score\": 0.7349760000000002,\n",
      "      \"domain\": null,\n",
      "      \"compound_parts\": [\n",
      "        \"pilvi\",\n",
      "        \"palvella\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"keyword\": \"asiakaspysyvyys\",\n",
      "      \"score\": 0.6737280000000002,\n",
      "      \"domain\": null,\n",
      "      \"compound_parts\": [\n",
      "        \"asiakas\",\n",
      "        \"pysy\\u00e4\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"keyword\": \"kustannus\",\n",
      "      \"score\": 0.528,\n",
      "      \"domain\": null,\n",
      "      \"compound_parts\": null\n",
      "    },\n",
      "    {\n",
      "      \"keyword\": \"segmentti\",\n",
      "      \"score\": 0.528,\n",
      "      \"domain\": null,\n",
      "      \"compound_parts\": null\n",
      "    }\n",
      "  ],\n",
      "  \"compound_words\": [\n",
      "    \"liikevaihto\",\n",
      "    \"asiakashankinta\",\n",
      "    \"vuosinelj\\u00e4nnes\",\n",
      "    \"markkinaosuus\",\n",
      "    \"pilvipalvelu\",\n",
      "    \"asiakaspysyvyys\"\n",
      "  ],\n",
      "  \"domain_keywords\": {\n",
      "    \"business\": [\n",
      "      \"liikevaihto\",\n",
      "      \"asiakashankinta\",\n",
      "      \"vuosinelj\\u00e4nnes\",\n",
      "      \"kasvu\"\n",
      "    ]\n",
      "  },\n",
      "  \"language\": \"fi\",\n",
      "  \"success\": true,\n",
      "  \"error\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# test_case = \"technical_fi_1\"\n",
    "\n",
    "test_case = \"business_fi_1\"\n",
    "\n",
    "results = await test_keyword_analyzer(finnish_texts[test_case], language=\"fi\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.nb_helpers.testers import analyze_problematic_words, KeywordTester\n",
    "# Method 1: Use directly with a processor\n",
    "# processor = create_text_processor(language=\"fi\")\n",
    "# problematic_words = [\"para\", \"parani\", \"parantua\", \"kasvu\", \"kasvaa\"]\n",
    "# analyze_problematic_words(processor, problematic_words)\n",
    "\n",
    "# # Method 2: Use through KeywordTester\n",
    "# tester = KeywordTester(language_processor=create_text_processor(language=\"fi\"))\n",
    "# tester.analyze_words([\"para\", \"parani\", \"parantua\", \"kasvu\", \"kasvaa\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.core.language_processing.finnish import FinnishTextProcessor,analyze_problematic_words\n",
    "# # Usage example:\n",
    "# problematic_words = [\"para\", \"parani\", \"parantua\", \"kasvu\", \"kasvaa\"]\n",
    "# analyze_problematic_words(processor, problematic_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_case = \"technical_fi_1\"\n",
    "test_case = \"business_fi_1\"\n",
    "\n",
    "results = await test_keyword_analyzer(finnish_texts[test_case], language=\"fi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_case = \"Mixed Domain Content\"\n",
    "results = await test_keyword_analyzer(test_texts[test_case])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run tests\n",
    "for case_name, text in test_texts.items():\n",
    "    print(f\"\\nTesting: {case_name}\")\n",
    "    print(\"=\" * 50)\n",
    "    results = await test_keyword_analyzer(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "previous examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2. Test single language analysis\n",
    "# print(\"\\nSingle Language Analysis:\")\n",
    "# print(\"-\" * 50)\n",
    "\n",
    "# options_en = AnalysisOptions(\n",
    "#     show_confidence=True,\n",
    "#     show_evidence=True,\n",
    "#     show_keywords=True,\n",
    "#     show_raw_data=True,\n",
    "#     debug_mode=True,\n",
    "#     language=\"en\"  # Explicitly set language\n",
    "# )\n",
    "\n",
    "# # Analyze English text\n",
    "# text = example_texts[\"English Technical\"]\n",
    "# results_en = await analyze_keywords(text, options_en)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test Finnish keyword analysis\n",
    "# print(\"\\nTest Finnish keyword analysis:\")\n",
    "# print(\"-\" * 50)\n",
    "\n",
    "# options_fi = AnalysisOptions(\n",
    "#     show_confidence=True,\n",
    "#     show_evidence=True,\n",
    "#     debug_mode=True,\n",
    "#     language=\"fi\"  # Explicitly set language\n",
    "# )\n",
    "\n",
    "# # Analyze Finnish text with auto-detection\n",
    "# text = example_texts[\"Finnish Technical\"]\n",
    "# # results_fi = await analyze_text(text, options_fi)\n",
    "# keywords_fi = await analyze_keywords(text, options_fi)\n",
    "\n",
    "\n",
    "# # # 4. Test batch analysis with mixed languages\n",
    "# # print(\"\\nBatch Analysis with Mixed Languages:\")\n",
    "# # print(\"-\" * 50)\n",
    "\n",
    "# # batch_results = {}\n",
    "# # for name, text in example_texts.items():\n",
    "# #     print(f\"\\nAnalyzing {name}:\")\n",
    "# #     results = await analyze_text(text, options_auto)\n",
    "# #     batch_results[name] = results\n",
    "\n",
    "# # # 5. Test Excel file processing\n",
    "# # from src.nb_helpers.analyzers import analyze_excel_content\n",
    "\n",
    "# # # Create a test DataFrame\n",
    "# # import pandas as pd\n",
    "# # df = pd.DataFrame({\n",
    "# #     \"content\": example_texts.values(),\n",
    "# #     \"type\": [name.split()[0] for name in example_texts.keys()],  # \"English\" or \"Finnish\"\n",
    "# # })\n",
    "\n",
    "# # # Save to temporary Excel file\n",
    "# # temp_excel = \"temp_test_content.xlsx\"\n",
    "# # df.to_excel(temp_excel, index=False)\n",
    "\n",
    "# # print(\"\\nExcel File Analysis:\")\n",
    "# # print(\"-\" * 50)\n",
    "\n",
    "# # await analyze_excel_content(\n",
    "# #     input_file=temp_excel,\n",
    "# #     output_file=\"analysis_results\",\n",
    "# #     content_column=\"content\",\n",
    "# #     parameter_file=\"parameters_en.xlsx\",  # Use our parameter file\n",
    "# #     language_column=\"type\"  # Use type column for language\n",
    "# # )\n",
    "\n",
    "# # # Clean up temporary file\n",
    "# # os.remove(temp_excel)\n",
    "\n",
    "# # # 6. Compare analysis results\n",
    "# # print(\"\\nAnalysis Results Comparison:\")\n",
    "# # print(\"-\" * 50)\n",
    "\n",
    "# # def print_analysis_summary(results: dict, name: str):\n",
    "# #     print(f\"\\n{name}:\")\n",
    "# #     if \"keywords\" in results:\n",
    "# #         keywords = results[\"keywords\"].get(\"keywords\", [])\n",
    "# #         print(f\"Keywords found: {len(keywords)}\")\n",
    "# #         for kw in keywords[:3]:  # Show top 3 keywords\n",
    "# #             print(f\"- {kw.keyword}: {kw.score:.2f}\")\n",
    "    \n",
    "# #     if \"themes\" in results:\n",
    "# #         themes = results[\"themes\"].get(\"themes\", [])\n",
    "# #         print(f\"Themes found: {len(themes)}\")\n",
    "# #         for theme in themes[:2]:  # Show top 2 themes\n",
    "# #             print(f\"- {theme.name}: {theme.confidence:.2f}\")\n",
    "            \n",
    "# #     if \"categories\" in results:\n",
    "# #         categories = results[\"categories\"].get(\"categories\", [])\n",
    "# #         print(f\"Categories found: {len(categories)}\")\n",
    "# #         for cat in categories[:2]:  # Show top 2 categories\n",
    "# #             print(f\"- {cat.name}: {cat.confidence:.2f}\")\n",
    "\n",
    "# # for name, results in batch_results.items():\n",
    "# #     print_analysis_summary(results, name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run environment verification\n",
    "# from src.nb_helpers.environment import verify_environment\n",
    "# verify_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test logging\n",
    "logger = logging.getLogger(\"src.analyzers.keyword_analyzer\")\n",
    "logger.debug(\"Testing keyword analyzer logging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: following are not working with the new parameter handling model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example texts in different languages\n",
    "example_texts = {\n",
    "    \"English Technical\": \"\"\"\n",
    "        The cloud migration project improved system scalability while reducing costs.\n",
    "        New DevOps practices streamlined the deployment pipeline significantly.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Finnish Technical\": \"\"\"\n",
    "        Pilvipalveluihin siirtyminen paransi järjestelmän skaalautuvuutta ja vähensi kustannuksia.\n",
    "        Uudet DevOps-käytännöt tehostivat merkittävästi käyttöönottoprosessia.\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "# Analyze with automatic language detection\n",
    "for name, text in example_texts.items():\n",
    "    print(f\"\\nAnalyzing {name}:\")\n",
    "    results = await analyze_text(text, options)\n",
    "\n",
    "# Example with specific language and parameters\n",
    "fi_options = AnalysisOptions(\n",
    "    show_confidence=True,\n",
    "    show_evidence=True,\n",
    "    debug_mode=True,\n",
    "    language=\"fi\",\n",
    "    parameter_file=\"finnish_params.yaml\"\n",
    ")\n",
    "\n",
    "# Analyze Finnish text with specific parameters\n",
    "fi_results = await analyze_text(example_texts[\"Finnish Technical\"], fi_options)\n",
    "\n",
    "# Batch process Excel file with language detection\n",
    "await analyze_excel_content(\n",
    "    input_file=\"multilingual_texts.xlsx\",\n",
    "    output_file=\"analysis_results\",\n",
    "    content_column=\"content\",\n",
    "    parameter_file=\"analysis_params.yaml\",\n",
    "    language_column=\"language\"  # Optional column specifying language\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ## Analysis Functions\n",
    "\n",
    "### Single Analysis with Debug Output\n",
    "Run detailed analysis for a single text: -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_texts = {\n",
    "    \"Business Analysis\": \"\"\"\n",
    "        Q3 revenue increased by 15% with strong growth in enterprise sales.\n",
    "        Customer retention improved while acquisition costs decreased.\n",
    "        New market expansion initiatives are showing positive early results.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Technical Content\": \"\"\"\n",
    "        The application uses microservices architecture with containerized deployments.\n",
    "        Data processing pipeline incorporates machine learning models for prediction.\n",
    "        System monitoring ensures high availability and performance metrics.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Mixed Content\": \"\"\"\n",
    "        The IT department's cloud migration project reduced infrastructure costs by 25%.\n",
    "        DevOps implementation improved deployment frequency while maintaining quality.\n",
    "        Monthly recurring revenue from SaaS products grew steadily.\n",
    "    \"\"\",\n",
    "    \"koulutus\":\n",
    "    \"\"\"\n",
    "        Verkko-oppimisalusta sisältää interaktiivisia moduuleja ja oman tahdin edistymisen seurannan. \n",
    "        Virtuaaliluokat mahdollistavat reaaliaikaisen yhteistyön opiskelijoiden ja ohjaajien välillä. \n",
    "        Digitaaliset arviointityökalut antavat välitöntä palautetta oppimistuloksista.\n",
    "    \"\"\",\n",
    "    \"tekninen\":\n",
    "    \"\"\"\n",
    "        Koneoppimismalleja koulutetaan suurilla datajoukolla tunnistamaan kaavoja. \n",
    "        Neuroverkon arkkitehtuuri sisältää useita kerroksia piirteiden erottamiseen. \n",
    "        Datan esikäsittely ja piirteiden suunnittelu ovat keskeisiä vaiheita prosessissa.\n",
    "\n",
    "    \"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # New imports\n",
    "# from src.core.language_parameters import LanguageParameterManager\n",
    "\n",
    "# # Initialize parameter manager\n",
    "# param_manager = LanguageParameterManager()\n",
    "\n",
    "# # Example analysis with automatic language detection\n",
    "# text_en = \"Cloud computing enables scalable infrastructure deployment.\"\n",
    "# text_fi = \"Pilvipalvelut mahdollistavat skaalautuvan infrastruktuurin käyttöönoton.\"\n",
    "\n",
    "# # Analyze with automatic language detection and default parameters\n",
    "# async def analyze_text_with_language(text: str, parameter_file: Optional[str] = None):\n",
    "#     \"\"\"Analyze text with automatic language handling.\"\"\"\n",
    "#     # Get language-specific parameters\n",
    "#     params = param_manager.get_parameters(text, parameter_file)\n",
    "    \n",
    "#     # Create analyzers with parameters\n",
    "#     keyword_analyzer = KeywordAnalyzer(config=params.dict())\n",
    "#     theme_analyzer = ThemeAnalyzer(config=params.dict())\n",
    "#     category_analyzer = CategoryAnalyzer(config=params.dict())\n",
    "    \n",
    "#     # Run analysis\n",
    "#     results = {\n",
    "#         \"keywords\": await keyword_analyzer.analyze(text),\n",
    "#         \"themes\": await theme_analyzer.analyze(text),\n",
    "#         \"categories\": await category_analyzer.analyze(text)\n",
    "#     }\n",
    "    \n",
    "#     return results\n",
    "\n",
    "# # Example with Excel parameters\n",
    "# async def analyze_batch_with_excel_params(texts: List[str], excel_params: str):\n",
    "#     \"\"\"Analyze texts using parameters from Excel.\"\"\"\n",
    "#     # Load language-specific parameters\n",
    "#     params_by_lang = param_manager.load_excel_parameters(excel_params)\n",
    "    \n",
    "#     results = []\n",
    "#     for text in texts:\n",
    "#         # Detect language\n",
    "#         lang = param_manager.detect_language(text)\n",
    "#         # Get parameters for language\n",
    "#         params = params_by_lang.get(lang, param_manager.get_parameters(text))\n",
    "        \n",
    "#         # Create analyzer with language-specific parameters\n",
    "#         analyzer = KeywordAnalyzer(config=params.dict())\n",
    "#         result = await analyzer.analyze(text)\n",
    "#         results.append(result)\n",
    "    \n",
    "#     return results\n",
    "\n",
    "# # Example usage:\n",
    "# # With default parameters\n",
    "# results_en = await analyze_text_with_language(text_en)\n",
    "\n",
    "# # With parameter file\n",
    "# results_fi = await analyze_text_with_language(text_fi, \"finnish_params.yaml\")\n",
    "\n",
    "# # With Excel parameters\n",
    "# texts = [text_en, text_fi]\n",
    "# batch_results = await analyze_batch_with_excel_params(texts, \"analysis_params.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = example_texts[\"Mixed Content\"]\n",
    "# text = example_texts[\"koulutussisältö\"]\n",
    "# Debug specific analyzer\n",
    "\n",
    "# Example usage\n",
    "text = example_texts[\"Mixed Content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await analyze_keywords(text, options=options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await analyze_themes(text, options=options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await analyze_categories(text, options=options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or run full pipeline with debug info\n",
    "await debug_full_pipeline(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Processing from Excel\n",
    "Process multiple texts from Excel file:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await analyze_excel_content(\n",
    "    input_file=\"test_content.xlsx\",  # Input Excel file path\n",
    "    output_file=\"analysis_results\",  # Output filename (without extension)\n",
    "    content_column=\"content\"         # Column containing text to analyze\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "- Configure analyzers using parameter files\n",
    "- Control output detail with DebugOptions\n",
    "- Set logging level for verbosity control\n",
    "\n",
    "## Example Outputs\n",
    "The analysis provides:\n",
    "- Keywords with confidence scores\n",
    "- Theme identification and descriptions\n",
    "- Category classification with evidence\n",
    "- Confidence visualizations with Unicode bars\n",
    "\n",
    "## Notes\n",
    "- Set logging level to WARNING to minimize output\n",
    "- Use debug functions for detailed analysis inspection\n",
    "- Excel output combines all analysis types"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semantic-analyzer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
