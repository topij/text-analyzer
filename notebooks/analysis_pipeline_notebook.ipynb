{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added C:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer to Python path\n"
     ]
    }
   ],
   "source": [
    "# notebooks/analysis_pipeline_notebook.ipynb\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = str(Path().resolve().parent)\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "    print(f\"Added {project_root} to Python path\")\n",
    "\n",
    "# Core components\n",
    "from src.semantic_analyzer.analyzer import SemanticAnalyzer\n",
    "from src.utils.FileUtils.file_utils import FileUtils\n",
    "from src.analyzers.keyword_analyzer import KeywordAnalyzer\n",
    "from src.analyzers.theme_analyzer import ThemeAnalyzer\n",
    "from src.analyzers.category_analyzer import CategoryAnalyzer, CategoryOutput\n",
    "from src.core.language_processing import create_text_processor\n",
    "from src.loaders.parameter_adapter import ParameterAdapter\n",
    "from src.loaders.models import CategoryConfig\n",
    "from src.schemas import KeywordAnalysisResult\n",
    "\n",
    "\n",
    "# Initialize FileUtils and set up logging\n",
    "file_utils = FileUtils()\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tester classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# revise to have the option to set loading files to False (when custom texts are used)\n",
    "class BaseTester:\n",
    "    \"\"\"Base class for analysis testing.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        from src.core.llm.factory import create_llm\n",
    "        self.file_utils = FileUtils()\n",
    "        self.llm = create_llm()  # Create LLM instance\n",
    "        self.test_texts = self._load_test_texts()\n",
    "        \n",
    "    def _load_test_texts(self) -> Dict[str, str]:\n",
    "        \"\"\"Load test texts from files.\"\"\"\n",
    "        try:\n",
    "            # Try to load from existing files\n",
    "            texts = {}\n",
    "            for lang in [\"en\", \"fi\"]:\n",
    "                df = self.file_utils.load_single_file(\n",
    "                    f\"test_content_{lang}.xlsx\",\n",
    "                    input_type=\"raw\"\n",
    "                )\n",
    "                if df is not None:\n",
    "                    for _, row in df.iterrows():\n",
    "                        key = f\"{lang}_{row['type']}\"\n",
    "                        texts[key] = row['content']\n",
    "            return texts\n",
    "                        \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not load test texts: {e}. Using defaults.\")\n",
    "            return self._create_default_texts()\n",
    "    \n",
    "    def _create_default_texts(self) -> Dict[str, str]:\n",
    "        \"\"\"Create default test texts.\"\"\"\n",
    "        return {\n",
    "            \"en_technical\": \"\"\"\n",
    "                Machine learning models are trained using large datasets.\n",
    "                Neural networks extract features through multiple layers.\n",
    "                Data preprocessing improves model performance.\n",
    "            \"\"\",\n",
    "            \"en_business\": \"\"\"\n",
    "                Q3 financial results show 15% revenue growth.\n",
    "                Customer acquisition costs decreased while retention improved.\n",
    "                Market expansion strategy targets emerging sectors.\n",
    "            \"\"\",\n",
    "            \"fi_technical\": \"\"\"\n",
    "                Ohjelmistokehittäjä työskentelee asiakasprojektissa.\n",
    "                Tekninen toteutus vaatii erityistä huomiota.\n",
    "                Tietoturva on keskeinen osa kehitystä.\n",
    "            \"\"\"\n",
    "        }\n",
    "\n",
    "    def save_test_texts(self) -> None:\n",
    "        \"\"\"Save test texts using FileUtils.\"\"\"\n",
    "        df = pd.DataFrame([\n",
    "            {\n",
    "                \"language\": key.split(\"_\")[0],\n",
    "                \"type\": key.split(\"_\")[1],\n",
    "                \"content\": content.strip()\n",
    "            }\n",
    "            for key, content in self.test_texts.items()\n",
    "        ])\n",
    "        \n",
    "        self.file_utils.save_data_to_disk(\n",
    "            data={\"texts\": df},\n",
    "            output_type=\"raw\",\n",
    "            file_name=\"test_texts\",\n",
    "            output_filetype=\"xlsx\",\n",
    "            include_timestamp=False\n",
    "        )\n",
    "\n",
    "    async def analyze_text(self, text: str, language: str, analyzer: Any) -> Dict[str, Any]:\n",
    "        \"\"\"Base method for text analysis.\"\"\"\n",
    "        try:\n",
    "            return await analyzer.analyze(text)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Analysis error: {e}\")\n",
    "            return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeywordTester(BaseTester):\n",
    "    \"\"\"Helper class for testing keyword analysis.\"\"\"\n",
    "    \n",
    "    async def test_statistical_analysis(self, text: str, language: str = None) -> Dict[str, Any]:\n",
    "        \"\"\"Test statistical keyword extraction.\"\"\"\n",
    "        if language is None:\n",
    "            from langdetect import detect\n",
    "            try:\n",
    "                language = detect(text)\n",
    "            except:\n",
    "                language = \"en\"\n",
    "        \n",
    "        # Create processor and analyzer\n",
    "        processor = create_text_processor(language=language)\n",
    "        analyzer = KeywordAnalyzer(\n",
    "            llm=self.llm,  # Pass LLM instance\n",
    "            config={\"weights\": {\"statistical\": 1.0, \"llm\": 0.0}},  # Statistical only\n",
    "            language_processor=processor\n",
    "        )\n",
    "        \n",
    "        return await self.analyze_text(text, language, analyzer)\n",
    "\n",
    "    async def test_llm_analysis(self, text: str, language: str = None) -> Dict[str, Any]:\n",
    "        \"\"\"Test LLM-based keyword extraction.\"\"\"\n",
    "        if language is None:\n",
    "            from langdetect import detect\n",
    "            try:\n",
    "                language = detect(text)\n",
    "            except:\n",
    "                language = \"en\"\n",
    "        \n",
    "        analyzer = KeywordAnalyzer(\n",
    "            llm=self.llm,  # Pass LLM instance\n",
    "            config={\"weights\": {\"statistical\": 0.0, \"llm\": 1.0}},  # LLM only\n",
    "            language_processor=create_text_processor(language=language)\n",
    "        )\n",
    "        \n",
    "        return await self.analyze_text(text, language, analyzer)\n",
    "\n",
    "    async def test_combined_analysis(self, text: str, language: str = None) -> Dict[str, Any]:\n",
    "        \"\"\"Test combined statistical and LLM analysis.\"\"\"\n",
    "        if language is None:\n",
    "            from langdetect import detect\n",
    "            try:\n",
    "                language = detect(text)\n",
    "            except:\n",
    "                language = \"en\"\n",
    "        \n",
    "        analyzer = KeywordAnalyzer(\n",
    "            llm=self.llm,  # Pass LLM instance\n",
    "            config={\n",
    "                \"weights\": {\"statistical\": 0.4, \"llm\": 0.6},\n",
    "                \"max_keywords\": 8,\n",
    "                \"min_confidence\": 0.3\n",
    "            },\n",
    "            language_processor=create_text_processor(language=language)\n",
    "        )\n",
    "        \n",
    "        return await self.analyze_text(text, language, analyzer)\n",
    "\n",
    "    def display_keyword_results(self, results: Dict[str, Any]) -> None:\n",
    "        \"\"\"Display keyword analysis results.\"\"\"\n",
    "        print(\"\\nKeyword Analysis Results:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        if \"error\" in results:\n",
    "            print(f\"Error: {results['error']}\")\n",
    "            return\n",
    "            \n",
    "        if \"keywords\" in results:\n",
    "            print(\"\\nKeywords:\", results[\"keywords\"])\n",
    "            \n",
    "        if \"domain_keywords\" in results:\n",
    "            print(\"\\nDomain Keywords:\")\n",
    "            for domain, keywords in results[\"domain_keywords\"].items():\n",
    "                print(f\"{domain}: {keywords}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThemeTester(BaseTester):\n",
    "    \"\"\"Helper class for testing theme analysis.\"\"\"\n",
    "    \n",
    "    async def test_theme_analysis(self, text: str, language: str = None) -> Dict[str, Any]:\n",
    "        \"\"\"Test theme analysis on text.\"\"\"\n",
    "        if language is None:\n",
    "            from langdetect import detect\n",
    "            try:\n",
    "                language = detect(text)\n",
    "            except:\n",
    "                language = \"en\"\n",
    "        \n",
    "        analyzer = ThemeAnalyzer(\n",
    "            llm=self.llm,\n",
    "            config={\n",
    "                \"max_themes\": 3,\n",
    "                \"min_confidence\": 0.3,\n",
    "                \"focus_areas\": \"business,technical\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return await self.analyze_text(text, language, analyzer)\n",
    "        \n",
    "    def display_theme_results(self, results: Any) -> None:\n",
    "        \"\"\"Display theme analysis results.\n",
    "        \n",
    "        Args:\n",
    "            results: Either a dict or ThemeOutput model\n",
    "        \"\"\"\n",
    "        print(\"\\nTheme Analysis Results:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Convert to dict if it's a pydantic model\n",
    "        if hasattr(results, \"model_dump\"):\n",
    "            results = results.model_dump()\n",
    "        elif hasattr(results, \"dict\"):\n",
    "            results = results.dict()\n",
    "            \n",
    "        # Handle error case\n",
    "        if isinstance(results, dict) and \"error\" in results:\n",
    "            print(f\"Error: {results['error']}\")\n",
    "            return\n",
    "\n",
    "        # Access theme data\n",
    "        themes_data = results.get(\"themes\", {})\n",
    "        if isinstance(themes_data, dict):\n",
    "            themes = themes_data.get(\"themes\", [])\n",
    "            descriptions = themes_data.get(\"theme_descriptions\", {})\n",
    "            confidence = themes_data.get(\"theme_confidence\", {})\n",
    "            keywords = themes_data.get(\"related_keywords\", {})\n",
    "        else:\n",
    "            themes = []\n",
    "            descriptions = {}\n",
    "            confidence = {}\n",
    "            keywords = {}\n",
    "            \n",
    "        # Display themes\n",
    "        if not themes:\n",
    "            print(\"No themes found.\")\n",
    "            return\n",
    "            \n",
    "        for theme in themes:\n",
    "            print(f\"\\nTheme: {theme}\")\n",
    "            print(f\"Description: {descriptions.get(theme, 'No description available')}\")\n",
    "            print(f\"Confidence: {confidence.get(theme, 0):.2f}\")\n",
    "            theme_keywords = keywords.get(theme, [])\n",
    "            if theme_keywords:\n",
    "                print(f\"Keywords: {', '.join(theme_keywords)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoryTester(BaseTester):\n",
    "    \"\"\"Helper class for testing category analysis.\"\"\"\n",
    "    \n",
    "    def __init__(self, parameter_file: Optional[str] = None):\n",
    "        \"\"\"Initialize with optional parameter file path.\"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load categories from parameters if file provided\n",
    "        if parameter_file:\n",
    "            parameter_adapter = ParameterAdapter(parameter_file)\n",
    "            params = parameter_adapter.load_and_convert()\n",
    "            self.categories = params.categories\n",
    "        else:\n",
    "            # Default categories for testing\n",
    "            self.categories = {\n",
    "                \"technical_content\": CategoryConfig(\n",
    "                    description=\"Technical and software development content\",\n",
    "                    keywords=[\"software\", \"development\", \"api\", \"programming\", \"technical\"],\n",
    "                    threshold=0.6\n",
    "                ),\n",
    "                \"business_content\": CategoryConfig(\n",
    "                    description=\"Business and financial content\",\n",
    "                    keywords=[\"revenue\", \"sales\", \"market\", \"growth\", \"business\"],\n",
    "                    threshold=0.6\n",
    "                ),\n",
    "                \"educational_content\": CategoryConfig(\n",
    "                    description=\"Educational and learning content\",\n",
    "                    keywords=[\"learning\", \"education\", \"training\", \"teaching\"],\n",
    "                    threshold=0.5\n",
    "                )\n",
    "            }\n",
    "    \n",
    "    async def test_category_analysis(\n",
    "        self, \n",
    "        text: str, \n",
    "        language: str = None,\n",
    "        min_confidence: float = 0.3\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Test category analysis on text.\"\"\"\n",
    "        if language is None:\n",
    "            from langdetect import detect\n",
    "            try:\n",
    "                language = detect(text)\n",
    "            except:\n",
    "                language = \"en\"\n",
    "        \n",
    "        # Create processor for language\n",
    "        processor = create_text_processor(language=language)\n",
    "        \n",
    "        analyzer = CategoryAnalyzer(\n",
    "            categories=self.categories,\n",
    "            llm=self.llm,\n",
    "            config={\n",
    "                \"min_confidence\": min_confidence\n",
    "            },\n",
    "            language_processor=processor\n",
    "        )\n",
    "        \n",
    "        return await self.analyze_text(text, language, analyzer)\n",
    "        \n",
    "    def display_category_results(self, results: Any) -> None:\n",
    "        \"\"\"Display category analysis results.\n",
    "        \n",
    "        Args:\n",
    "            results: Either a dict or CategoryOutput model\n",
    "        \"\"\"\n",
    "        print(\"\\nCategory Analysis Results:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Convert to dict if needed\n",
    "        if hasattr(results, \"model_dump\"):\n",
    "            results = results.model_dump()\n",
    "        elif hasattr(results, \"dict\"):\n",
    "            results = results.dict()\n",
    "            \n",
    "        # Handle error case\n",
    "        if isinstance(results, dict) and \"error\" in results:\n",
    "            print(f\"Error: {results['error']}\")\n",
    "            return\n",
    "\n",
    "        # Access category data\n",
    "        categories = results.get(\"categories\", [])\n",
    "        explanations = results.get(\"explanations\", {})\n",
    "        evidence = results.get(\"evidence\", {})\n",
    "        \n",
    "        if not categories:\n",
    "            print(\"No matching categories found.\")\n",
    "            return\n",
    "            \n",
    "        # Display results\n",
    "        for category in categories:\n",
    "            name = category.get(\"name\", \"\")\n",
    "            confidence = category.get(\"confidence\", 0.0)\n",
    "            print(f\"\\nCategory: {name}\")\n",
    "            print(f\"Confidence: {confidence:.2f}\")\n",
    "            \n",
    "            # Show explanation\n",
    "            if name in explanations:\n",
    "                print(f\"Explanation: {explanations[name]}\")\n",
    "                \n",
    "            # Show evidence\n",
    "            if name in evidence:\n",
    "                print(\"Evidence:\")\n",
    "                for item in evidence[name]:\n",
    "                    print(f\"- {item}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnalysisPipeline:\n",
    "    \"\"\"Complete analysis pipeline for testing multiple analyzers.\"\"\"\n",
    "    \n",
    "    def __init__(self, parameter_file: Optional[str] = None):\n",
    "        self.file_utils = FileUtils()\n",
    "        self.keyword_tester = KeywordTester()\n",
    "        self.theme_tester = ThemeTester()\n",
    "        self.category_tester = CategoryTester(parameter_file)\n",
    "        \n",
    "    async def analyze_text(self, text: str, language: str = None) -> Dict[str, Any]:\n",
    "        \"\"\"Run complete analysis pipeline on text.\"\"\"\n",
    "        if language is None:\n",
    "            from langdetect import detect\n",
    "            try:\n",
    "                language = detect(text)\n",
    "            except:\n",
    "                language = \"en\"\n",
    "        \n",
    "        # Run analyses\n",
    "        keyword_results = await self.keyword_tester.test_combined_analysis(\n",
    "            text, language=language\n",
    "        )\n",
    "        theme_results = await self.theme_tester.test_theme_analysis(\n",
    "            text, language=language\n",
    "        )\n",
    "        category_results = await self.category_tester.test_category_analysis(\n",
    "            text, language=language\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"keywords\": keyword_results,\n",
    "            \"themes\": theme_results,\n",
    "            \"categories\": category_results,\n",
    "            \"language\": language\n",
    "        }\n",
    "    \n",
    "    def display_results(self, results: Dict[str, Any]) -> None:\n",
    "        \"\"\"Display complete analysis results.\"\"\"\n",
    "        print(\"\\nComplete Analysis Results\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Display keyword results\n",
    "        print(\"\\nKeyword Analysis:\")\n",
    "        print(\"-\" * 20)\n",
    "        self.keyword_tester.display_keyword_results(results.get(\"keywords\", {}))\n",
    "        \n",
    "        # Display theme results\n",
    "        print(\"\\nTheme Analysis:\")\n",
    "        print(\"-\" * 20)\n",
    "        self.theme_tester.display_theme_results(results.get(\"themes\", {}))\n",
    "        \n",
    "        # Display category results\n",
    "        print(\"\\nCategory Analysis:\")  \n",
    "        print(\"-\" * 20)\n",
    "        self.category_tester.display_category_results(results.get(\"categories\", {}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_analysis_examples():\n",
    "    \"\"\"Run example analyses on different content types.\"\"\"\n",
    "    # Configure root logger to reduce noise\n",
    "    logging.getLogger().setLevel(logging.WARNING)\n",
    "    \n",
    "    # Example texts\n",
    "    example_texts = {\n",
    "        \"Business Analysis\": \"\"\"\n",
    "            Q3 revenue increased by 15% with strong growth in enterprise sales.\n",
    "            Customer retention improved while acquisition costs decreased.\n",
    "            New market expansion initiatives are showing positive early results.\n",
    "        \"\"\",\n",
    "        \n",
    "        \"Technical Content\": \"\"\"\n",
    "            The application uses microservices architecture with containerized deployments.\n",
    "            Data processing pipeline incorporates machine learning models for prediction.\n",
    "            System monitoring ensures high availability and performance metrics.\n",
    "        \"\"\",\n",
    "        \n",
    "        \"Mixed Content\": \"\"\"\n",
    "            The IT department's cloud migration project reduced infrastructure costs by 25%.\n",
    "            DevOps implementation improved deployment frequency while maintaining quality.\n",
    "            Monthly recurring revenue from SaaS products grew steadily.\n",
    "        \"\"\"\n",
    "    }\n",
    "    \n",
    "    # Initialize pipeline with error handling for each analyzer\n",
    "    pipeline = AnalysisPipeline()\n",
    "    \n",
    "    for title, text in example_texts.items():\n",
    "        print(f\"\\nAnalyzing {title}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        try:\n",
    "            # Create analyzers with proper error handling\n",
    "            processor = create_text_processor()\n",
    "            \n",
    "            # 1. Keyword Analysis\n",
    "            print(\"\\n1. Keyword Analysis\")\n",
    "            try:\n",
    "                keyword_analyzer = KeywordAnalyzer(\n",
    "                    llm=pipeline.keyword_tester.llm,\n",
    "                    config={\n",
    "                        \"weights\": {\"statistical\": 0.4, \"llm\": 0.6},\n",
    "                        \"max_keywords\": 8,\n",
    "                        \"min_confidence\": 0.3\n",
    "                    },\n",
    "                    language_processor=processor\n",
    "                )\n",
    "                keyword_results = await keyword_analyzer.analyze(text)\n",
    "                print(\"✓ Complete\")\n",
    "                \n",
    "                if hasattr(keyword_results, 'keywords') and keyword_results.keywords:\n",
    "                    print(\"\\nKeywords Found:\")\n",
    "                    for kw in keyword_results.keywords:\n",
    "                        bar = \"█\" * int(kw.score * 20) + \"░\" * (20 - int(kw.score * 20))\n",
    "                        print(f\"  • {kw.keyword:<20} [{bar}] ({kw.score:.2f})\")\n",
    "                else:\n",
    "                    print(\"No keywords found\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"✗ Keyword analysis failed: {str(e)}\")\n",
    "                keyword_results = None\n",
    "            \n",
    "            # 2. Theme Analysis\n",
    "            print(\"\\n2. Theme Analysis\")\n",
    "            try:\n",
    "                theme_results = await pipeline.theme_tester.test_theme_analysis(text)\n",
    "                print(\"✓ Complete\")\n",
    "                \n",
    "                if hasattr(theme_results, 'themes') and theme_results.themes:\n",
    "                    print(\"\\nThemes Found:\")\n",
    "                    for theme in theme_results.themes:\n",
    "                        print(f\"\\n  • {theme.name} ({theme.confidence:.2f})\")\n",
    "                        print(f\"    {theme.description}\")\n",
    "                        if theme.keywords:\n",
    "                            print(f\"    Keywords: {', '.join(theme.keywords)}\")\n",
    "                else:\n",
    "                    print(\"No themes found\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"✗ Theme analysis failed: {str(e)}\")\n",
    "                theme_results = None\n",
    "            \n",
    "            # 3. Category Analysis\n",
    "            print(\"\\n3. Category Analysis\")\n",
    "            try:\n",
    "                category_results = await pipeline.category_tester.test_category_analysis(text)\n",
    "                print(\"✓ Complete\")\n",
    "                \n",
    "                if hasattr(category_results, 'categories') and category_results.categories:\n",
    "                    print(\"\\nCategories Found:\")\n",
    "                    for cat in category_results.categories:\n",
    "                        bar = \"█\" * int(cat.confidence * 20) + \"░\" * (20 - int(cat.confidence * 20))\n",
    "                        print(f\"\\n  • {cat.name}\")\n",
    "                        print(f\"    Confidence: [{bar}] ({cat.confidence:.2f})\")\n",
    "                        print(f\"    {cat.explanation}\")\n",
    "                        if cat.evidence:\n",
    "                            print(\"    Evidence:\")\n",
    "                            for ev in cat.evidence:\n",
    "                                print(f\"      - {ev}\")\n",
    "                else:\n",
    "                    print(\"No categories found\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"✗ Category analysis failed: {str(e)}\")\n",
    "                category_results = None\n",
    "            \n",
    "            print(\"\\n\" + \"-\" * 80 + \"\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Analysis failed: {str(e)}\\n\")\n",
    "            continue\n",
    "\n",
    "# Usage:\n",
    "# await run_analysis_examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def analyze_custom_text(\n",
    "    text: str, \n",
    "    parameter_file: Optional[str] = None,\n",
    "    detailed_output: bool = True,\n",
    "    timing_info: bool = True\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Analyze custom text with all analyzers.\n",
    "    \n",
    "    Args:\n",
    "        text: Text to analyze\n",
    "        parameter_file: Optional path to parameter file\n",
    "        detailed_output: Whether to show detailed analysis output\n",
    "        timing_info: Whether to show timing information\n",
    "    \"\"\"\n",
    "    # Configure root logger to reduce noise\n",
    "    logging.getLogger().setLevel(logging.WARNING)\n",
    "    \n",
    "    print(\"\\nComplete Analysis Pipeline\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(\"\\nInput Text:\")\n",
    "    print(\"-\" * 20)\n",
    "    print(text.strip())\n",
    "    \n",
    "    try:\n",
    "        pipeline = AnalysisPipeline(parameter_file)\n",
    "        \n",
    "        print(\"\\nRunning Analysis...\")\n",
    "        print(\"-\" * 20)\n",
    "        \n",
    "        # Track timing for each step\n",
    "        import time\n",
    "        total_start = time.time()\n",
    "        results = {}\n",
    "        timings = {}\n",
    "        \n",
    "        # Keyword Analysis\n",
    "        print(\"\\n1. Keyword Analysis\")\n",
    "        start = time.time()\n",
    "        try:\n",
    "            processor = create_text_processor()\n",
    "            analyzer = KeywordAnalyzer(\n",
    "                llm=pipeline.keyword_tester.llm,\n",
    "                config={\n",
    "                    \"weights\": {\"statistical\": 0.4, \"llm\": 0.6},\n",
    "                    \"max_keywords\": 8,\n",
    "                    \"min_confidence\": 0.3\n",
    "                },\n",
    "                language_processor=processor\n",
    "            )\n",
    "            keyword_results = await analyzer.analyze(text)\n",
    "            print(\"✓ Complete\")\n",
    "            \n",
    "            print(\"\\nKeywords Found:\")\n",
    "            if keyword_results.keywords:\n",
    "                for kw in keyword_results.keywords:\n",
    "                    bar = \"█\" * int(kw.score * 20) + \"░\" * (20 - int(kw.score * 20))\n",
    "                    print(f\"  • {kw.keyword:<20} [{bar}] ({kw.score:.2f})\")\n",
    "            if keyword_results.domain_keywords:\n",
    "                print(\"\\nDomain Keywords:\")\n",
    "                for domain, keywords in keyword_results.domain_keywords.items():\n",
    "                    print(f\"  {domain}: {', '.join(keywords)}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Failed: {str(e)}\")\n",
    "            keyword_results = KeywordAnalysisResult(\n",
    "                keywords=[],\n",
    "                compound_words=[],\n",
    "                domain_keywords={},\n",
    "                language=\"unknown\",\n",
    "                success=False,\n",
    "                error=str(e)\n",
    "            )\n",
    "        \n",
    "        # Theme Analysis\n",
    "        print(\"\\n2. Theme Analysis\")\n",
    "        start = time.time()\n",
    "        try:\n",
    "            theme_results = await pipeline.theme_tester.test_theme_analysis(text)\n",
    "            print(\"✓ Complete\")\n",
    "            \n",
    "            if detailed_output and hasattr(theme_results, 'themes'):\n",
    "                confidence_scores = []\n",
    "                print(\"\\nThemes Found:\")\n",
    "                for theme in theme_results.themes:\n",
    "                    confidence_scores.append(theme.confidence)\n",
    "                    print(f\"\\n  • {theme.name} ({theme.confidence:.2f})\")\n",
    "                    print(f\"    {theme.description}\")\n",
    "                    if theme.keywords:\n",
    "                        print(f\"    Keywords: {', '.join(theme.keywords)}\")\n",
    "                if confidence_scores:\n",
    "                    print(f\"\\n  Average confidence: {sum(confidence_scores)/len(confidence_scores):.2f}\")\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Failed: {str(e)}\")\n",
    "            theme_results = {\"error\": str(e)}\n",
    "        timings['themes'] = time.time() - start\n",
    "        if timing_info:\n",
    "            print(f\"Time: {timings['themes']:.2f}s\")\n",
    "        \n",
    "        # Category Analysis\n",
    "        print(\"\\n3. Category Analysis\")\n",
    "        start = time.time()\n",
    "        try:\n",
    "            category_results = await pipeline.category_tester.test_category_analysis(text)\n",
    "            print(\"✓ Complete\")\n",
    "            \n",
    "            if detailed_output and hasattr(category_results, 'categories'):\n",
    "                confidence_scores = []\n",
    "                print(\"\\nCategories Found:\")\n",
    "                for cat in category_results.categories:\n",
    "                    confidence_scores.append(cat.confidence)\n",
    "                    bar = \"█\" * int(cat.confidence * 20) + \"░\" * (20 - int(cat.confidence * 20))\n",
    "                    print(f\"\\n  • {cat.name}\")\n",
    "                    print(f\"    Confidence: [{bar}] ({cat.confidence:.2f})\")\n",
    "                    print(f\"    {cat.explanation}\")\n",
    "                    if cat.evidence:\n",
    "                        print(\"    Evidence:\")\n",
    "                        for ev in cat.evidence:\n",
    "                            print(f\"      - {ev}\")\n",
    "                if confidence_scores:\n",
    "                    print(f\"\\n  Average confidence: {sum(confidence_scores)/len(confidence_scores):.2f}\")\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Failed: {str(e)}\")\n",
    "            category_results = {\"error\": str(e)}\n",
    "        timings['categories'] = time.time() - start\n",
    "        if timing_info:\n",
    "            print(f\"Time: {timings['categories']:.2f}s\")\n",
    "        \n",
    "        total_time = time.time() - total_start\n",
    "        results = {\n",
    "            \"keywords\": keyword_results,\n",
    "            \"themes\": theme_results,\n",
    "            \"categories\": category_results,\n",
    "            \"language\": \"en\",\n",
    "            \"timings\": timings,\n",
    "            \"total_time\": total_time\n",
    "        }\n",
    "        \n",
    "        print(\"\\nAnalysis Complete\")\n",
    "        print(\"=\" * 50)\n",
    "        if timing_info:\n",
    "            print(f\"Total time: {total_time:.2f}s\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nPipeline Error: {str(e)}\")\n",
    "        return {\n",
    "            \"error\": str(e),\n",
    "            \"keywords\": {\"error\": str(e)},\n",
    "            \"themes\": {\"error\": str(e)},\n",
    "            \"categories\": {\"error\": str(e)}\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def analyze_excel_content(\n",
    "    input_file: str,\n",
    "    output_file: str,\n",
    "    content_column: str = \"content\",\n",
    "    parameter_file: Optional[str] = None\n",
    ") -> None:\n",
    "    \"\"\"Analyze content from Excel file and save results to new Excel file.\n",
    "    \n",
    "    Args:\n",
    "        input_file: Path to input Excel file\n",
    "        output_file: Path to save results\n",
    "        content_column: Name of the column containing text to analyze\n",
    "        parameter_file: Optional path to parameter file\n",
    "    \"\"\"\n",
    "    # Configure logging\n",
    "    logging.getLogger().setLevel(logging.WARNING)\n",
    "    \n",
    "    try:\n",
    "        # Initialize components\n",
    "        file_utils = FileUtils()\n",
    "        pipeline = AnalysisPipeline(parameter_file)\n",
    "        \n",
    "        print(f\"\\nAnalyzing content from: {input_file}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Load input data\n",
    "        df = file_utils.load_single_file(input_file)\n",
    "        if content_column not in df.columns:\n",
    "            raise ValueError(f\"Column '{content_column}' not found in input file\")\n",
    "            \n",
    "        # Prepare results storage\n",
    "        results = []\n",
    "        total = len(df)\n",
    "        \n",
    "        # Process each text\n",
    "        for idx, row in df.iterrows():\n",
    "            text = str(row[content_column])\n",
    "            print(f\"\\nProcessing text {idx + 1}/{total}\")\n",
    "            \n",
    "            try:\n",
    "                # Run analysis\n",
    "                analysis = await pipeline.analyze_text(text)\n",
    "                \n",
    "                # Extract keywords\n",
    "                keywords = []\n",
    "                if \"keywords\" in analysis and hasattr(analysis[\"keywords\"], \"keywords\"):\n",
    "                    keywords = [k.keyword for k in analysis[\"keywords\"].keywords]\n",
    "                \n",
    "                # Extract categories\n",
    "                categories = []\n",
    "                if \"categories\" in analysis and hasattr(analysis[\"categories\"], \"categories\"):\n",
    "                    categories = [c.name for c in analysis[\"categories\"].categories]\n",
    "                \n",
    "                # Extract themes\n",
    "                themes = []\n",
    "                if \"themes\" in analysis and hasattr(analysis[\"themes\"], \"themes\"):\n",
    "                    themes = [t.name for t in analysis[\"themes\"].themes]\n",
    "                \n",
    "                # Store results\n",
    "                results.append({\n",
    "                    \"content\": text,\n",
    "                    \"keywords\": \", \".join(keywords) if keywords else \"\",\n",
    "                    \"categories\": \", \".join(categories) if categories else \"\",\n",
    "                    \"themes\": \", \".join(themes) if themes else \"\"\n",
    "                })\n",
    "                \n",
    "                print(\"✓ Analysis complete\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"✗ Analysis failed: {str(e)}\")\n",
    "                results.append({\n",
    "                    \"content\": text,\n",
    "                    \"keywords\": \"\",\n",
    "                    \"categories\": \"\",\n",
    "                    \"themes\": \"\"\n",
    "                })\n",
    "        \n",
    "        # Create output DataFrame\n",
    "        output_df = pd.DataFrame(results)\n",
    "        \n",
    "        # Save to Excel\n",
    "        file_utils.save_data_to_disk(\n",
    "            data={\"Analysis Results\": output_df},\n",
    "            output_filetype=\"xlsx\",\n",
    "            file_name=output_file\n",
    "        )\n",
    "        \n",
    "        print(\"\\nAnalysis Complete\")\n",
    "        print(f\"Results saved to: {output_file}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Process failed: {str(e)}\")\n",
    "\n",
    "# Example usage:\n",
    "# await analyze_excel_content(\n",
    "#     input_file=\"input_texts.xlsx\",\n",
    "#     output_file=\"analysis_results\",\n",
    "#     content_column=\"content\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def debug_theme_analysis(text: str):\n",
    "    \"\"\"Debug theme analysis with detailed output.\"\"\"\n",
    "    print(\"\\nDebug Theme Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(\"\\nInput Text:\")\n",
    "    print(\"-\" * 20)\n",
    "    print(text.strip())\n",
    "    \n",
    "    # Configure logging\n",
    "    logger = logging.getLogger(\"src.analyzers.theme_analyzer\")\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    \n",
    "    # Add handler if not already present\n",
    "    if not logger.handlers:\n",
    "        handler = logging.StreamHandler()\n",
    "        formatter = logging.Formatter(\n",
    "            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        handler.setFormatter(formatter)\n",
    "        logger.addHandler(handler)\n",
    "    \n",
    "    print(\"\\nRunning Analysis...\")\n",
    "    print(\"-\" * 20)\n",
    "    \n",
    "    # Run analysis\n",
    "    theme_tester = ThemeTester()\n",
    "    results = await theme_tester.test_theme_analysis(text)\n",
    "    \n",
    "    # Display results\n",
    "    theme_tester.display_theme_results(results)\n",
    "    \n",
    "    if logger.isEnabledFor(logging.DEBUG):\n",
    "        print(\"\\nDebug Information:\")\n",
    "        print(\"-\" * 20)\n",
    "        if hasattr(results, \"model_dump\"):\n",
    "            print(json.dumps(results.model_dump(), indent=2))\n",
    "        else:\n",
    "            print(json.dumps(results, indent=2))\n",
    "    \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def debug_category_analysis(text: str, parameter_file: Optional[str] = None):\n",
    "    \"\"\"Debug category analysis with detailed output and visualizations.\"\"\"\n",
    "    print(\"\\nDebug Category Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Show input text\n",
    "    print(\"\\nInput Text:\")\n",
    "    print(\"-\" * 20)\n",
    "    print(text.strip())\n",
    "    \n",
    "    # Configure logging\n",
    "    logger = logging.getLogger(\"src.analyzers.category_analyzer\")\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    \n",
    "    if not logger.handlers:\n",
    "        handler = logging.StreamHandler()\n",
    "        formatter = logging.Formatter(\n",
    "            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        handler.setFormatter(formatter)\n",
    "        logger.addHandler(handler)\n",
    "    \n",
    "    try:\n",
    "        # Initialize analyzer\n",
    "        category_tester = CategoryTester(parameter_file)\n",
    "        \n",
    "        # Show configured categories with hierarchy\n",
    "        print(\"\\nConfigured Categories:\")\n",
    "        print(\"-\" * 20)\n",
    "        for name, config in category_tester.categories.items():\n",
    "            indent = \"  \" if config.parent else \"\"\n",
    "            print(f\"\\n{indent}{name}:\")\n",
    "            print(f\"{indent}  Description: {config.description}\")\n",
    "            print(f\"{indent}  Keywords: {', '.join(config.keywords)}\")\n",
    "            print(f\"{indent}  Threshold: {config.threshold}\")\n",
    "            if config.parent:\n",
    "                print(f\"{indent}  Parent: {config.parent}\")\n",
    "        \n",
    "        print(\"\\nRunning Analysis...\")\n",
    "        print(\"-\" * 20)\n",
    "        \n",
    "        # Run analysis\n",
    "        results = await category_tester.test_category_analysis(text)\n",
    "        \n",
    "        # Display formatted results\n",
    "        print(\"\\nAnalysis Results:\")\n",
    "        print(\"-\" * 20)\n",
    "        \n",
    "        if results.error:\n",
    "            print(f\"Error: {results.error}\")\n",
    "            return results\n",
    "            \n",
    "        # Show matched categories with details and visualization\n",
    "        categories = results.categories if isinstance(results.categories, list) else []\n",
    "        \n",
    "        if not categories:\n",
    "            print(\"No categories matched the confidence threshold.\")\n",
    "        else:\n",
    "            print(f\"\\nMatched {len(categories)} categories:\")\n",
    "            \n",
    "            # Sort categories by confidence\n",
    "            categories.sort(key=lambda x: x.confidence, reverse=True)\n",
    "            \n",
    "            for category in categories:\n",
    "                # Create confidence bar visualization\n",
    "                bar_length = 40\n",
    "                filled = int(category.confidence * bar_length)\n",
    "                confidence_bar = \"█\" * filled + \"░\" * (bar_length - filled)\n",
    "                \n",
    "                print(f\"\\n{category.name}\")\n",
    "                print(f\"Confidence: [{confidence_bar}] {category.confidence:.2%}\")\n",
    "                print(\"  \" + \"-\" * 40)\n",
    "                print(f\"  Explanation: {results.explanations.get(category.name, 'No explanation')}\")\n",
    "                \n",
    "                if evidence := results.evidence.get(category.name, []):\n",
    "                    print(\"\\n  Evidence:\")\n",
    "                    for idx, item in enumerate(evidence, 1):\n",
    "                        print(f\"    {idx}. {item}\")\n",
    "                        \n",
    "                if hasattr(category, 'themes') and category.themes:\n",
    "                    print(\"\\n  Related Themes:\")\n",
    "                    for theme in category.themes:\n",
    "                        print(f\"    • {theme}\")\n",
    "                        \n",
    "                # Show keyword matches if available\n",
    "                if category.name in category_tester.categories:\n",
    "                    config = category_tester.categories[category.name]\n",
    "                    matched_keywords = [\n",
    "                        kw for kw in config.keywords \n",
    "                        if kw.lower() in text.lower()\n",
    "                    ]\n",
    "                    if matched_keywords:\n",
    "                        print(\"\\n  Matched Keywords:\")\n",
    "                        print(f\"    {', '.join(matched_keywords)}\")\n",
    "        \n",
    "        # Add confidence threshold summary\n",
    "        print(\"\\nConfidence Summary:\")\n",
    "        print(\"-\" * 20)\n",
    "        thresholds = {\n",
    "            \"High (>0.8)\": len([c for c in categories if c.confidence > 0.8]),\n",
    "            \"Medium (0.6-0.8)\": len([c for c in categories if 0.6 <= c.confidence <= 0.8]),\n",
    "            \"Low (0.3-0.6)\": len([c for c in categories if 0.3 <= c.confidence < 0.6])\n",
    "        }\n",
    "        for level, count in thresholds.items():\n",
    "            print(f\"{level}: {count} categories\")\n",
    "        \n",
    "        # Show complete debug output\n",
    "        print(\"\\nRaw Analysis Data:\")\n",
    "        print(\"-\" * 20)\n",
    "        if hasattr(results, \"model_dump\"):\n",
    "            print(json.dumps(results.model_dump(), indent=2))\n",
    "        else:\n",
    "            print(json.dumps(results, indent=2))\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Analysis failed: {str(e)}\")\n",
    "        return CategoryOutput(\n",
    "            error=str(e),\n",
    "            success=False,\n",
    "            language=\"unknown\",\n",
    "            categories=[],\n",
    "            explanations={},\n",
    "            evidence={}\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_category_analysis():\n",
    "    \"\"\"Test category analysis with educational content.\"\"\"\n",
    "    text = \"\"\"\n",
    "    The online learning platform features interactive modules and self-paced progress tracking.\n",
    "    Virtual classrooms enable real-time collaboration between students and instructors.\n",
    "    Digital assessment tools provide immediate feedback on learning outcomes.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create test categories\n",
    "    categories = {\n",
    "        \"online_learning\": CategoryConfig(\n",
    "            description=\"Online and e-learning content\",\n",
    "            keywords=[\"online\", \"virtual\", \"digital\", \"platform\", \"e-learning\"],\n",
    "            threshold=0.5\n",
    "        ),\n",
    "        \"in_person_learning\": CategoryConfig(\n",
    "            description=\"Traditional classroom learning\",\n",
    "            keywords=[\"classroom\", \"face-to-face\", \"physical\", \"workshop\"],\n",
    "            threshold=0.5\n",
    "        ),\n",
    "        \"assessment\": CategoryConfig(\n",
    "            description=\"Learning assessment and feedback\",\n",
    "            keywords=[\"assessment\", \"feedback\", \"tracking\", \"progress\", \"outcomes\"],\n",
    "            threshold=0.5\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Run analysis with debug output\n",
    "    results = await debug_category_analysis(text)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # implement custom text analysis\n",
    "# async def analyze_custom_text(text: str, language: str = None):\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and verify environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_environment() -> bool:\n",
    "    \"\"\"Verify notebook environment setup.\"\"\"\n",
    "    from dotenv import load_dotenv\n",
    "    \n",
    "    # Load environment variables\n",
    "    env_path = Path(project_root) / \".env\"\n",
    "    env_loaded = load_dotenv(env_path)\n",
    "    \n",
    "    # Required variables\n",
    "    required_env_vars = [\n",
    "        'OPENAI_API_KEY',\n",
    "        'ANTHROPIC_API_KEY',\n",
    "    ]\n",
    "    \n",
    "    # Basic checks\n",
    "    basic_checks = {\n",
    "        \"Project root in path\": project_root in sys.path,\n",
    "        \"Can import src\": \"src\" in sys.modules,\n",
    "        \"FileUtils initialized\": hasattr(file_utils, \"project_root\"),\n",
    "        \".env file loaded\": env_loaded,\n",
    "    }\n",
    "    \n",
    "    # Environment variable checks\n",
    "    env_var_checks = {\n",
    "        f\"{var} set\": os.getenv(var) is not None\n",
    "        for var in required_env_vars\n",
    "    }\n",
    "    \n",
    "    # Path checks\n",
    "    expected_paths = {\n",
    "        \"Raw data\": file_utils.get_data_path(\"raw\"),\n",
    "        \"Processed data\": file_utils.get_data_path(\"processed\"),\n",
    "        \"Configuration\": file_utils.get_data_path(\"configurations\"),\n",
    "        \"Main config.yaml\": Path(project_root) / \"config.yaml\"\n",
    "    }\n",
    "    \n",
    "    path_checks = {\n",
    "        f\"{name} exists\": path.exists()\n",
    "        for name, path in expected_paths.items()\n",
    "    }\n",
    "    \n",
    "    # Combine all checks\n",
    "    all_checks = {\n",
    "        **basic_checks,\n",
    "        **env_var_checks,\n",
    "        **path_checks\n",
    "    }\n",
    "    \n",
    "    print(\"Environment Check Results:\")\n",
    "    print(\"=\" * 50)\n",
    "    print()\n",
    "    \n",
    "    # Print Basic Setup section\n",
    "    print(\"Basic Setup:\")\n",
    "    print(\"-\" * 11)\n",
    "    for check, result in basic_checks.items():\n",
    "        status = \"✓\" if result else \"✗\"\n",
    "        print(f\"{status} {check}\")\n",
    "    \n",
    "    # Print Environment Variables section\n",
    "    print(\"\\nEnvironment Variables:\")\n",
    "    print(\"-\" * 21)\n",
    "    for check, result in env_var_checks.items():\n",
    "        status = \"✓\" if result else \"✗\"\n",
    "        print(f\"{status} {check}\")\n",
    "    \n",
    "    # Print Project Structure section\n",
    "    print(\"\\nProject Structure:\")\n",
    "    print(\"-\" * 17)\n",
    "    for check, result in path_checks.items():\n",
    "        status = \"✓\" if result else \"✗\"\n",
    "        print(f\"{status} {check}\")\n",
    "    \n",
    "    # Overall status\n",
    "    all_passed = all(all_checks.values())\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Environment Status:\", \"Ready ✓\" if all_passed else \"Setup needed ✗\")\n",
    "    \n",
    "    # Print setup instructions if needed\n",
    "    if not all_passed:\n",
    "        print(\"\\nSetup Instructions:\")\n",
    "        if not env_loaded:\n",
    "            print(\"- Create a .env file in the project root with required API keys\")\n",
    "        for var in required_env_vars:\n",
    "            if not os.getenv(var):\n",
    "                print(f\"- Add {var} to your .env file\")\n",
    "        for name, path in expected_paths.items():\n",
    "            if not path.exists():\n",
    "                print(f\"- Create {name} directory at {path}\")\n",
    "    \n",
    "    return all_passed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Check Results:\n",
      "==================================================\n",
      "\n",
      "Basic Setup:\n",
      "-----------\n",
      "✓ Project root in path\n",
      "✓ Can import src\n",
      "✓ FileUtils initialized\n",
      "✓ .env file loaded\n",
      "\n",
      "Environment Variables:\n",
      "---------------------\n",
      "✓ OPENAI_API_KEY set\n",
      "✓ ANTHROPIC_API_KEY set\n",
      "\n",
      "Project Structure:\n",
      "-----------------\n",
      "✓ Raw data exists\n",
      "✓ Processed data exists\n",
      "✓ Configuration exists\n",
      "✓ Main config.yaml exists\n",
      "\n",
      "==================================================\n",
      "Environment Status: Ready ✓\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First cell: Verify environment\n",
    "verify_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run single analysis with debug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Q3 revenue increased by 15% with strong growth in enterprise sales.\n",
    "Customer retention improved while acquisition costs decreased.\n",
    "New market expansion initiatives are showing positive early results.\n",
    "\"\"\"\n",
    "# analyze with debug output\n",
    "#### uncomment the line below to run the test\n",
    "await debug_theme_analysis(text)\n",
    "\n",
    "# analyze specific text\n",
    "# await analyze_custom_text(text) # implement this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "The online learning platform features interactive modules and self-paced progress tracking.\n",
    "Virtual classrooms enable real-time collaboration between students and instructors.\n",
    "Digital assessment tools provide immediate feedback on learning outcomes.\n",
    "\"\"\"\n",
    "\n",
    "# Debug category analysis\n",
    "# results = await debug_category_analysis(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example:\n",
    "#### uncomment the line below to run the test\n",
    "# await test_category_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run complete analysis examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-13 20:37:42 - src.utils.FileUtils.file_utils - INFO - Attempting to load file: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\raw\\test_content_en.xlsx\n",
      "2024-11-13 20:37:42 - src.utils.FileUtils.file_utils - INFO - Successfully loaded Excel file: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\raw\\test_content_en.xlsx\n",
      "2024-11-13 20:37:42 - src.utils.FileUtils.file_utils - INFO - Attempting to load file: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\raw\\test_content_fi.xlsx\n",
      "2024-11-13 20:37:42 - src.utils.FileUtils.file_utils - INFO - Successfully loaded Excel file: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\raw\\test_content_fi.xlsx\n",
      "2024-11-13 20:37:44 - src.utils.FileUtils.file_utils - INFO - Attempting to load file: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\raw\\test_content_en.xlsx\n",
      "2024-11-13 20:37:44 - src.utils.FileUtils.file_utils - INFO - Successfully loaded Excel file: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\raw\\test_content_en.xlsx\n",
      "2024-11-13 20:37:44 - src.utils.FileUtils.file_utils - INFO - Attempting to load file: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\raw\\test_content_fi.xlsx\n",
      "2024-11-13 20:37:44 - src.utils.FileUtils.file_utils - INFO - Successfully loaded Excel file: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\raw\\test_content_fi.xlsx\n",
      "2024-11-13 20:37:45 - src.utils.FileUtils.file_utils - INFO - Attempting to load file: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\raw\\test_content_en.xlsx\n",
      "2024-11-13 20:37:45 - src.utils.FileUtils.file_utils - INFO - Successfully loaded Excel file: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\raw\\test_content_en.xlsx\n",
      "2024-11-13 20:37:45 - src.utils.FileUtils.file_utils - INFO - Attempting to load file: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\raw\\test_content_fi.xlsx\n",
      "2024-11-13 20:37:45 - src.utils.FileUtils.file_utils - INFO - Successfully loaded Excel file: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\raw\\test_content_fi.xlsx\n",
      "2024-11-13 20:37:45 - src.core.language_processing.factory - INFO - Using default configuration\n",
      "2024-11-13 20:37:45 - src.core.language_processing.english - INFO - Initialized English processor with 831 stopwords\n",
      "2024-11-13 20:37:45 - src.core.language_processing.english - INFO - Initialized English processor with 831 stopwords\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing Business Analysis\n",
      "==================================================\n",
      "\n",
      "1. Keyword Analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-13 20:37:49 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Complete\n",
      "\n",
      "Keywords Found:\n",
      "  • Q3 revenue           [█████████████░░░░░░░] (0.68)\n",
      "  • enterprise sales     [████████████░░░░░░░░] (0.65)\n",
      "  • customer retention   [████████████░░░░░░░░] (0.61)\n",
      "  • acquisition costs    [███████████░░░░░░░░░] (0.58)\n",
      "  • market expansion     [██████████░░░░░░░░░░] (0.54)\n",
      "  • strong growth        [██████████░░░░░░░░░░] (0.50)\n",
      "  • increase             [█████████░░░░░░░░░░░] (0.48)\n",
      "  • growth               [█████████░░░░░░░░░░░] (0.48)\n",
      "\n",
      "2. Theme Analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-13 20:37:55 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-13 20:37:55 - src.core.language_processing.factory - INFO - Using default configuration\n",
      "2024-11-13 20:37:55 - src.core.language_processing.english - INFO - Initialized English processor with 831 stopwords\n",
      "2024-11-13 20:37:55 - src.core.language_processing.english - INFO - Initialized English processor with 831 stopwords\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Complete\n",
      "\n",
      "Themes Found:\n",
      "\n",
      "  • Revenue Growth (0.90)\n",
      "    The increase in revenue by 15% indicates strong performance in enterprise sales.\n",
      "    Keywords: enterprise sales, Q3 revenue, growth\n",
      "\n",
      "  • Customer Retention (0.80)\n",
      "    Improvement in customer retention and decrease in acquisition costs suggest effective customer relationship management.\n",
      "    Keywords: customer retention, acquisition costs, customer management\n",
      "\n",
      "  • Market Expansion (0.75)\n",
      "    Positive early results from new market expansion initiatives highlight successful strategic growth efforts.\n",
      "    Keywords: market expansion, initiatives, positive results\n",
      "\n",
      "3. Category Analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-13 20:38:00 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-13 20:38:00 - src.core.language_processing.factory - INFO - Using default configuration\n",
      "2024-11-13 20:38:00 - src.core.language_processing.english - INFO - Initialized English processor with 831 stopwords\n",
      "2024-11-13 20:38:00 - src.core.language_processing.english - INFO - Initialized English processor with 831 stopwords\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Complete\n",
      "\n",
      "Categories Found:\n",
      "\n",
      "  • business_content\n",
      "    Confidence: [███████████████████░] (0.95)\n",
      "    The text discusses financial performance metrics such as revenue growth, customer retention, and market expansion, which are all key indicators of business performance.\n",
      "    Evidence:\n",
      "      - Q3 revenue increased by 15%\n",
      "      - strong growth in enterprise sales\n",
      "      - Customer retention improved\n",
      "      - acquisition costs decreased\n",
      "      - New market expansion initiatives are showing positive early results\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Analyzing Technical Content\n",
      "==================================================\n",
      "\n",
      "1. Keyword Analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-13 20:38:04 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Complete\n",
      "\n",
      "Keywords Found:\n",
      "  • microservices        [████████████████████] (1.00)\n",
      "  • architecture         [████████████████████] (1.00)\n",
      "  • machine learning     [█████████████░░░░░░░] (0.66)\n",
      "  • data processing      [████████████░░░░░░░░] (0.63)\n",
      "  • prediction           [████████████░░░░░░░░] (0.63)\n",
      "  • system monitoring    [████████████░░░░░░░░] (0.62)\n",
      "  • containerized        [████████████░░░░░░░░] (0.61)\n",
      "  • deployments          [███████████░░░░░░░░░] (0.58)\n",
      "\n",
      "2. Theme Analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-13 20:38:08 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-13 20:38:08 - src.core.language_processing.factory - INFO - Using default configuration\n",
      "2024-11-13 20:38:08 - src.core.language_processing.english - INFO - Initialized English processor with 831 stopwords\n",
      "2024-11-13 20:38:08 - src.core.language_processing.english - INFO - Initialized English processor with 831 stopwords\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Complete\n",
      "\n",
      "Themes Found:\n",
      "\n",
      "  • Microservices Architecture (0.90)\n",
      "    The application is built using a microservices architecture, allowing for modular and scalable deployments.\n",
      "    Keywords: containerization, scalability\n",
      "\n",
      "  • Machine Learning Integration (0.85)\n",
      "    The data processing pipeline utilizes machine learning models to enhance prediction capabilities.\n",
      "    Keywords: data processing, prediction models\n",
      "\n",
      "  • System Monitoring (0.80)\n",
      "    Continuous system monitoring is implemented to ensure high availability and track performance metrics.\n",
      "    Keywords: high availability, performance metrics\n",
      "\n",
      "3. Category Analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-13 20:38:13 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-13 20:38:13 - src.core.language_processing.factory - INFO - Using default configuration\n",
      "2024-11-13 20:38:13 - src.core.language_processing.english - INFO - Initialized English processor with 831 stopwords\n",
      "2024-11-13 20:38:13 - src.core.language_processing.english - INFO - Initialized English processor with 831 stopwords\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Complete\n",
      "\n",
      "Categories Found:\n",
      "\n",
      "  • technical_content\n",
      "    Confidence: [███████████████████░] (0.98)\n",
      "    The text discusses concepts related to software architecture, specifically microservices and containerization, which are key topics in technical and software development.\n",
      "    Evidence:\n",
      "      - The application uses microservices architecture\n",
      "      - containerized deployments\n",
      "      - Data processing pipeline incorporates machine learning models\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Analyzing Mixed Content\n",
      "==================================================\n",
      "\n",
      "1. Keyword Analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-13 20:38:16 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Complete\n",
      "\n",
      "Keywords Found:\n",
      "  • cloud migration      [█████████████░░░░░░░] (0.68)\n",
      "  • DevOps implementation [████████████░░░░░░░░] (0.65)\n",
      "  • infrastructure costs [████████████░░░░░░░░] (0.61)\n",
      "  • deployment frequency [███████████░░░░░░░░░] (0.58)\n",
      "  • monthly recurring revenue [██████████░░░░░░░░░░] (0.54)\n",
      "  • SaaS products        [██████████░░░░░░░░░░] (0.50)\n",
      "  • department           [█████████░░░░░░░░░░░] (0.48)\n",
      "  • migration            [█████████░░░░░░░░░░░] (0.48)\n",
      "\n",
      "2. Theme Analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-13 20:38:20 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-13 20:38:20 - src.core.language_processing.factory - INFO - Using default configuration\n",
      "2024-11-13 20:38:20 - src.core.language_processing.english - INFO - Initialized English processor with 831 stopwords\n",
      "2024-11-13 20:38:20 - src.core.language_processing.english - INFO - Initialized English processor with 831 stopwords\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Complete\n",
      "\n",
      "Themes Found:\n",
      "\n",
      "  • Cost Reduction (0.90)\n",
      "    The IT department's cloud migration project led to a significant decrease in infrastructure costs.\n",
      "    Keywords: cloud migration, infrastructure costs\n",
      "\n",
      "  • Operational Efficiency (0.85)\n",
      "    The implementation of DevOps practices resulted in an increase in deployment frequency while ensuring quality.\n",
      "    Keywords: DevOps, deployment frequency\n",
      "\n",
      "  • Revenue Growth (0.80)\n",
      "    There was a steady increase in monthly recurring revenue from SaaS products.\n",
      "    Keywords: SaaS, monthly recurring revenue\n",
      "\n",
      "3. Category Analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-13 20:38:25 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Complete\n",
      "\n",
      "Categories Found:\n",
      "\n",
      "  • technical_content\n",
      "    Confidence: [█████████████████░░░] (0.85)\n",
      "    The text discusses a cloud migration project and DevOps implementation, both of which are technical topics related to software development and IT infrastructure.\n",
      "    Evidence:\n",
      "      - cloud migration project\n",
      "      - DevOps implementation\n",
      "      - deployment frequency\n",
      "\n",
      "  • business_content\n",
      "    Confidence: [███████████████░░░░░] (0.75)\n",
      "    The text mentions reduced infrastructure costs and growth in monthly recurring revenue, which are key indicators of business performance and financial metrics.\n",
      "    Evidence:\n",
      "      - reduced infrastructure costs by 25%\n",
      "      - monthly recurring revenue from SaaS products grew steadily\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "await run_analysis_examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or analyze with complete pipeline\n",
    "results = await analyze_custom_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or analyze specific text:\n",
    "# text = \"\"\"Your text here...\"\"\"\n",
    "# await analyze_custom_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process your Excel file\n",
    "await analyze_excel_content(\n",
    "    input_file=\"test_content_en.xlsx\",\n",
    "    output_file=\"analysis_results\",\n",
    "    content_column=\"content\"  # Change this to match your column name\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semantic-analyzer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
