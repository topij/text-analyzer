{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Import required modules\n",
        "import sys\n",
        "from pathlib import Path\n",
        "# from typing import List, Dict, Any, Tuple, Union\n",
        "# import logging\n",
        "# import asyncio\n",
        "\n",
        "# Add project root to Python path if needed\n",
        "project_root = str(Path().resolve().parent)\n",
        "if project_root not in sys.path:\n",
        "    sys.path.append(project_root)\n",
        "print(f\"Project root: {project_root}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Project root: /mnt/batch/tasks/shared/LS_root/mounts/clusters/basic-cpu/code/Users/topi.jarvinen/semantic-text-analyzer\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733234080337
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# azure_notebook.ipynb\n",
        "import os\n",
        "from src.az_helpers.setup_azure import init_azure_ml # setup_environment\n",
        "from src.semantic_analyzer import SemanticAnalyzer"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1733234134039
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from src.az_helpers.az_environment import setup_notebook_env, verify_environment"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733234134257
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up environment and logging\n",
        "setup_notebook_env(log_level=\"DEBUG\")\n",
        "verify_environment()\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Environment Check Results:\n==================================================\n✓ Project root in path\n✓ FileUtils initialized\n✓ .env file loaded\n✓ OPENAI_API_KEY set\n✓ ANTHROPIC_API_KEY set\n✓ Raw data exists\n✓ Processed data exists\n✓ Configuration exists\n✓ Main config.yaml exists\n\n==================================================\nEnvironment Status: Ready ✓\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733234134733
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup environment\n",
        "from FileUtils import FileUtils\n",
        "file_utils = FileUtils()"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733234134996
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize analyzer\n",
        "analyzer = SemanticAnalyzer(\n",
        "    parameter_file=\"azure://parameters/parameters_en.xlsx\",\n",
        "    file_utils=file_utils\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733234151485
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test analysis\n",
        "text = \"Machine learning models process data efficiently.\"\n",
        "result = await analyzer.analyze(text)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\nProcessing response: {'categories': [{'category': 'Technical', 'confidence': 0.9, 'explanation': 'The text discusses machine learning models, which are a key concept in the technical domain, particularly in data processing and analysis.', 'evidence': [{'text': 'Machine learning models process data efficiently.', 'relevance': 0.95, 'matched_keywords': ['machine', 'learning', 'models', 'process', 'data', 'efficiently'], 'context': 'The entire sentence focuses on the capabilities of machine learning in handling data.'}], 'themes': ['data processing', 'machine learning']}], 'relationships': {'Technical': ['Business']}}\n\nProcessing category: {'category': 'Technical', 'confidence': 0.9, 'explanation': 'The text discusses machine learning models, which are a key concept in the technical domain, particularly in data processing and analysis.', 'evidence': [{'text': 'Machine learning models process data efficiently.', 'relevance': 0.95, 'matched_keywords': ['machine', 'learning', 'models', 'process', 'data', 'efficiently'], 'context': 'The entire sentence focuses on the capabilities of machine learning in handling data.'}], 'themes': ['data processing', 'machine learning']}\nCreated category: name='' confidence=0.9 description='' evidence=[Evidence(text='Machine learning models process data efficiently.', relevance=0.95)] themes=['data processing', 'machine learning']\n\nFinal categories: [CategoryMatch(name='', confidence=0.9, description='', evidence=[Evidence(text='Machine learning models process data efficiently.', relevance=0.95)], themes=['data processing', 'machine learning'])]\n\nRaw LLM response: {'themes': [{'name': 'Efficiency in Data Processing', 'description': 'The theme focuses on how machine learning models enhance the efficiency of data processing, which is crucial for businesses looking to optimize operations.', 'confidence': 0.95, 'keywords': ['efficiently', 'process', 'data'], 'domain': 'technical', 'parent_theme': None}, {'name': 'Machine Learning Models', 'description': 'This theme highlights the role of machine learning models in analyzing and interpreting data, which is essential for informed decision-making in business.', 'confidence': 0.9, 'keywords': ['machine', 'model', 'learn'], 'domain': 'technical', 'parent_theme': None}], 'evidence': {'Efficiency in Data Processing': [{'text': 'Machine learning models process data efficiently.', 'relevance': 0.9, 'keywords': ['efficiently', 'process', 'data']}], 'Machine Learning Models': [{'text': 'Machine learning models process data efficiently.', 'relevance': 0.85, 'keywords': ['machine', 'model', 'learn']}]}, 'relationships': {'Efficiency in Data Processing': ['Machine Learning Models'], 'Machine Learning Models': ['Efficiency in Data Processing']}}\n\nProcessed LLM response: {'themes': [{'name': 'Efficiency in Data Processing', 'description': 'The theme focuses on how machine learning models enhance the efficiency of data processing, which is crucial for businesses looking to optimize operations.', 'confidence': 0.95, 'keywords': ['efficiently', 'process', 'data'], 'domain': 'technical', 'parent_theme': None}, {'name': 'Machine Learning Models', 'description': 'This theme highlights the role of machine learning models in analyzing and interpreting data, which is essential for informed decision-making in business.', 'confidence': 0.9, 'keywords': ['machine', 'model', 'learn'], 'domain': 'technical', 'parent_theme': None}], 'evidence': {'Efficiency in Data Processing': [{'text': 'Machine learning models process data efficiently.', 'relevance': 0.9, 'keywords': ['efficiently', 'process', 'data']}], 'Machine Learning Models': [{'text': 'Machine learning models process data efficiently.', 'relevance': 0.85, 'keywords': ['machine', 'model', 'learn']}]}, 'relationships': {'Efficiency in Data Processing': ['Machine Learning Models'], 'Machine Learning Models': ['Efficiency in Data Processing']}}\n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733234160517
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Helper functions for handling analysis results.\"\"\"\n",
        "\n",
        "from typing import Any, Dict, Optional, Union\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "from FileUtils import FileUtils, OutputFileType\n",
        "\n",
        "\n",
        "def save_analysis_to_excel(\n",
        "    result: Any,\n",
        "    file_utils: FileUtils,\n",
        "    file_name: str = \"analysis_results\",\n",
        "    output_type: str = \"processed\",\n",
        "    include_timestamp: Optional[bool] = None,\n",
        "    include_metadata: bool = True\n",
        ") -> Dict[str, str]:\n",
        "    \"\"\"Convert analysis result to DataFrames and save to Excel.\n",
        "    \n",
        "    Args:\n",
        "        result: Analysis result object\n",
        "        file_utils: FileUtils instance\n",
        "        file_name: Name for the output file\n",
        "        output_type: Type of output directory (e.g., \"processed\")\n",
        "        include_timestamp: Whether to include timestamp in filename\n",
        "        include_metadata: Whether to include metadata sheet\n",
        "    \n",
        "    Returns:\n",
        "        Dict[str, str]: Dictionary of saved file paths\n",
        "        \n",
        "    Example:\n",
        "        >>> file_utils = FileUtils()\n",
        "        >>> save_analysis_to_excel(analysis_result, file_utils)\n",
        "    \"\"\"\n",
        "    # Convert keywords to DataFrame\n",
        "    keywords_df = pd.DataFrame([\n",
        "        {\n",
        "            'keyword': k.keyword,\n",
        "            'score': k.score,\n",
        "            'domain': k.domain,\n",
        "            'compound_parts': ', '.join(k.compound_parts)\n",
        "        } for k in result.keywords.keywords\n",
        "    ])\n",
        "    \n",
        "    # Convert themes to DataFrame\n",
        "    themes_df = pd.DataFrame([\n",
        "        {\n",
        "            'name': t.name,\n",
        "            'description': t.description,\n",
        "            'confidence': t.confidence,\n",
        "            'keywords': ', '.join(t.keywords)\n",
        "        } for t in result.themes.themes\n",
        "    ])\n",
        "    \n",
        "    # Convert categories to DataFrame\n",
        "    categories_df = pd.DataFrame([\n",
        "        {\n",
        "            'name': m.name,\n",
        "            'confidence': m.confidence,\n",
        "            'description': m.description,\n",
        "            'themes': ', '.join(m.themes),\n",
        "            'evidence': '\\n'.join([e.text for e in m.evidence]) if hasattr(m, 'evidence') else ''\n",
        "        } for m in result.categories.matches\n",
        "    ])\n",
        "    \n",
        "    # Prepare data dictionary\n",
        "    data_dict = {\n",
        "        'Keywords': keywords_df,\n",
        "        'Themes': themes_df,\n",
        "        'Categories': categories_df,\n",
        "    }\n",
        "    \n",
        "    # Add metadata if requested\n",
        "    if include_metadata:\n",
        "        metadata_df = pd.DataFrame([{\n",
        "            'language': result.language,\n",
        "            'success': result.success,\n",
        "            'processing_time': result.processing_time,\n",
        "            'keywords_language': result.keywords.language,\n",
        "            'themes_language': result.themes.language,\n",
        "            'categories_language': result.categories.language,\n",
        "            'compound_words': ', '.join(result.keywords.compound_words)\n",
        "        }])\n",
        "        data_dict['Metadata'] = metadata_df\n",
        "    \n",
        "    # Save to Excel\n",
        "    saved_files, _ = file_utils.save_data_to_disk(\n",
        "        data=data_dict,\n",
        "        output_filetype=OutputFileType.XLSX,\n",
        "        output_type=output_type,\n",
        "        file_name=file_name,\n",
        "        include_timestamp=include_timestamp\n",
        "    )\n",
        "    \n",
        "    return saved_files\n",
        "\n",
        "\n",
        "def read_analysis_from_excel(\n",
        "    file_path: Union[str, Path],\n",
        "    file_utils: FileUtils,\n",
        "    input_type: str = \"processed\"\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"Read analysis results from Excel file.\n",
        "    \n",
        "    Args:\n",
        "        file_path: Path to Excel file\n",
        "        file_utils: FileUtils instance\n",
        "        input_type: Type of input directory (e.g., \"processed\")\n",
        "    \n",
        "    Returns:\n",
        "        Dict[str, pd.DataFrame]: Dictionary of DataFrames for each sheet\n",
        "        \n",
        "    Example:\n",
        "        >>> file_utils = FileUtils()\n",
        "        >>> data = read_analysis_from_excel(\"analysis_results.xlsx\", file_utils)\n",
        "        >>> keywords_df = data['Keywords']\n",
        "    \"\"\"\n",
        "    return file_utils.load_excel_sheets(file_path, input_type=input_type)"
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733234759605
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "analysis_result = result"
      ],
      "outputs": [],
      "execution_count": 17,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733234980997
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Converter functions for analysis results.\"\"\"\n",
        "\n",
        "from typing import Any, Dict, List, Optional, Union\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def convert_analysis_to_dataframes(result: Any) -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"Convert analysis result to a dictionary of DataFrames.\n",
        "    \n",
        "    Args:\n",
        "        result: Analysis result object\n",
        "        \n",
        "    Returns:\n",
        "        Dict[str, pd.DataFrame]: Dictionary with DataFrames for each component\n",
        "        \n",
        "    Example:\n",
        "        >>> dfs = convert_analysis_to_dataframes(analysis_result)\n",
        "        >>> file_utils.save_data_to_disk(\n",
        "        ...     data=dfs,\n",
        "        ...     output_filetype=\"xlsx\",\n",
        "        ...     file_name=\"analysis\"\n",
        "        ... )\n",
        "    \"\"\"\n",
        "    # Keywords DataFrame\n",
        "    keywords_df = pd.DataFrame([\n",
        "        {\n",
        "            'keyword': k.keyword,\n",
        "            'score': k.score,\n",
        "            'domain': k.domain,\n",
        "            'compound_parts': ', '.join(k.compound_parts)\n",
        "        } for k in result.keywords.keywords\n",
        "    ])\n",
        "    \n",
        "    # Themes DataFrame\n",
        "    themes_df = pd.DataFrame([\n",
        "        {\n",
        "            'name': t.name,\n",
        "            'description': t.description,\n",
        "            'confidence': t.confidence,\n",
        "            'keywords': ', '.join(t.keywords)\n",
        "        } for t in result.themes.themes\n",
        "    ])\n",
        "    \n",
        "    # Categories DataFrame\n",
        "    categories_df = pd.DataFrame([\n",
        "        {\n",
        "            'name': m.name,\n",
        "            'confidence': m.confidence,\n",
        "            'description': m.description,\n",
        "            'themes': ', '.join(m.themes),\n",
        "            'evidence': '\\n'.join([e.text for e in m.evidence]) if hasattr(m, 'evidence') else ''\n",
        "        } for m in result.categories.matches\n",
        "    ])\n",
        "    \n",
        "    # Metadata DataFrame\n",
        "    metadata_df = pd.DataFrame([{\n",
        "        'language': result.language,\n",
        "        'success': result.success,\n",
        "        'processing_time': result.processing_time,\n",
        "        'keywords_language': result.keywords.language,\n",
        "        'themes_language': result.themes.language,\n",
        "        'categories_language': result.categories.language,\n",
        "        'compound_words': ', '.join(result.keywords.compound_words)\n",
        "    }])\n",
        "    \n",
        "    return {\n",
        "        'Keywords': keywords_df,\n",
        "        'Themes': themes_df,\n",
        "        'Categories': categories_df,\n",
        "        'Metadata': metadata_df\n",
        "    }\n",
        "\n",
        "\n",
        "def convert_analysis_to_dict(result: Any) -> Dict[str, Any]:\n",
        "    \"\"\"Convert analysis result to a nested dictionary.\n",
        "    \n",
        "    Args:\n",
        "        result: Analysis result object\n",
        "        \n",
        "    Returns:\n",
        "        Dict[str, Any]: Dictionary representation of the analysis result\n",
        "        \n",
        "    Example:\n",
        "        >>> data_dict = convert_analysis_to_dict(analysis_result)\n",
        "        >>> file_utils.save_json(\n",
        "        ...     data=data_dict,\n",
        "        ...     file_path=\"analysis\"\n",
        "        ... )\n",
        "    \"\"\"\n",
        "    return {\n",
        "        'keywords': {\n",
        "            'items': [\n",
        "                {\n",
        "                    'keyword': k.keyword,\n",
        "                    'score': k.score,\n",
        "                    'domain': k.domain,\n",
        "                    'compound_parts': k.compound_parts\n",
        "                } for k in result.keywords.keywords\n",
        "            ],\n",
        "            'compound_words': result.keywords.compound_words,\n",
        "            'language': result.keywords.language,\n",
        "            'success': result.keywords.success,\n",
        "            'error': result.keywords.error\n",
        "        },\n",
        "        'themes': {\n",
        "            'items': [\n",
        "                {\n",
        "                    'name': t.name,\n",
        "                    'description': t.description,\n",
        "                    'confidence': t.confidence,\n",
        "                    'keywords': t.keywords,\n",
        "                    'parent_theme': t.parent_theme\n",
        "                } for t in result.themes.themes\n",
        "            ],\n",
        "            'theme_hierarchy': result.themes.theme_hierarchy,\n",
        "            'language': result.themes.language,\n",
        "            'success': result.themes.success,\n",
        "            'error': result.themes.error\n",
        "        },\n",
        "        'categories': {\n",
        "            'matches': [\n",
        "                {\n",
        "                    'name': m.name,\n",
        "                    'confidence': m.confidence,\n",
        "                    'description': m.description,\n",
        "                    'evidence': [\n",
        "                        {\n",
        "                            'text': e.text,\n",
        "                            'relevance': e.relevance\n",
        "                        } for e in m.evidence\n",
        "                    ] if hasattr(m, 'evidence') else [],\n",
        "                    'themes': m.themes\n",
        "                } for m in result.categories.matches\n",
        "            ],\n",
        "            'language': result.categories.language,\n",
        "            'success': result.categories.success,\n",
        "            'error': result.categories.error\n",
        "        },\n",
        "        'metadata': {\n",
        "            'language': result.language,\n",
        "            'success': result.success,\n",
        "            'error': result.error,\n",
        "            'processing_time': result.processing_time\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "def get_analysis_summary(result: Any) -> pd.DataFrame:\n",
        "    \"\"\"Create a summary DataFrame of the analysis results.\n",
        "    \n",
        "    Args:\n",
        "        result: Analysis result object\n",
        "        \n",
        "    Returns:\n",
        "        pd.DataFrame: Summary of key findings\n",
        "        \n",
        "    Example:\n",
        "        >>> summary_df = get_analysis_summary(analysis_result)\n",
        "        >>> file_utils.save_data_to_disk(\n",
        "        ...     data={'Summary': summary_df},\n",
        "        ...     output_filetype=\"xlsx\",\n",
        "        ...     file_name=\"analysis_summary\"\n",
        "        ... )\n",
        "    \"\"\"\n",
        "    # Get top keywords by score\n",
        "    top_keywords = sorted(\n",
        "        result.keywords.keywords,\n",
        "        key=lambda k: k.score,\n",
        "        reverse=True\n",
        "    )[:5]\n",
        "    \n",
        "    # Get top themes by confidence\n",
        "    top_themes = sorted(\n",
        "        result.themes.themes,\n",
        "        key=lambda t: t.confidence,\n",
        "        reverse=True\n",
        "    )[:5]\n",
        "    \n",
        "    # Create summary rows\n",
        "    rows = []\n",
        "    \n",
        "    # Add keyword information\n",
        "    rows.append({\n",
        "        'Category': 'Top Keywords',\n",
        "        'Item': ', '.join(k.keyword for k in top_keywords),\n",
        "        'Score': ', '.join(f\"{k.score:.2f}\" for k in top_keywords)\n",
        "    })\n",
        "    \n",
        "    # Add theme information\n",
        "    for theme in top_themes:\n",
        "        rows.append({\n",
        "            'Category': 'Theme',\n",
        "            'Item': theme.name,\n",
        "            'Description': theme.description,\n",
        "            'Score': f\"{theme.confidence:.2f}\"\n",
        "        })\n",
        "    \n",
        "    # Add metadata\n",
        "    rows.append({\n",
        "        'Category': 'Metadata',\n",
        "        'Item': 'Processing Time',\n",
        "        'Score': f\"{result.processing_time:.2f}s\"\n",
        "    })\n",
        "    \n",
        "    return pd.DataFrame(rows)"
      ],
      "outputs": [],
      "execution_count": 18,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733234995967
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "# if __name__ == \"__main__\":\n",
        "# Convert to DataFrames\n",
        "dfs = convert_analysis_to_dataframes(analysis_result)\n",
        "\n",
        "# Use with FileUtils\n",
        "file_utils = FileUtils()\n",
        "\n",
        "# Save as Excel\n",
        "file_utils.save_data_to_disk(\n",
        "    data=dfs,\n",
        "    output_filetype=\"xlsx\",\n",
        "    output_type=\"processed\",\n",
        "    file_name=\"analysis_results\"\n",
        ")\n",
        "\n",
        "# Convert to dict and save as JSON\n",
        "data_dict = convert_analysis_to_dict(analysis_result)\n",
        "file_utils.save_json(\n",
        "    data=data_dict,\n",
        "    file_path=\"analysis_results\",\n",
        "    output_type=\"processed\"\n",
        ")\n",
        "\n",
        "# Get summary and save\n",
        "summary_df = get_analysis_summary(analysis_result)\n",
        "file_utils.save_data_to_disk(\n",
        "    data={'Summary': summary_df},\n",
        "    output_filetype=\"xlsx\",\n",
        "    output_type=\"processed\",\n",
        "    file_name=\"analysis_summary\"\n",
        "    )"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 21,
          "data": {
            "text/plain": "({'analysis_summary_20241203_141116': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/basic-cpu/code/Users/topi.jarvinen/semantic-text-analyzer/data/processed/analysis_summary_20241203_141116.xlsx'},\n None)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 21,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733235077225
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Save results\n",
        "# file_utils.save_data_to_disk(\n",
        "#     data=result.to_dict(),\n",
        "#     output_filetype=\"csv\",\n",
        "#     output_type=\"processed\",\n",
        "#     file_name=\"analysis_result\"\n",
        "# )"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'CompleteAnalysisResult' object has no attribute 'to_dict'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Save results\u001b[39;00m\n\u001b[1;32m      2\u001b[0m file_utils\u001b[38;5;241m.\u001b[39msave_data_to_disk(\n\u001b[0;32m----> 3\u001b[0m     data\u001b[38;5;241m=\u001b[39m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m(),\n\u001b[1;32m      4\u001b[0m     output_filetype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     output_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessed\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     file_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manalysis_result\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m )\n",
            "File \u001b[0;32m/anaconda/envs/semantic-analyzer/lib/python3.9/site-packages/pydantic/main.py:892\u001b[0m, in \u001b[0;36mBaseModel.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    889\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    891\u001b[0m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[0;32m--> 892\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'CompleteAnalysisResult' object has no attribute 'to_dict'"
          ]
        }
      ],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733234300859
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "semantic-analyzer",
      "language": "python",
      "display_name": "Python (semantic-analyzer)"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.20",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "semantic-analyzer"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}