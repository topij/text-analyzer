{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Import required modules\n",
        "import sys\n",
        "from pathlib import Path\n",
        "# from typing import List, Dict, Any, Tuple, Union\n",
        "# import logging\n",
        "# import asyncio\n",
        "\n",
        "# Add project root to Python path if needed\n",
        "project_root = str(Path().resolve().parent)\n",
        "if project_root not in sys.path:\n",
        "    sys.path.append(project_root)\n",
        "print(f\"Project root: {project_root}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Project root: /mnt/batch/tasks/shared/LS_root/mounts/clusters/basic-cpu/code/Users/topi.jarvinen/semantic-text-analyzer\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733581549598
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# azure_notebook.ipynb\n",
        "import os\n",
        "from src.az_helpers.setup_azure import init_azure_ml # setup_environment\n",
        "from src.semantic_analyzer import SemanticAnalyzer"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1733581572502
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from src.az_helpers.az_environment import setup_notebook_env, verify_environment"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733581572696
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up environment and logging\n",
        "setup_notebook_env(log_level=\"DEBUG\")\n",
        "verify_environment()\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "2024-12-07 14:26:12,522 - FileUtils.core.file_utils - INFO - Project root: /mnt/batch/tasks/shared/LS_root/mounts/clusters/basic-cpu/code/Users/topi.jarvinen/semantic-text-analyzer\n2024-12-07 14:26:12,565 - FileUtils.core.file_utils - INFO - FileUtils initialized with local storage\n2024-12-07 14:26:12,611 - FileUtils.core.file_utils - INFO - Project root: /mnt/batch/tasks/shared/LS_root/mounts/clusters/basic-cpu/code/Users/topi.jarvinen/semantic-text-analyzer\n2024-12-07 14:26:12,619 - FileUtils.core.file_utils - INFO - FileUtils initialized with local storage\nEnvironment Check Results:\n==================================================\n✓ Project root in path\n✓ FileUtils initialized\n✓ .env file loaded\n✓ OPENAI_API_KEY set\n✓ ANTHROPIC_API_KEY set\n✓ Raw data exists\n✓ Processed data exists\n✓ Configuration exists\n✓ Main config.yaml exists\n\n==================================================\nEnvironment Status: Ready ✓\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733581572972
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup environment\n",
        "from FileUtils import FileUtils\n",
        "file_utils = FileUtils()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "2024-12-07 14:26:53,485 - FileUtils.core.file_utils - INFO - Project root: /mnt/batch/tasks/shared/LS_root/mounts/clusters/basic-cpu/code/Users/topi.jarvinen/semantic-text-analyzer\n2024-12-07 14:26:53,541 - FileUtils.core.file_utils - INFO - FileUtils initialized with local storage\n"
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733581613754
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize analyzer\n",
        "analyzer = SemanticAnalyzer(\n",
        "    parameter_file=\"azure://parameters/parameters_en.xlsx\",\n",
        "    file_utils=file_utils\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "2024-12-07 14:26:56,537 - FileUtils.core.file_utils - INFO - Project root: /mnt/batch/tasks/shared/LS_root/mounts/clusters/basic-cpu/code/Users/topi.jarvinen/semantic-text-analyzer\n2024-12-07 14:26:56,539 - FileUtils.core.file_utils - INFO - FileUtils initialized with local storage\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "INFO: FileUtils initialized with local storage\nINFO: Semantic analyzer initialization complete\n"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733581632532
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test analysis\n",
        "text = \"Machine learning models process data efficiently.\"\n",
        "result = await analyzer.analyze(text)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "INFO: HTTP Request: POST https://ri-feedback-analysis.openai.azure.com/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-02-15-preview \"HTTP/1.1 200 OK\"\nINFO: HTTP Request: POST https://ri-feedback-analysis.openai.azure.com/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-02-15-preview \"HTTP/1.1 200 OK\"\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\nProcessing response: {'categories': [{'category': 'Machine Learning', 'confidence': 0.85, 'explanation': 'The text discusses machine learning models, which are a core component of the machine learning domain.', 'evidence': [{'text': 'Machine learning models process data efficiently.', 'relevance': 0.9, 'matched_keywords': ['machine', 'model', 'process', 'data', 'efficiently'], 'context': 'The entire sentence focuses on the functionality of machine learning models.'}], 'themes': ['artificial intelligence', 'data processing']}], 'relationships': {'Machine Learning': ['Artificial Intelligence', 'Data Science']}}\n\nProcessing category: {'category': 'Machine Learning', 'confidence': 0.85, 'explanation': 'The text discusses machine learning models, which are a core component of the machine learning domain.', 'evidence': [{'text': 'Machine learning models process data efficiently.', 'relevance': 0.9, 'matched_keywords': ['machine', 'model', 'process', 'data', 'efficiently'], 'context': 'The entire sentence focuses on the functionality of machine learning models.'}], 'themes': ['artificial intelligence', 'data processing']}\nCreated category: name='Machine Learning' confidence=0.85 description='The text discusses machine learning models, which are a core component of the machine learning domain.' evidence=[Evidence(text='Machine learning models process data efficiently.', relevance=0.9)] themes=['artificial intelligence', 'data processing']\n\nFinal categories: [CategoryMatch(name='Machine Learning', confidence=0.85, description='The text discusses machine learning models, which are a core component of the machine learning domain.', evidence=[Evidence(text='Machine learning models process data efficiently.', relevance=0.9)], themes=['artificial intelligence', 'data processing'])]\n\nRaw LLM response: {'themes': [{'name': 'Efficiency in Data Processing', 'description': 'The theme focuses on how machine learning models enhance the efficiency of data processing, highlighting the importance of speed and effectiveness in handling large datasets.', 'confidence': 0.95, 'keywords': ['efficiency', 'data processing'], 'domain': 'general content analysis'}, {'name': 'Machine Learning Models', 'description': 'This theme encompasses the concept of machine learning models, their structure, and their role in learning from data to make predictions or decisions.', 'confidence': 0.9, 'keywords': ['machine learning', 'models'], 'domain': 'general content analysis', 'parent_theme': 'Efficiency in Data Processing'}, {'name': 'Data Handling', 'description': 'This theme pertains to the methods and techniques used in managing and processing data, which is a critical aspect of machine learning.', 'confidence': 0.85, 'keywords': ['data', 'process'], 'domain': 'general content analysis', 'parent_theme': 'Efficiency in Data Processing'}], 'evidence': {'Efficiency in Data Processing': [{'text': 'Machine learning models process data efficiently.', 'relevance': 0.9, 'keywords': ['efficiently', 'process']}], 'Machine Learning Models': [{'text': 'Machine learning models process data efficiently.', 'relevance': 0.9, 'keywords': ['machine learning', 'models']}], 'Data Handling': [{'text': 'Machine learning models process data efficiently.', 'relevance': 0.9, 'keywords': ['data', 'process']}]}, 'relationships': {'Efficiency in Data Processing': ['Machine Learning Models', 'Data Handling'], 'Machine Learning Models': [], 'Data Handling': []}}\n\nProcessed LLM response: {'themes': [{'name': 'Efficiency in Data Processing', 'description': 'The theme focuses on how machine learning models enhance the efficiency of data processing, highlighting the importance of speed and effectiveness in handling large datasets.', 'confidence': 0.95, 'keywords': ['efficiency', 'data processing'], 'domain': 'general content analysis'}, {'name': 'Machine Learning Models', 'description': 'This theme encompasses the concept of machine learning models, their structure, and their role in learning from data to make predictions or decisions.', 'confidence': 0.9, 'keywords': ['machine learning', 'models'], 'domain': 'general content analysis', 'parent_theme': 'Efficiency in Data Processing'}, {'name': 'Data Handling', 'description': 'This theme pertains to the methods and techniques used in managing and processing data, which is a critical aspect of machine learning.', 'confidence': 0.85, 'keywords': ['data', 'process'], 'domain': 'general content analysis', 'parent_theme': 'Efficiency in Data Processing'}], 'evidence': {'Efficiency in Data Processing': [{'text': 'Machine learning models process data efficiently.', 'relevance': 0.9, 'keywords': ['efficiently', 'process']}], 'Machine Learning Models': [{'text': 'Machine learning models process data efficiently.', 'relevance': 0.9, 'keywords': ['machine learning', 'models']}], 'Data Handling': [{'text': 'Machine learning models process data efficiently.', 'relevance': 0.9, 'keywords': ['data', 'process']}]}, 'relationships': {'Efficiency in Data Processing': ['Machine Learning Models', 'Data Handling'], 'Machine Learning Models': [], 'Data Handling': []}}\n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733580118371
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import libvoikko"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733582457602
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dir(libvoikko)\n",
        "libvoikko?"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733582730928
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Helper functions for handling analysis results.\"\"\"\n",
        "\n",
        "from typing import Any, Dict, Optional, Union\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "from FileUtils import FileUtils, OutputFileType\n",
        "\n",
        "\n",
        "def save_analysis_to_excel(\n",
        "    result: Any,\n",
        "    file_utils: FileUtils,\n",
        "    file_name: str = \"analysis_results\",\n",
        "    output_type: str = \"processed\",\n",
        "    include_timestamp: Optional[bool] = None,\n",
        "    include_metadata: bool = True\n",
        ") -> Dict[str, str]:\n",
        "    \"\"\"Convert analysis result to DataFrames and save to Excel.\n",
        "    \n",
        "    Args:\n",
        "        result: Analysis result object\n",
        "        file_utils: FileUtils instance\n",
        "        file_name: Name for the output file\n",
        "        output_type: Type of output directory (e.g., \"processed\")\n",
        "        include_timestamp: Whether to include timestamp in filename\n",
        "        include_metadata: Whether to include metadata sheet\n",
        "    \n",
        "    Returns:\n",
        "        Dict[str, str]: Dictionary of saved file paths\n",
        "        \n",
        "    Example:\n",
        "        >>> file_utils = FileUtils()\n",
        "        >>> save_analysis_to_excel(analysis_result, file_utils)\n",
        "    \"\"\"\n",
        "    # Convert keywords to DataFrame\n",
        "    keywords_df = pd.DataFrame([\n",
        "        {\n",
        "            'keyword': k.keyword,\n",
        "            'score': k.score,\n",
        "            'domain': k.domain,\n",
        "            'compound_parts': ', '.join(k.compound_parts)\n",
        "        } for k in result.keywords.keywords\n",
        "    ])\n",
        "    \n",
        "    # Convert themes to DataFrame\n",
        "    themes_df = pd.DataFrame([\n",
        "        {\n",
        "            'name': t.name,\n",
        "            'description': t.description,\n",
        "            'confidence': t.confidence,\n",
        "            'keywords': ', '.join(t.keywords)\n",
        "        } for t in result.themes.themes\n",
        "    ])\n",
        "    \n",
        "    # Convert categories to DataFrame\n",
        "    categories_df = pd.DataFrame([\n",
        "        {\n",
        "            'name': m.name,\n",
        "            'confidence': m.confidence,\n",
        "            'description': m.description,\n",
        "            'themes': ', '.join(m.themes),\n",
        "            'evidence': '\\n'.join([e.text for e in m.evidence]) if hasattr(m, 'evidence') else ''\n",
        "        } for m in result.categories.matches\n",
        "    ])\n",
        "    \n",
        "    # Prepare data dictionary\n",
        "    data_dict = {\n",
        "        'Keywords': keywords_df,\n",
        "        'Themes': themes_df,\n",
        "        'Categories': categories_df,\n",
        "    }\n",
        "    \n",
        "    # Add metadata if requested\n",
        "    if include_metadata:\n",
        "        metadata_df = pd.DataFrame([{\n",
        "            'language': result.language,\n",
        "            'success': result.success,\n",
        "            'processing_time': result.processing_time,\n",
        "            'keywords_language': result.keywords.language,\n",
        "            'themes_language': result.themes.language,\n",
        "            'categories_language': result.categories.language,\n",
        "            'compound_words': ', '.join(result.keywords.compound_words)\n",
        "        }])\n",
        "        data_dict['Metadata'] = metadata_df\n",
        "    \n",
        "    # Save to Excel\n",
        "    saved_files, _ = file_utils.save_data_to_storage(\n",
        "        data=data_dict,\n",
        "        output_filetype=OutputFileType.XLSX,\n",
        "        output_type=output_type,\n",
        "        file_name=file_name,\n",
        "        include_timestamp=include_timestamp\n",
        "    )\n",
        "    \n",
        "    return saved_files\n",
        "\n",
        "\n",
        "def read_analysis_from_excel(\n",
        "    file_path: Union[str, Path],\n",
        "    file_utils: FileUtils,\n",
        "    input_type: str = \"processed\"\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"Read analysis results from Excel file.\n",
        "    \n",
        "    Args:\n",
        "        file_path: Path to Excel file\n",
        "        file_utils: FileUtils instance\n",
        "        input_type: Type of input directory (e.g., \"processed\")\n",
        "    \n",
        "    Returns:\n",
        "        Dict[str, pd.DataFrame]: Dictionary of DataFrames for each sheet\n",
        "        \n",
        "    Example:\n",
        "        >>> file_utils = FileUtils()\n",
        "        >>> data = read_analysis_from_excel(\"analysis_results.xlsx\", file_utils)\n",
        "        >>> keywords_df = data['Keywords']\n",
        "    \"\"\"\n",
        "    return file_utils.load_excel_sheets(file_path, input_type=input_type)"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733580148009
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "analysis_result = result"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733580154604
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Converter functions for analysis results.\"\"\"\n",
        "\n",
        "from typing import Any, Dict, List, Optional, Union\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def convert_analysis_to_dataframes(result: Any) -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"Convert analysis result to a dictionary of DataFrames.\n",
        "    \n",
        "    Args:\n",
        "        result: Analysis result object\n",
        "        \n",
        "    Returns:\n",
        "        Dict[str, pd.DataFrame]: Dictionary with DataFrames for each component\n",
        "        \n",
        "    Example:\n",
        "        >>> dfs = convert_analysis_to_dataframes(analysis_result)\n",
        "        >>> file_utils.save_data_to_storage(\n",
        "        ...     data=dfs,\n",
        "        ...     output_filetype=\"xlsx\",\n",
        "        ...     file_name=\"analysis\"\n",
        "        ... )\n",
        "    \"\"\"\n",
        "    # Keywords DataFrame\n",
        "    keywords_df = pd.DataFrame([\n",
        "        {\n",
        "            'keyword': k.keyword,\n",
        "            'score': k.score,\n",
        "            'domain': k.domain,\n",
        "            'compound_parts': ', '.join(k.compound_parts)\n",
        "        } for k in result.keywords.keywords\n",
        "    ])\n",
        "    \n",
        "    # Themes DataFrame\n",
        "    themes_df = pd.DataFrame([\n",
        "        {\n",
        "            'name': t.name,\n",
        "            'description': t.description,\n",
        "            'confidence': t.confidence,\n",
        "            'keywords': ', '.join(t.keywords)\n",
        "        } for t in result.themes.themes\n",
        "    ])\n",
        "    \n",
        "    # Categories DataFrame\n",
        "    categories_df = pd.DataFrame([\n",
        "        {\n",
        "            'name': m.name,\n",
        "            'confidence': m.confidence,\n",
        "            'description': m.description,\n",
        "            'themes': ', '.join(m.themes),\n",
        "            'evidence': '\\n'.join([e.text for e in m.evidence]) if hasattr(m, 'evidence') else ''\n",
        "        } for m in result.categories.matches\n",
        "    ])\n",
        "    \n",
        "    # Metadata DataFrame\n",
        "    metadata_df = pd.DataFrame([{\n",
        "        'language': result.language,\n",
        "        'success': result.success,\n",
        "        'processing_time': result.processing_time,\n",
        "        'keywords_language': result.keywords.language,\n",
        "        'themes_language': result.themes.language,\n",
        "        'categories_language': result.categories.language,\n",
        "        'compound_words': ', '.join(result.keywords.compound_words)\n",
        "    }])\n",
        "    \n",
        "    return {\n",
        "        'Keywords': keywords_df,\n",
        "        'Themes': themes_df,\n",
        "        'Categories': categories_df,\n",
        "        'Metadata': metadata_df\n",
        "    }\n",
        "\n",
        "\n",
        "def convert_analysis_to_dict(result: Any) -> Dict[str, Any]:\n",
        "    \"\"\"Convert analysis result to a nested dictionary.\n",
        "    \n",
        "    Args:\n",
        "        result: Analysis result object\n",
        "        \n",
        "    Returns:\n",
        "        Dict[str, Any]: Dictionary representation of the analysis result\n",
        "        \n",
        "    Example:\n",
        "        >>> data_dict = convert_analysis_to_dict(analysis_result)\n",
        "        >>> file_utils.save_json(\n",
        "        ...     data=data_dict,\n",
        "        ...     file_path=\"analysis\"\n",
        "        ... )\n",
        "    \"\"\"\n",
        "    return {\n",
        "        'keywords': {\n",
        "            'items': [\n",
        "                {\n",
        "                    'keyword': k.keyword,\n",
        "                    'score': k.score,\n",
        "                    'domain': k.domain,\n",
        "                    'compound_parts': k.compound_parts\n",
        "                } for k in result.keywords.keywords\n",
        "            ],\n",
        "            'compound_words': result.keywords.compound_words,\n",
        "            'language': result.keywords.language,\n",
        "            'success': result.keywords.success,\n",
        "            'error': result.keywords.error\n",
        "        },\n",
        "        'themes': {\n",
        "            'items': [\n",
        "                {\n",
        "                    'name': t.name,\n",
        "                    'description': t.description,\n",
        "                    'confidence': t.confidence,\n",
        "                    'keywords': t.keywords,\n",
        "                    'parent_theme': t.parent_theme\n",
        "                } for t in result.themes.themes\n",
        "            ],\n",
        "            'theme_hierarchy': result.themes.theme_hierarchy,\n",
        "            'language': result.themes.language,\n",
        "            'success': result.themes.success,\n",
        "            'error': result.themes.error\n",
        "        },\n",
        "        'categories': {\n",
        "            'matches': [\n",
        "                {\n",
        "                    'name': m.name,\n",
        "                    'confidence': m.confidence,\n",
        "                    'description': m.description,\n",
        "                    'evidence': [\n",
        "                        {\n",
        "                            'text': e.text,\n",
        "                            'relevance': e.relevance\n",
        "                        } for e in m.evidence\n",
        "                    ] if hasattr(m, 'evidence') else [],\n",
        "                    'themes': m.themes\n",
        "                } for m in result.categories.matches\n",
        "            ],\n",
        "            'language': result.categories.language,\n",
        "            'success': result.categories.success,\n",
        "            'error': result.categories.error\n",
        "        },\n",
        "        'metadata': {\n",
        "            'language': result.language,\n",
        "            'success': result.success,\n",
        "            'error': result.error,\n",
        "            'processing_time': result.processing_time\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "def get_analysis_summary(result: Any) -> pd.DataFrame:\n",
        "    \"\"\"Create a summary DataFrame of the analysis results.\n",
        "    \n",
        "    Args:\n",
        "        result: Analysis result object\n",
        "        \n",
        "    Returns:\n",
        "        pd.DataFrame: Summary of key findings\n",
        "        \n",
        "    Example:\n",
        "        >>> summary_df = get_analysis_summary(analysis_result)\n",
        "        >>> file_utils.save_data_to_storage(\n",
        "        ...     data={'Summary': summary_df},\n",
        "        ...     output_filetype=\"xlsx\",\n",
        "        ...     file_name=\"analysis_summary\"\n",
        "        ... )\n",
        "    \"\"\"\n",
        "    # Get top keywords by score\n",
        "    top_keywords = sorted(\n",
        "        result.keywords.keywords,\n",
        "        key=lambda k: k.score,\n",
        "        reverse=True\n",
        "    )[:5]\n",
        "    \n",
        "    # Get top themes by confidence\n",
        "    top_themes = sorted(\n",
        "        result.themes.themes,\n",
        "        key=lambda t: t.confidence,\n",
        "        reverse=True\n",
        "    )[:5]\n",
        "    \n",
        "    # Create summary rows\n",
        "    rows = []\n",
        "    \n",
        "    # Add keyword information\n",
        "    rows.append({\n",
        "        'Category': 'Top Keywords',\n",
        "        'Item': ', '.join(k.keyword for k in top_keywords),\n",
        "        'Score': ', '.join(f\"{k.score:.2f}\" for k in top_keywords)\n",
        "    })\n",
        "    \n",
        "    # Add theme information\n",
        "    for theme in top_themes:\n",
        "        rows.append({\n",
        "            'Category': 'Theme',\n",
        "            'Item': theme.name,\n",
        "            'Description': theme.description,\n",
        "            'Score': f\"{theme.confidence:.2f}\"\n",
        "        })\n",
        "    \n",
        "    # Add metadata\n",
        "    rows.append({\n",
        "        'Category': 'Metadata',\n",
        "        'Item': 'Processing Time',\n",
        "        'Score': f\"{result.processing_time:.2f}s\"\n",
        "    })\n",
        "    \n",
        "    return pd.DataFrame(rows)"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733580157767
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "# if __name__ == \"__main__\":\n",
        "# Convert to DataFrames\n",
        "dfs = convert_analysis_to_dataframes(analysis_result)\n",
        "\n",
        "# Use with FileUtils\n",
        "file_utils = FileUtils()\n",
        "\n",
        "# Save as Excel\n",
        "file_utils.save_data_to_storage(\n",
        "    data=dfs,\n",
        "    output_filetype=\"xlsx\",\n",
        "    output_type=\"processed\",\n",
        "    file_name=\"analysis_results\"\n",
        ")\n",
        "\n",
        "# Convert to dict and save as JSON\n",
        "data_dict = convert_analysis_to_dict(analysis_result)\n",
        "file_utils.save_json(\n",
        "    data=data_dict,\n",
        "    file_path=\"analysis_results\",\n",
        "    output_type=\"processed\"\n",
        ")\n",
        "\n",
        "# Get summary and save\n",
        "summary_df = get_analysis_summary(analysis_result)\n",
        "file_utils.save_data_to_storage(\n",
        "    data={'Summary': summary_df},\n",
        "    output_filetype=\"xlsx\",\n",
        "    output_type=\"processed\",\n",
        "    file_name=\"analysis_summary\"\n",
        "    )"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "INFO: Project root: /mnt/batch/tasks/shared/LS_root/mounts/clusters/basic-cpu/code/Users/topi.jarvinen/semantic-text-analyzer\nINFO: FileUtils initialized with local storage\nINFO: Data saved successfully: {'analysis_results_20241207_140246': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/basic-cpu/code/Users/topi.jarvinen/semantic-text-analyzer/data/processed/analysis_results_20241207_140246.xlsx'}\n"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'FileUtils' object has no attribute 'save_json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[11], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Convert to dict and save as JSON\u001b[39;00m\n\u001b[1;32m     18\u001b[0m data_dict \u001b[38;5;241m=\u001b[39m convert_analysis_to_dict(analysis_result)\n\u001b[0;32m---> 19\u001b[0m \u001b[43mfile_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_json\u001b[49m(\n\u001b[1;32m     20\u001b[0m     data\u001b[38;5;241m=\u001b[39mdata_dict,\n\u001b[1;32m     21\u001b[0m     file_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manalysis_results\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     22\u001b[0m     output_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Get summary and save\u001b[39;00m\n\u001b[1;32m     26\u001b[0m summary_df \u001b[38;5;241m=\u001b[39m get_analysis_summary(analysis_result)\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'FileUtils' object has no attribute 'save_json'"
          ]
        }
      ],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733580168854
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Save results\n",
        "# file_utils.save_data_to_storage(\n",
        "#     data=result.to_dict(),\n",
        "#     output_filetype=\"csv\",\n",
        "#     output_type=\"processed\",\n",
        "#     file_name=\"analysis_result\"\n",
        "# )"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'CompleteAnalysisResult' object has no attribute 'to_dict'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Save results\u001b[39;00m\n\u001b[1;32m      2\u001b[0m file_utils\u001b[38;5;241m.\u001b[39msave_data_to_storage(\n\u001b[0;32m----> 3\u001b[0m     data\u001b[38;5;241m=\u001b[39m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m(),\n\u001b[1;32m      4\u001b[0m     output_filetype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     output_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessed\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     file_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manalysis_result\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m )\n",
            "File \u001b[0;32m/anaconda/envs/semantic-analyzer/lib/python3.9/site-packages/pydantic/main.py:892\u001b[0m, in \u001b[0;36mBaseModel.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    889\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    891\u001b[0m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[0;32m--> 892\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'CompleteAnalysisResult' object has no attribute 'to_dict'"
          ]
        }
      ],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733234300859
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "semantic-analyzer",
      "language": "python",
      "display_name": "Python (semantic-analyzer)"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.21",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "semantic-analyzer"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}