{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple, Union\n",
    "import logging\n",
    "import asyncio\n",
    "\n",
    "# Add project root to Python path if needed\n",
    "project_root = str(Path().resolve().parent)\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'setup_notebook_env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Set up environment and logging\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43msetup_notebook_env\u001b[49m(log_level\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDEBUG\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m verify_environment()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'setup_notebook_env' is not defined"
     ]
    }
   ],
   "source": [
    "# Set up environment and logging\n",
    "setup_notebook_env(log_level=\"DEBUG\")\n",
    "verify_environment()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data to usetest_texts = {\n",
    "    \"en\": {\n",
    "        \"technical\": \"\"\"Machine learning models are trained using large datasets to recognize patterns. \n",
    "                     The neural network architecture includes multiple layers for feature extraction. \n",
    "                     Data preprocessing and feature engineering are crucial steps.\"\"\",\n",
    "        \"business\": \"\"\"Q3 financial results show 15% revenue growth and improved profit margins. \n",
    "                    Customer acquisition costs decreased while retention rates increased. \n",
    "                    Market expansion strategy focuses on emerging technology sectors.\"\"\"\n",
    "    },\n",
    "    \"fi\": {\n",
    "        \"technical\": \"\"\"Koneoppimismalleja koulutetaan suurilla datajoukolla tunnistamaan kaavoja. \n",
    "                     Neuroverkon arkkitehtuuri sisältää useita kerroksia piirteiden erottamiseen. \n",
    "                     Datan esikäsittely ja piirteiden suunnittelu ovat keskeisiä vaiheita.\"\"\",\n",
    "        \"business\": \"\"\"Q3 taloudelliset tulokset osoittavat 15% liikevaihdon kasvun ja parantuneet katteet. \n",
    "                    Asiakashankinnan kustannukset laskivat ja asiakaspysyvyys parani. \n",
    "                    Markkinalaajennusstrategia keskittyy nouseviin teknologiasektoreihin.\"\"\"\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Test individual keyword analyzer\n",
    "async def test_keyword_analyzer():\n",
    "    \"\"\"Test keyword analyzer with different languages.\"\"\"\n",
    "    print(\"Testing Keyword Analyzer\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Initialize components\n",
    "    parameter_handler = ParameterHandler(\"parameters_fi.xlsx\")\n",
    "    llm = create_llm()\n",
    "    \n",
    "    # Test English\n",
    "    print(\"\\nTesting English Technical Content:\")\n",
    "    en_processor = create_text_processor(language=\"en\")\n",
    "    keyword_analyzer_en = KeywordAnalyzer(\n",
    "        llm=llm,\n",
    "        config=parameter_handler.parameters.general.model_dump(),\n",
    "        language_processor=en_processor\n",
    "    )\n",
    "    await test_individual_analyzer(keyword_analyzer_en, test_texts[\"en\"][\"technical\"], \"Keyword\")\n",
    "    \n",
    "    # Test Finnish\n",
    "    print(\"\\nTesting Finnish Technical Content:\")\n",
    "    fi_processor = create_text_processor(language=\"fi\")\n",
    "    keyword_analyzer_fi = KeywordAnalyzer(\n",
    "        llm=llm,\n",
    "        config=parameter_handler.parameters.general.model_dump(),\n",
    "        language_processor=fi_processor\n",
    "    )\n",
    "    await test_individual_analyzer(keyword_analyzer_fi, test_texts[\"fi\"][\"technical\"], \"Keyword\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Test all components\n",
    "async def test_components_for_language(language: str):\n",
    "    \"\"\"Test all components for a specific language.\"\"\"\n",
    "    print(f\"\\nTesting All Components for {language.upper()}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Initialize components\n",
    "    parameter_handler = ParameterHandler(f\"parameters_{language}.xlsx\")\n",
    "    llm = create_llm()\n",
    "    language_processor = create_text_processor(language=language)\n",
    "    \n",
    "    # Create analyzers\n",
    "    keyword_analyzer = KeywordAnalyzer(\n",
    "        llm=llm,\n",
    "        config=parameter_handler.parameters.general.model_dump(),\n",
    "        language_processor=language_processor\n",
    "    )\n",
    "    \n",
    "    theme_analyzer = ThemeAnalyzer(\n",
    "        llm=llm,\n",
    "        config=parameter_handler.parameters.general.model_dump(),\n",
    "        language_processor=language_processor\n",
    "    )\n",
    "    \n",
    "    category_analyzer = CategoryAnalyzer(\n",
    "        categories=parameter_handler.parameters.categories,\n",
    "        llm=llm,\n",
    "        config=parameter_handler.parameters.general.model_dump(),\n",
    "        language_processor=language_processor\n",
    "    )\n",
    "    \n",
    "    # Test technical content\n",
    "    print(f\"\\nTesting {language.upper()} Technical Content:\")\n",
    "    await test_individual_analyzer(keyword_analyzer, test_texts[language][\"technical\"], \"Keyword\")\n",
    "    await test_individual_analyzer(theme_analyzer, test_texts[language][\"technical\"], \"Theme\")\n",
    "    await test_individual_analyzer(category_analyzer, test_texts[language][\"technical\"], \"Category\")\n",
    "    \n",
    "    # Test business content\n",
    "    print(f\"\\nTesting {language.upper()} Business Content:\")\n",
    "    await test_individual_analyzer(keyword_analyzer, test_texts[language][\"business\"], \"Keyword\")\n",
    "    await test_individual_analyzer(theme_analyzer, test_texts[language][\"business\"], \"Theme\")\n",
    "    await test_individual_analyzer(category_analyzer, test_texts[language][\"business\"], \"Category\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Quick test of full pipeline\n",
    "async def test_pipeline():\n",
    "    \"\"\"Test full pipeline with both languages.\"\"\"\n",
    "    print(\"Testing Full Pipeline\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Test English pipeline\n",
    "    print(\"\\nEnglish Pipeline:\")\n",
    "    en_analyzer = SemanticAnalyzer(parameter_file=\"parameters_en.xlsx\")\n",
    "    result = await en_analyzer.analyze(test_texts[\"en\"][\"technical\"])\n",
    "    print(f\"Success: {result.success}\")\n",
    "    print(f\"Keywords found: {len(result.keywords.keywords)}\")\n",
    "    print(f\"Themes found: {len(result.themes.themes)}\")\n",
    "    print(f\"Categories found: {len(result.categories.matches)}\")\n",
    "    \n",
    "    # Test Finnish pipeline\n",
    "    print(\"\\nFinnish Pipeline:\")\n",
    "    fi_analyzer = SemanticAnalyzer(parameter_file=\"parameters_fi.xlsx\")\n",
    "    result = await fi_analyzer.analyze(test_texts[\"fi\"][\"technical\"])\n",
    "    print(f\"Success: {result.success}\")\n",
    "    print(f\"Keywords found: {len(result.keywords.keywords)}\")\n",
    "    print(f\"Themes found: {len(result.themes.themes)}\")\n",
    "    print(f\"Categories found: {len(result.categories.matches)}\")\n",
    "\n",
    "# Run the tests\n",
    "async def run_all_tests():\n",
    "    \"\"\"Run all tests.\"\"\"\n",
    "    # Test individual component\n",
    "    await test_keyword_analyzer()\n",
    "    \n",
    "    # Test all components by language\n",
    "    await test_components_for_language(\"en\")\n",
    "    await test_components_for_language(\"fi\")\n",
    "    \n",
    "    # Test full pipeline\n",
    "    await test_pipeline()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run in notebook\n",
    "# await run_all_tests()\n",
    "\n",
    "# Or run individual tests:\n",
    "# await test_keyword_analyzer()\n",
    "await test_components_for_language(\"fi\")\n",
    "# await test_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FileUtils import FileUtils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary components\n",
    "from src.nb_helpers.environment import setup_notebook_env, verify_environment\n",
    "from src.semantic_analyzer import SemanticAnalyzer\n",
    "from src.utils.FileUtils.file_utils import FileUtils\n",
    "from src.core.language_processing import create_text_processor\n",
    "from src.core.llm.factory import create_llm\n",
    "from src.loaders.parameter_handler import ParameterHandler\n",
    "from src.analyzers.keyword_analyzer import KeywordAnalyzer\n",
    "from src.analyzers.theme_analyzer import ThemeAnalyzer\n",
    "from src.analyzers.category_analyzer import CategoryAnalyzer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.FileUtils import FileUtils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import FileUtils"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semantic-analyzer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
