{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any\n",
    "import logging\n",
    "import asyncio\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Add project root to Python path if needed\n",
    "project_root = str(Path().resolve().parent)\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Import necessary components\n",
    "from src.nb_helpers.environment import setup_notebook_env, verify_environment\n",
    "from src.semantic_analyzer import SemanticAnalyzer\n",
    "from src.core.config import AnalyzerConfig\n",
    "from src.core.language_processing import create_text_processor\n",
    "from src.core.llm.factory import create_llm\n",
    "from src.utils.output_formatter import (\n",
    "    ExcelFormatter, DetailedFormatter, OutputDetail, ExcelOutputConfig, BaseColumnFormat\n",
    ")\n",
    "from FileUtils import FileUtils, OutputFileType\n",
    "\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-12 14:00:34,900 - FileUtils.core.file_utils - INFO - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n",
      "2024-12-12 14:00:34,906 - FileUtils.core.file_utils - INFO - FileUtils initialized with local storage\n",
      "2024-12-12 14:00:34,980 - FileUtils.core.file_utils - INFO - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n",
      "2024-12-12 14:00:34,988 - FileUtils.core.file_utils - INFO - FileUtils initialized with local storage\n",
      "Environment Check Results:\n",
      "==================================================\n",
      "\n",
      "Basic Setup:\n",
      "-----------\n",
      "✓ Project root in path\n",
      "✓ FileUtils initialized\n",
      "✓ .env file loaded\n",
      "\n",
      "Environment Variables:\n",
      "---------------------\n",
      "✓ OPENAI_API_KEY set\n",
      "✓ ANTHROPIC_API_KEY set\n",
      "\n",
      "Project Structure:\n",
      "-----------------\n",
      "✓ Raw data exists\n",
      "✓ Processed data exists\n",
      "✓ Configuration exists\n",
      "✓ Main config.yaml exists\n",
      "\n",
      "==================================================\n",
      "Environment Status: Ready ✓\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up environment and verify\n",
    "setup_notebook_env(log_level=\"DEBUG\")\n",
    "verify_environment()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-12 14:00:35,081 - FileUtils.core.file_utils - INFO - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n",
      "2024-12-12 14:00:35,087 - FileUtils.core.file_utils - INFO - FileUtils initialized with local storage\n",
      "2024-12-12 14:00:35,123 - FileUtils.core.file_utils - INFO - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n",
      "2024-12-12 14:00:35,128 - FileUtils.core.file_utils - INFO - FileUtils initialized with local storage\n",
      "2024-12-12 14:00:42,695 - FileUtils.core.file_utils - INFO - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-12 14:00:42,701 - FileUtils.core.file_utils - INFO - FileUtils initialized with local storage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: FileUtils initialized with local storage\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-12 14:00:42,740 - FileUtils.core.file_utils - INFO - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-12 14:00:42,748 - FileUtils.core.file_utils - INFO - FileUtils initialized with local storage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: FileUtils initialized with local storage\n",
      "INFO: Loaded 747 stopwords from c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\configurations\\stop_words\\fi.txt\n",
      "DEBUG: Initialized with config: {}\n",
      "DEBUG: Trying library paths: ['C:\\\\scripts\\\\Voikko\\\\libvoikko-1.dll', 'C:\\\\Program Files\\\\Voikko\\\\libvoikko-1.dll', 'C:\\\\Voikko\\\\libvoikko-1.dll']\n",
      "DEBUG: Trying dictionary paths: ['C:\\\\scripts\\\\Voikko', 'C:\\\\Program Files\\\\Voikko', 'C:\\\\Voikko']\n",
      "DEBUG: Added C:\\scripts\\Voikko to DLL search path\n",
      "INFO: Successfully initialized Voikko with path: C:\\scripts\\Voikko\n"
     ]
    }
   ],
   "source": [
    "# Initialize FileUtils\n",
    "file_utils = FileUtils()\n",
    "\n",
    "# Create analyzer config with OpenAI as default\n",
    "config = AnalyzerConfig()\n",
    "config.config[\"models\"][\"default_provider\"] = \"openai\"\n",
    "config.config[\"models\"][\"default_model\"] = \"gpt-4o-mini\"\n",
    "\n",
    "# Create LLM instance\n",
    "llm = create_llm(provider=\"openai\", config=config)\n",
    "\n",
    "# Initialize language processors\n",
    "en_processor = create_text_processor(language=\"en\")\n",
    "fi_processor = create_text_processor(language=\"fi\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-12 14:02:55,689 - FileUtils.core.file_utils - INFO - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-12 14:02:55,698 - FileUtils.core.file_utils - INFO - FileUtils initialized with local storage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: FileUtils initialized with local storage\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-12 14:02:55,785 - FileUtils.core.file_utils - INFO - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-12 14:02:55,793 - FileUtils.core.file_utils - INFO - FileUtils initialized with local storage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: FileUtils initialized with local storage\n",
      "INFO: Loaded 747 stopwords from c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\configurations\\stop_words\\fi.txt\n",
      "INFO: Loaded 747 stopwords from c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\configurations\\stop_words\\fi.txt\n",
      "DEBUG: Initialized with config: {'min_word_length': 3, 'include_compounds': True}\n",
      "DEBUG: Trying library paths: ['C:\\\\scripts\\\\Voikko\\\\libvoikko-1.dll', 'C:\\\\Program Files\\\\Voikko\\\\libvoikko-1.dll', 'C:\\\\Voikko\\\\libvoikko-1.dll']\n",
      "DEBUG: Trying dictionary paths: ['C:\\\\scripts\\\\Voikko', 'C:\\\\Program Files\\\\Voikko', 'C:\\\\Voikko']\n",
      "DEBUG: Added C:\\scripts\\Voikko to DLL search path\n",
      "INFO: Successfully initialized Voikko with path: C:\\scripts\\Voikko\n",
      "INFO: Semantic analyzer initialization complete\n"
     ]
    }
   ],
   "source": [
    "# Create analyzer with proper initialization\n",
    "analyzer = SemanticAnalyzer(\n",
    "    llm=llm,\n",
    "    file_utils=file_utils,\n",
    "    parameter_file=\"parameters_fi.xlsx\"  # Default to English parameters\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data for demonstrations\n",
    "test_texts = {\n",
    "    \"en\": {\n",
    "        \"technical\": \"\"\"Machine learning models are trained using large datasets to recognize patterns. \n",
    "                    The neural network architecture includes multiple layers for feature extraction. \n",
    "                    Data preprocessing and feature engineering are crucial steps.\"\"\",\n",
    "        \n",
    "        \"business\": \"\"\"Q3 financial results show 15% revenue growth and improved profit margins. \n",
    "                    Customer acquisition costs decreased while retention rates increased. \n",
    "                    Market expansion strategy focuses on emerging technology sectors.\"\"\",\n",
    "        \n",
    "        \"mixed\": \"\"\"Our AI platform leverages machine learning to optimize customer engagement.\n",
    "                 The system analyzes user behavior patterns to improve conversion rates.\n",
    "                 Q2 results showed 25% improvement in customer retention metrics.\"\"\"\n",
    "    },\n",
    "    \"fi\": {\n",
    "        \"technical\": \"\"\"Koneoppimismalleja koulutetaan suurilla datajoukolla tunnistamaan kaavoja. \n",
    "                    Neuroverkon arkkitehtuuri sisältää useita kerroksia piirteiden erottamiseen. \n",
    "                    Datan esikäsittely ja piirteiden suunnittelu ovat keskeisiä vaiheita.\"\"\",\n",
    "        \n",
    "        \"business\": \"\"\"Q3 taloudelliset tulokset osoittavat 15% liikevaihdon kasvun ja parantuneet katteet. \n",
    "                    Asiakashankinnan kustannukset laskivat ja asiakaspysyvyys parani. \n",
    "                    Markkinalaajennusstrategia keskittyy nouseviin teknologiasektoreihin.\"\"\"\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize formatters with different detail levels\n",
    "summary_formatter = ExcelFormatter(\n",
    "    file_utils=file_utils,\n",
    "    config=ExcelOutputConfig(detail_level=OutputDetail.SUMMARY)\n",
    ")\n",
    "\n",
    "detailed_formatter = DetailedFormatter(\n",
    "    file_utils=file_utils,\n",
    "    config=ExcelOutputConfig(detail_level=OutputDetail.DETAILED)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Single Text Analysis with Different Detail Levels\n",
    "async def demonstrate_single_analysis(text: str, language: str = \"en\"):\n",
    "    \"\"\"Demonstrate analysis of single text with different detail levels.\"\"\"\n",
    "    print(f\"Analyzing {language.upper()} text:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(text)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    try:\n",
    "        # Update analyzer's language and configuration\n",
    "        analyzer.set_language(language)\n",
    "        \n",
    "        # Perform analysis\n",
    "        result = await analyzer.analyze(\n",
    "            text,\n",
    "            analysis_types=[\"keywords\", \"themes\", \"categories\"],\n",
    "            language=language  # Pass language explicitly\n",
    "        )\n",
    "\n",
    "        print(\"\\nAnalysis Results:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(result)\n",
    "\n",
    "        # Show summary format\n",
    "        print(\"\\nSummary Output:\")\n",
    "        print(\"-\" * 50)\n",
    "        summary = summary_formatter.format_output(\n",
    "            results={\n",
    "                \"keywords\": result.keywords,\n",
    "                \"themes\": result.themes,\n",
    "                \"categories\": result.categories\n",
    "            },\n",
    "            analysis_types=[\"keywords\", \"themes\", \"categories\"]\n",
    "        )\n",
    "\n",
    "        logger.debug(f\"Summary formatter results: {summary}\")\n",
    "        \n",
    "        for analysis_type, output in summary.items():\n",
    "            print(f\"\\n{analysis_type.title()}:\")\n",
    "            if output:\n",
    "                items = output.split(\"; \")\n",
    "                for item in items:\n",
    "                    print(f\"  • {item}\")\n",
    "            else:\n",
    "                print(\"  No results\")\n",
    "\n",
    "        # Show detailed format\n",
    "        print(\"\\nDetailed Output:\")\n",
    "        print(\"-\" * 50)\n",
    "        detailed = detailed_formatter.format_detailed_output(\n",
    "            results={\n",
    "                \"keywords\": result.keywords,\n",
    "                \"themes\": result.themes,\n",
    "                \"categories\": result.categories\n",
    "            },\n",
    "            analysis_types=[\"keywords\", \"themes\", \"categories\"]\n",
    "        )\n",
    "        \n",
    "        for analysis_type, output in detailed.items():\n",
    "            print(f\"\\n{analysis_type.title()}:\")\n",
    "            print(\"Summary:\")\n",
    "            if output[\"summary\"]:\n",
    "                items = output[\"summary\"].split(\"; \")\n",
    "                for item in items:\n",
    "                    print(f\"  • {item}\")\n",
    "            \n",
    "            print(\"\\nDetails:\")\n",
    "            if isinstance(output[\"details\"], str):\n",
    "                # Handle string format\n",
    "                items = output[\"details\"].split(\"; \")\n",
    "                for item in items:\n",
    "                    print(f\"  • {item}\")\n",
    "            elif isinstance(output[\"details\"], dict):\n",
    "                # Handle dictionary format\n",
    "                for key, value in output[\"details\"].items():\n",
    "                    print(f\"  • {key}:\")\n",
    "                    if isinstance(value, list):\n",
    "                        for item in value:\n",
    "                            print(f\"    - {item}\")\n",
    "                    else:\n",
    "                        print(f\"    {value}\")\n",
    "\n",
    "            print(\"\\nMetadata:\")\n",
    "            for key, value in output[\"metadata\"].items():\n",
    "                print(f\"  • {key}: {value}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Analysis failed: {e}\")\n",
    "        print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def demonstrate_excel_processing_details():\n",
    "    \"\"\"Demonstrate processing of Excel file.\"\"\"\n",
    "    input_filename = \"test_content_short.xlsx\"  # Without extension\n",
    "    output_filename = \"analysis_results\"  # Without extension\n",
    "    text_column = \"keskustelu\"\n",
    "    language_column = \"language\"  # Optional column for language detection\n",
    "\n",
    "    print(f\"Processing Excel file: {input_filename}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    try:\n",
    "        # Load input file using FileUtils\n",
    "        input_df = file_utils.load_single_file(\n",
    "            file_path=input_filename,\n",
    "            input_type=\"raw\"\n",
    "        )\n",
    "\n",
    "        if text_column not in input_df.columns:\n",
    "            print(f\"Error: Column '{text_column}' not found in input file\")\n",
    "            return\n",
    "\n",
    "        # Process each row with appropriate language handling\n",
    "        results = []\n",
    "        for idx, row in input_df.iterrows():\n",
    "            try:\n",
    "                # Determine language\n",
    "                language = row[language_column] if language_column in input_df.columns else \"en\"\n",
    "                \n",
    "                # Update analyzer's language\n",
    "                analyzer.set_language(language)\n",
    "\n",
    "                # Analyze text\n",
    "                result = await analyzer.analyze(\n",
    "                    row[text_column],\n",
    "                    analysis_types=[\"keywords\", \"themes\", \"categories\"],\n",
    "                    language=language\n",
    "                )\n",
    "\n",
    "                # Format results using the formatters\n",
    "                formatted_result = {\n",
    "                    \"keywords\": summary_formatter.format_output(\n",
    "                        {\"keywords\": result.keywords},\n",
    "                        [\"keywords\"]\n",
    "                    ).get(\"keywords\", \"\"),\n",
    "                    \n",
    "                    \"themes\": summary_formatter.format_output(\n",
    "                        {\"themes\": result.themes},\n",
    "                        [\"themes\"]\n",
    "                    ).get(\"themes\", \"\"),\n",
    "                    \n",
    "                    \"categories\": summary_formatter.format_output(\n",
    "                        {\"categories\": result.categories},\n",
    "                        [\"categories\"]\n",
    "                    ).get(\"categories\", \"\"),\n",
    "                    \n",
    "                    \"language\": language\n",
    "                }\n",
    "                \n",
    "                results.append(formatted_result)\n",
    "                print(f\"Processed row {idx + 1}/{len(input_df)} ({language})\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing row {idx + 1}: {e}\")\n",
    "                results.append({\n",
    "                    \"keywords\": f\"Error: {str(e)}\",\n",
    "                    \"themes\": f\"Error: {str(e)}\",\n",
    "                    \"categories\": f\"Error: {str(e)}\",\n",
    "                    \"language\": language\n",
    "                })\n",
    "\n",
    "        # Create results DataFrame\n",
    "        results_df = input_df.copy()\n",
    "        for key in [\"keywords\", \"themes\", \"categories\"]:\n",
    "            results_df[key] = [r.get(key, \"\") for r in results]\n",
    "\n",
    "        # Save using FileUtils\n",
    "        saved_files, _ = file_utils.save_data_to_storage(\n",
    "            data={\"analysis_results\": results_df},\n",
    "            output_type=\"processed\",\n",
    "            file_name=output_filename,\n",
    "            output_filetype=OutputFileType.XLSX,\n",
    "            include_timestamp=True\n",
    "        )\n",
    "\n",
    "        print(\"\\nProcessing complete!\")\n",
    "        output_path = next(iter(saved_files.values()))\n",
    "        print(f\"Results saved to: {output_path}\")\n",
    "        \n",
    "        print(\"\\nFirst few rows of results:\")\n",
    "        pd.set_option('display.max_columns', None)\n",
    "        pd.set_option('display.max_colwidth', 50)\n",
    "        print(results_df.head())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Excel file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def demonstrate_excel_processing(file_name: str = \"test_content_fi.xlsx\", \n",
    "                                       output_filename: str = \"analysis_results\", \n",
    "                                       text_column: str = \"content\", \n",
    "                                       language_column: str = \"language\"):\n",
    "    \"\"\"Demonstrate processing of Excel file with simplified output.\"\"\"\n",
    "    file_name = file_name\n",
    "    output_filename = output_filename\n",
    "    text_column = text_column\n",
    "    language_column = language_column\n",
    "\n",
    "    # Create simplified formatter configuration\n",
    "    simple_config = ExcelOutputConfig(\n",
    "        detail_level=OutputDetail.MINIMAL,  # Use minimal detail level\n",
    "        keywords_format=BaseColumnFormat(\n",
    "            column_name=\"keywords\",\n",
    "            format_template=\"{keyword}\",  # Just show keyword without score/domain\n",
    "            included_fields=[\"keyword\"],\n",
    "            max_items=5  # Limit number of keywords\n",
    "        ),\n",
    "        themes_format=BaseColumnFormat(\n",
    "            column_name=\"themes\",\n",
    "            format_template=\"{name}\",  # Just show theme name\n",
    "            included_fields=[\"name\"],\n",
    "            max_items=3  # Limit number of themes\n",
    "        ),\n",
    "        categories_format=BaseColumnFormat(\n",
    "            column_name=\"categories\",\n",
    "            format_template=\"{name}\",  # Just show category name\n",
    "            included_fields=[\"name\"],\n",
    "            max_items=2  # Limit number of categories\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Create formatter with simplified config\n",
    "    simple_formatter = ExcelFormatter(\n",
    "        file_utils=file_utils,\n",
    "        config=simple_config\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Load and process input file\n",
    "        input_df = file_utils.load_single_file(\n",
    "            file_path=file_name,\n",
    "            input_type=\"raw\"\n",
    "        )\n",
    "\n",
    "        if text_column not in input_df.columns:\n",
    "            print(f\"Error: Column '{text_column}' not found in input file\")\n",
    "            return\n",
    "\n",
    "        results = []\n",
    "        for idx, row in input_df.iterrows():\n",
    "            try:\n",
    "                language = row[language_column] if language_column in input_df.columns else \"en\"\n",
    "                analyzer.set_language(language)\n",
    "\n",
    "                result = await analyzer.analyze(\n",
    "                    row[text_column],\n",
    "                    analysis_types=[\"keywords\", \"themes\", \"categories\"],\n",
    "                    language=language\n",
    "                )\n",
    "\n",
    "                # Format with simplified formatter\n",
    "                formatted_result = {\n",
    "                    \"keywords\": simple_formatter.format_output(\n",
    "                        {\"keywords\": result.keywords},\n",
    "                        [\"keywords\"]\n",
    "                    ).get(\"keywords\", \"\"),\n",
    "                    \n",
    "                    \"themes\": simple_formatter.format_output(\n",
    "                        {\"themes\": result.themes},\n",
    "                        [\"themes\"]\n",
    "                    ).get(\"themes\", \"\"),\n",
    "                    \n",
    "                    \"categories\": simple_formatter.format_output(\n",
    "                        {\"categories\": result.categories},\n",
    "                        [\"categories\"]\n",
    "                    ).get(\"categories\", \"\"),\n",
    "                    \n",
    "                    \"language\": language\n",
    "                }\n",
    "                results.append(formatted_result)\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing row {idx + 1}: {e}\")\n",
    "                results.append({\n",
    "                    \"keywords\": f\"Error: {str(e)}\",\n",
    "                    \"themes\": f\"Error: {str(e)}\",\n",
    "                    \"categories\": f\"Error: {str(e)}\",\n",
    "                    \"language\": language\n",
    "                })\n",
    "\n",
    "        # Create and save results\n",
    "        results_df = input_df.copy()\n",
    "        for key in [\"keywords\", \"themes\", \"categories\"]:\n",
    "            results_df[key] = [r.get(key, \"\") for r in results]\n",
    "\n",
    "        # Configure pandas display options\n",
    "        pd.set_option('display.max_columns', None)\n",
    "        pd.set_option('display.max_colwidth', 30)  # Shorter column width\n",
    "        pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "        # Save results\n",
    "        saved_files, _ = file_utils.save_data_to_storage(\n",
    "            data={\"analysis_results\": results_df},\n",
    "            output_type=\"processed\",\n",
    "            file_name=output_filename,\n",
    "            output_filetype=OutputFileType.XLSX,\n",
    "            include_timestamp=True\n",
    "        )\n",
    "\n",
    "        print(\"\\nProcessing complete!\")\n",
    "        print(f\"Results saved to: {next(iter(saved_files.values()))}\")\n",
    "        print(\"\\nFirst few rows of results:\")\n",
    "        print(results_df.head())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Excel file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def demonstrate_custom_format():\n",
    "    \"\"\"Demonstrate custom formatting options with language support.\"\"\"\n",
    "    # Create custom config with language-aware templates\n",
    "    custom_config = ExcelOutputConfig(\n",
    "        detail_level=OutputDetail.SUMMARY,\n",
    "        keywords_format=BaseColumnFormat(\n",
    "            column_name=\"key_terms\",\n",
    "            format_template=\"{keywords} ({domain}) [{language}]\",\n",
    "            included_fields=[\"keyword\", \"domain\", \"language\"],\n",
    "            confidence_threshold=0.5,\n",
    "            max_items=3\n",
    "        ),\n",
    "        themes_format=BaseColumnFormat(\n",
    "            column_name=\"main_themes\",\n",
    "            format_template=\"{name} ({confidence}) [{language}]\",\n",
    "            included_fields=[\"name\", \"confidence\", \"language\"],\n",
    "            confidence_threshold=0.6,\n",
    "            max_items=2\n",
    "        )\n",
    "    )\n",
    "\n",
    "    custom_formatter = ExcelFormatter(\n",
    "        file_utils=file_utils,\n",
    "        config=custom_config\n",
    "    )\n",
    "\n",
    "    # Test both languages\n",
    "    for lang, texts in test_texts.items():\n",
    "        print(f\"\\nAnalyzing {lang.upper()} text with custom format:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Set appropriate language processor\n",
    "        analyzer.language_processor = (\n",
    "            fi_processor if lang == \"fi\" else en_processor\n",
    "        )\n",
    "\n",
    "        # Analyze each type of text\n",
    "        for text_type, text in texts.items():\n",
    "            print(f\"\\nText type: {text_type}\")\n",
    "            \n",
    "            try:\n",
    "                result = await analyzer.analyze(\n",
    "                    text,\n",
    "                    analysis_types=[\"keywords\", \"themes\"]\n",
    "                )\n",
    "\n",
    "                custom_output = custom_formatter.format_output(\n",
    "                    results=result,\n",
    "                    analysis_types=[\"keywords\", \"themes\"]\n",
    "                )\n",
    "                \n",
    "                for analysis_type, output in custom_output.items():\n",
    "                    print(f\"\\n{analysis_type.title()}:\")\n",
    "                    print(output)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error analyzing {lang} {text_type} text: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_demos():\n",
    "    \"\"\"Run all demonstrations with proper error handling.\"\"\"\n",
    "    try:\n",
    "        # 1. English text analysis\n",
    "        print(\"=== English Text Analysis Demo ===\\n\")\n",
    "        await demonstrate_single_analysis(test_texts[\"en\"][\"mixed\"], language=\"en\")\n",
    "\n",
    "        # 2. Finnish text analysis\n",
    "        print(\"\\n=== Finnish Text Analysis Demo ===\\n\")\n",
    "        await demonstrate_single_analysis(test_texts[\"fi\"][\"technical\"], language=\"fi\")\n",
    "\n",
    "        # 3. Excel processing\n",
    "        print(\"\\n=== Excel Processing Demo ===\\n\")\n",
    "        await demonstrate_excel_processing()\n",
    "\n",
    "        # 4. Custom format demo\n",
    "        print(\"\\n=== Custom Format Demo ===\\n\")\n",
    "        await demonstrate_custom_format()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error running demonstrations: {e}\")\n",
    "        raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Run all demonstrations with proper error handling.\"\"\"\n",
    "# 1. English text analysis\n",
    "print(\"=== English Text Analysis Demo ===\\n\")\n",
    "await demonstrate_single_analysis(test_texts[\"en\"][\"mixed\"], language=\"en\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Finnish text analysis\n",
    "print(\"\\n=== Finnish Text Analysis Demo ===\\n\")\n",
    "await demonstrate_single_analysis(test_texts[\"fi\"][\"technical\"], language=\"fi\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Excel Processing Demo ===\n",
      "\n",
      "2024-12-12 14:03:25,702 - FileUtils.core.file_utils - INFO - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-12 14:03:25,709 - FileUtils.core.file_utils - INFO - FileUtils initialized with local storage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: FileUtils initialized with local storage\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-12 14:03:25,791 - FileUtils.core.file_utils - INFO - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-12 14:03:25,799 - FileUtils.core.file_utils - INFO - FileUtils initialized with local storage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: FileUtils initialized with local storage\n",
      "INFO: Loaded 747 stopwords from c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\configurations\\stop_words\\fi.txt\n",
      "DEBUG: Initialized with config: {'default_language': 'en', 'content_column': 'content', 'analysis': {'keywords': {'max_keywords': 5, 'min_keyword_length': 3, 'include_compounds': True}, 'themes': {'max_themes': 3, 'min_confidence': 0.5, 'include_hierarchy': True}, 'categories': {'max_categories': 3, 'min_confidence': 0.3, 'require_evidence': True}}, 'models': {'default_provider': 'openai', 'default_model': 'gpt-4o-mini', 'parameters': {'temperature': 0.0, 'max_tokens': 1000, 'top_p': 1.0, 'frequency_penalty': 0.0, 'presence_penalty': 0.0}, 'providers': {'azure': {'api_version': '2024-02-15-preview', 'api_type': 'azure'}, 'openai': {'api_type': 'open_ai'}, 'anthropic': {'api_type': 'anthropic'}}}, 'features': {'use_caching': True, 'use_async': True, 'use_batching': True, 'enable_finnish_support': True}, 'language': 'fi'}\n",
      "DEBUG: Trying library paths: ['C:\\\\scripts\\\\Voikko\\\\libvoikko-1.dll', 'C:\\\\Program Files\\\\Voikko\\\\libvoikko-1.dll', 'C:\\\\Voikko\\\\libvoikko-1.dll']\n",
      "DEBUG: Trying dictionary paths: ['C:\\\\scripts\\\\Voikko', 'C:\\\\Program Files\\\\Voikko', 'C:\\\\Voikko']\n",
      "DEBUG: Added C:\\scripts\\Voikko to DLL search path\n",
      "INFO: Successfully initialized Voikko with path: C:\\scripts\\Voikko\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-12 14:03:25,872 - FileUtils.core.file_utils - INFO - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-12 14:03:25,885 - FileUtils.core.file_utils - INFO - FileUtils initialized with local storage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: FileUtils initialized with local storage\n",
      "INFO: Loaded 747 stopwords from c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\configurations\\stop_words\\fi.txt\n",
      "DEBUG: Initialized with config: {'min_word_length': 3, 'include_compounds': True}\n",
      "DEBUG: Trying library paths: ['C:\\\\scripts\\\\Voikko\\\\libvoikko-1.dll', 'C:\\\\Program Files\\\\Voikko\\\\libvoikko-1.dll', 'C:\\\\Voikko\\\\libvoikko-1.dll']\n",
      "DEBUG: Trying dictionary paths: ['C:\\\\scripts\\\\Voikko', 'C:\\\\Program Files\\\\Voikko', 'C:\\\\Voikko']\n",
      "DEBUG: Added C:\\scripts\\Voikko to DLL search path\n",
      "INFO: Successfully initialized Voikko with path: C:\\scripts\\Voikko\n",
      "INFO: Language switched to fi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-12 14:03:25,993 - FileUtils.core.file_utils - INFO - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-12 14:03:25,999 - FileUtils.core.file_utils - INFO - FileUtils initialized with local storage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: FileUtils initialized with local storage\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-12 14:03:26,097 - FileUtils.core.file_utils - INFO - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-12 14:03:26,103 - FileUtils.core.file_utils - INFO - FileUtils initialized with local storage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: FileUtils initialized with local storage\n",
      "INFO: Loaded 747 stopwords from c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\configurations\\stop_words\\fi.txt\n",
      "DEBUG: Initialized with config: {'default_language': 'en', 'content_column': 'content', 'analysis': {'keywords': {'max_keywords': 5, 'min_keyword_length': 3, 'include_compounds': True}, 'themes': {'max_themes': 3, 'min_confidence': 0.5, 'include_hierarchy': True}, 'categories': {'max_categories': 3, 'min_confidence': 0.3, 'require_evidence': True}}, 'models': {'default_provider': 'openai', 'default_model': 'gpt-4o-mini', 'parameters': {'temperature': 0.0, 'max_tokens': 1000, 'top_p': 1.0, 'frequency_penalty': 0.0, 'presence_penalty': 0.0}, 'providers': {'azure': {'api_version': '2024-02-15-preview', 'api_type': 'azure'}, 'openai': {'api_type': 'open_ai'}, 'anthropic': {'api_type': 'anthropic'}}}, 'features': {'use_caching': True, 'use_async': True, 'use_batching': True, 'enable_finnish_support': True}, 'language': 'fi'}\n",
      "DEBUG: Trying library paths: ['C:\\\\scripts\\\\Voikko\\\\libvoikko-1.dll', 'C:\\\\Program Files\\\\Voikko\\\\libvoikko-1.dll', 'C:\\\\Voikko\\\\libvoikko-1.dll']\n",
      "DEBUG: Trying dictionary paths: ['C:\\\\scripts\\\\Voikko', 'C:\\\\Program Files\\\\Voikko', 'C:\\\\Voikko']\n",
      "DEBUG: Added C:\\scripts\\Voikko to DLL search path\n",
      "INFO: Successfully initialized Voikko with path: C:\\scripts\\Voikko\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-12 14:03:26,215 - FileUtils.core.file_utils - INFO - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-12 14:03:26,227 - FileUtils.core.file_utils - INFO - FileUtils initialized with local storage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: FileUtils initialized with local storage\n",
      "INFO: Loaded 747 stopwords from c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\configurations\\stop_words\\fi.txt\n",
      "DEBUG: Initialized with config: {'min_word_length': 3, 'include_compounds': True}\n",
      "DEBUG: Trying library paths: ['C:\\\\scripts\\\\Voikko\\\\libvoikko-1.dll', 'C:\\\\Program Files\\\\Voikko\\\\libvoikko-1.dll', 'C:\\\\Voikko\\\\libvoikko-1.dll']\n",
      "DEBUG: Trying dictionary paths: ['C:\\\\scripts\\\\Voikko', 'C:\\\\Program Files\\\\Voikko', 'C:\\\\Voikko']\n",
      "DEBUG: Added C:\\scripts\\Voikko to DLL search path\n",
      "INFO: Successfully initialized Voikko with path: C:\\scripts\\Voikko\n",
      "INFO: Language switched to fi\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Raw LLM response: {'themes': [{'name': 'Koneoppimismallit', 'description': 'Koneoppimismallit ovat algoritmeja, joita koulutetaan suurilla datajoukoilla kaavojen tunnistamiseksi ja ennustamiseksi.', 'confidence': 0.95, 'keywords': ['koneoppimismalli', 'data', 'kaavat'], 'domain': 'technical', 'parent_theme': None}, {'name': 'Neuroverkon arkkitehtuuri', 'description': 'Neuroverkon arkkitehtuuri koostuu useista kerroksista, jotka mahdollistavat piirteiden erottamisen ja syvällisen oppimisen.', 'confidence': 0.9, 'keywords': ['neuroverkko', 'arkkitehtuuri', 'kerrokset'], 'domain': 'technical', 'parent_theme': 'Koneoppimismallit'}], 'evidence': {'Koneoppimismallit': [{'text': 'Koneoppimismalleja koulutetaan suurilla datajoukolla tunnistamaan kaavoja.', 'relevance': 0.9, 'keywords': ['koneoppimismalli', 'data', 'kaavat']}], 'Neuroverkon arkkitehtuuri': [{'text': 'Neuroverkon arkkitehtuuri sisältää useita kerroksia piirteiden erottamiseen.', 'relevance': 0.9, 'keywords': ['neuroverkko', 'arkkitehtuuri', 'kerrokset']}]}, 'relationships': {'Koneoppimismallit': ['Neuroverkon arkkitehtuuri'], 'Neuroverkon arkkitehtuuri': []}}\n",
      "\n",
      "Processed LLM response: {'themes': [{'name': 'Koneoppimismallit', 'description': 'Koneoppimismallit ovat algoritmeja, joita koulutetaan suurilla datajoukoilla kaavojen tunnistamiseksi ja ennustamiseksi.', 'confidence': 0.95, 'keywords': ['koneoppimismalli', 'data', 'kaavat'], 'domain': 'technical', 'parent_theme': None}, {'name': 'Neuroverkon arkkitehtuuri', 'description': 'Neuroverkon arkkitehtuuri koostuu useista kerroksista, jotka mahdollistavat piirteiden erottamisen ja syvällisen oppimisen.', 'confidence': 0.9, 'keywords': ['neuroverkko', 'arkkitehtuuri', 'kerrokset'], 'domain': 'technical', 'parent_theme': 'Koneoppimismallit'}], 'evidence': {'Koneoppimismallit': [{'text': 'Koneoppimismalleja koulutetaan suurilla datajoukolla tunnistamaan kaavoja.', 'relevance': 0.9, 'keywords': ['koneoppimismalli', 'data', 'kaavat']}], 'Neuroverkon arkkitehtuuri': [{'text': 'Neuroverkon arkkitehtuuri sisältää useita kerroksia piirteiden erottamiseen.', 'relevance': 0.9, 'keywords': ['neuroverkko', 'arkkitehtuuri', 'kerrokset']}]}, 'relationships': {'Koneoppimismallit': ['Neuroverkon arkkitehtuuri'], 'Neuroverkon arkkitehtuuri': []}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-12 14:03:36,893 - FileUtils.core.file_utils - INFO - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-12 14:03:36,901 - FileUtils.core.file_utils - INFO - FileUtils initialized with local storage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: FileUtils initialized with local storage\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-12 14:03:37,005 - FileUtils.core.file_utils - INFO - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-12 14:03:37,013 - FileUtils.core.file_utils - INFO - FileUtils initialized with local storage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: FileUtils initialized with local storage\n",
      "INFO: Loaded 747 stopwords from c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\configurations\\stop_words\\fi.txt\n",
      "DEBUG: Initialized with config: {'default_language': 'en', 'content_column': 'content', 'analysis': {'keywords': {'max_keywords': 5, 'min_keyword_length': 3, 'include_compounds': True}, 'themes': {'max_themes': 3, 'min_confidence': 0.5, 'include_hierarchy': True}, 'categories': {'max_categories': 3, 'min_confidence': 0.3, 'require_evidence': True}}, 'models': {'default_provider': 'openai', 'default_model': 'gpt-4o-mini', 'parameters': {'temperature': 0.0, 'max_tokens': 1000, 'top_p': 1.0, 'frequency_penalty': 0.0, 'presence_penalty': 0.0}, 'providers': {'azure': {'api_version': '2024-02-15-preview', 'api_type': 'azure'}, 'openai': {'api_type': 'open_ai'}, 'anthropic': {'api_type': 'anthropic'}}}, 'features': {'use_caching': True, 'use_async': True, 'use_batching': True, 'enable_finnish_support': True}, 'language': 'fi'}\n",
      "DEBUG: Trying library paths: ['C:\\\\scripts\\\\Voikko\\\\libvoikko-1.dll', 'C:\\\\Program Files\\\\Voikko\\\\libvoikko-1.dll', 'C:\\\\Voikko\\\\libvoikko-1.dll']\n",
      "DEBUG: Trying dictionary paths: ['C:\\\\scripts\\\\Voikko', 'C:\\\\Program Files\\\\Voikko', 'C:\\\\Voikko']\n",
      "DEBUG: Added C:\\scripts\\Voikko to DLL search path\n",
      "INFO: Successfully initialized Voikko with path: C:\\scripts\\Voikko\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-12 14:03:37,121 - FileUtils.core.file_utils - INFO - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-12 14:03:37,130 - FileUtils.core.file_utils - INFO - FileUtils initialized with local storage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: FileUtils initialized with local storage\n",
      "INFO: Loaded 747 stopwords from c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\configurations\\stop_words\\fi.txt\n",
      "DEBUG: Initialized with config: {'min_word_length': 3, 'include_compounds': True}\n",
      "DEBUG: Trying library paths: ['C:\\\\scripts\\\\Voikko\\\\libvoikko-1.dll', 'C:\\\\Program Files\\\\Voikko\\\\libvoikko-1.dll', 'C:\\\\Voikko\\\\libvoikko-1.dll']\n",
      "DEBUG: Trying dictionary paths: ['C:\\\\scripts\\\\Voikko', 'C:\\\\Program Files\\\\Voikko', 'C:\\\\Voikko']\n",
      "DEBUG: Added C:\\scripts\\Voikko to DLL search path\n",
      "INFO: Successfully initialized Voikko with path: C:\\scripts\\Voikko\n",
      "INFO: Language switched to fi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-12 14:03:37,213 - FileUtils.core.file_utils - INFO - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-12 14:03:37,219 - FileUtils.core.file_utils - INFO - FileUtils initialized with local storage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: FileUtils initialized with local storage\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-12 14:03:37,300 - FileUtils.core.file_utils - INFO - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-12 14:03:37,307 - FileUtils.core.file_utils - INFO - FileUtils initialized with local storage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: FileUtils initialized with local storage\n",
      "INFO: Loaded 747 stopwords from c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\configurations\\stop_words\\fi.txt\n",
      "DEBUG: Initialized with config: {'default_language': 'en', 'content_column': 'content', 'analysis': {'keywords': {'max_keywords': 5, 'min_keyword_length': 3, 'include_compounds': True}, 'themes': {'max_themes': 3, 'min_confidence': 0.5, 'include_hierarchy': True}, 'categories': {'max_categories': 3, 'min_confidence': 0.3, 'require_evidence': True}}, 'models': {'default_provider': 'openai', 'default_model': 'gpt-4o-mini', 'parameters': {'temperature': 0.0, 'max_tokens': 1000, 'top_p': 1.0, 'frequency_penalty': 0.0, 'presence_penalty': 0.0}, 'providers': {'azure': {'api_version': '2024-02-15-preview', 'api_type': 'azure'}, 'openai': {'api_type': 'open_ai'}, 'anthropic': {'api_type': 'anthropic'}}}, 'features': {'use_caching': True, 'use_async': True, 'use_batching': True, 'enable_finnish_support': True}, 'language': 'fi'}\n",
      "DEBUG: Trying library paths: ['C:\\\\scripts\\\\Voikko\\\\libvoikko-1.dll', 'C:\\\\Program Files\\\\Voikko\\\\libvoikko-1.dll', 'C:\\\\Voikko\\\\libvoikko-1.dll']\n",
      "DEBUG: Trying dictionary paths: ['C:\\\\scripts\\\\Voikko', 'C:\\\\Program Files\\\\Voikko', 'C:\\\\Voikko']\n",
      "DEBUG: Added C:\\scripts\\Voikko to DLL search path\n",
      "INFO: Successfully initialized Voikko with path: C:\\scripts\\Voikko\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-12 14:03:37,362 - FileUtils.core.file_utils - INFO - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-12 14:03:37,369 - FileUtils.core.file_utils - INFO - FileUtils initialized with local storage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: FileUtils initialized with local storage\n",
      "INFO: Loaded 747 stopwords from c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\configurations\\stop_words\\fi.txt\n",
      "DEBUG: Initialized with config: {'min_word_length': 3, 'include_compounds': True}\n",
      "DEBUG: Trying library paths: ['C:\\\\scripts\\\\Voikko\\\\libvoikko-1.dll', 'C:\\\\Program Files\\\\Voikko\\\\libvoikko-1.dll', 'C:\\\\Voikko\\\\libvoikko-1.dll']\n",
      "DEBUG: Trying dictionary paths: ['C:\\\\scripts\\\\Voikko', 'C:\\\\Program Files\\\\Voikko', 'C:\\\\Voikko']\n",
      "DEBUG: Added C:\\scripts\\Voikko to DLL search path\n",
      "INFO: Successfully initialized Voikko with path: C:\\scripts\\Voikko\n",
      "INFO: Language switched to fi\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Raw LLM response: {'themes': [{'name': 'Pilvipalvelut', 'description': 'Pilvipalvelut tarjoavat skaalautuvan infrastruktuurin, joka mahdollistaa joustavan ja tehokkaan resurssien käytön.', 'confidence': 0.95, 'keywords': ['pilvipalvelu', 'skaalautuva', 'infrastruktuuri'], 'domain': 'technical/business'}, {'name': 'Mikropalveluarkkitehtuuri', 'description': 'Mikropalveluarkkitehtuuri mahdollistaa modulaarisen järjestelmäsuunnittelun, mikä parantaa kehityksen joustavuutta ja ylläpidettävyyttä.', 'confidence': 0.9, 'keywords': ['mikropalveluarkkitehtuuri', 'modulaarinen'], 'domain': 'technical/business'}], 'evidence': {'Pilvipalvelut': [{'text': 'Pilvipalvelut tarjoavat skaalautuvan infrastruktuurin käyttöönottoon.', 'relevance': 0.9, 'keywords': ['pilvipalvelu', 'skaalautuva', 'infrastruktuuri']}], 'Mikropalveluarkkitehtuuri': [{'text': 'Mikropalveluarkkitehtuuri mahdollistaa modulaarisen järjestelmäsuunnittelun.', 'relevance': 0.9, 'keywords': ['mikropalveluarkkitehtuuri', 'modulaarinen']}]}, 'relationships': {'Pilvipalvelut': ['Mikropalveluarkkitehtuuri'], 'Mikropalveluarkkitehtuuri': []}}\n",
      "\n",
      "Processed LLM response: {'themes': [{'name': 'Pilvipalvelut', 'description': 'Pilvipalvelut tarjoavat skaalautuvan infrastruktuurin, joka mahdollistaa joustavan ja tehokkaan resurssien käytön.', 'confidence': 0.95, 'keywords': ['pilvipalvelu', 'skaalautuva', 'infrastruktuuri'], 'domain': 'technical/business'}, {'name': 'Mikropalveluarkkitehtuuri', 'description': 'Mikropalveluarkkitehtuuri mahdollistaa modulaarisen järjestelmäsuunnittelun, mikä parantaa kehityksen joustavuutta ja ylläpidettävyyttä.', 'confidence': 0.9, 'keywords': ['mikropalveluarkkitehtuuri', 'modulaarinen'], 'domain': 'technical/business'}], 'evidence': {'Pilvipalvelut': [{'text': 'Pilvipalvelut tarjoavat skaalautuvan infrastruktuurin käyttöönottoon.', 'relevance': 0.9, 'keywords': ['pilvipalvelu', 'skaalautuva', 'infrastruktuuri']}], 'Mikropalveluarkkitehtuuri': [{'text': 'Mikropalveluarkkitehtuuri mahdollistaa modulaarisen järjestelmäsuunnittelun.', 'relevance': 0.9, 'keywords': ['mikropalveluarkkitehtuuri', 'modulaarinen']}]}, 'relationships': {'Pilvipalvelut': ['Mikropalveluarkkitehtuuri'], 'Mikropalveluarkkitehtuuri': []}}\n",
      "2024-12-12 14:03:42,216 - FileUtils.core.file_utils - INFO - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-12 14:03:42,222 - FileUtils.core.file_utils - INFO - FileUtils initialized with local storage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: FileUtils initialized with local storage\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-12 14:03:42,305 - FileUtils.core.file_utils - INFO - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-12 14:03:42,311 - FileUtils.core.file_utils - INFO - FileUtils initialized with local storage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: FileUtils initialized with local storage\n",
      "INFO: Loaded 747 stopwords from c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\configurations\\stop_words\\fi.txt\n",
      "DEBUG: Initialized with config: {'default_language': 'en', 'content_column': 'content', 'analysis': {'keywords': {'max_keywords': 5, 'min_keyword_length': 3, 'include_compounds': True}, 'themes': {'max_themes': 3, 'min_confidence': 0.5, 'include_hierarchy': True}, 'categories': {'max_categories': 3, 'min_confidence': 0.3, 'require_evidence': True}}, 'models': {'default_provider': 'openai', 'default_model': 'gpt-4o-mini', 'parameters': {'temperature': 0.0, 'max_tokens': 1000, 'top_p': 1.0, 'frequency_penalty': 0.0, 'presence_penalty': 0.0}, 'providers': {'azure': {'api_version': '2024-02-15-preview', 'api_type': 'azure'}, 'openai': {'api_type': 'open_ai'}, 'anthropic': {'api_type': 'anthropic'}}}, 'features': {'use_caching': True, 'use_async': True, 'use_batching': True, 'enable_finnish_support': True}, 'language': 'fi'}\n",
      "DEBUG: Trying library paths: ['C:\\\\scripts\\\\Voikko\\\\libvoikko-1.dll', 'C:\\\\Program Files\\\\Voikko\\\\libvoikko-1.dll', 'C:\\\\Voikko\\\\libvoikko-1.dll']\n",
      "DEBUG: Trying dictionary paths: ['C:\\\\scripts\\\\Voikko', 'C:\\\\Program Files\\\\Voikko', 'C:\\\\Voikko']\n",
      "DEBUG: Added C:\\scripts\\Voikko to DLL search path\n",
      "INFO: Successfully initialized Voikko with path: C:\\scripts\\Voikko\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-12 14:03:42,400 - FileUtils.core.file_utils - INFO - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-12 14:03:42,408 - FileUtils.core.file_utils - INFO - FileUtils initialized with local storage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: FileUtils initialized with local storage\n",
      "INFO: Loaded 747 stopwords from c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\configurations\\stop_words\\fi.txt\n",
      "DEBUG: Initialized with config: {'min_word_length': 3, 'include_compounds': True}\n",
      "DEBUG: Trying library paths: ['C:\\\\scripts\\\\Voikko\\\\libvoikko-1.dll', 'C:\\\\Program Files\\\\Voikko\\\\libvoikko-1.dll', 'C:\\\\Voikko\\\\libvoikko-1.dll']\n",
      "DEBUG: Trying dictionary paths: ['C:\\\\scripts\\\\Voikko', 'C:\\\\Program Files\\\\Voikko', 'C:\\\\Voikko']\n",
      "DEBUG: Added C:\\scripts\\Voikko to DLL search path\n",
      "INFO: Successfully initialized Voikko with path: C:\\scripts\\Voikko\n",
      "INFO: Language switched to fi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-12 14:03:42,519 - FileUtils.core.file_utils - INFO - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-12 14:03:42,527 - FileUtils.core.file_utils - INFO - FileUtils initialized with local storage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: FileUtils initialized with local storage\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-12 14:03:42,609 - FileUtils.core.file_utils - INFO - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-12 14:03:42,615 - FileUtils.core.file_utils - INFO - FileUtils initialized with local storage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: FileUtils initialized with local storage\n",
      "INFO: Loaded 747 stopwords from c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\configurations\\stop_words\\fi.txt\n",
      "DEBUG: Initialized with config: {'default_language': 'en', 'content_column': 'content', 'analysis': {'keywords': {'max_keywords': 5, 'min_keyword_length': 3, 'include_compounds': True}, 'themes': {'max_themes': 3, 'min_confidence': 0.5, 'include_hierarchy': True}, 'categories': {'max_categories': 3, 'min_confidence': 0.3, 'require_evidence': True}}, 'models': {'default_provider': 'openai', 'default_model': 'gpt-4o-mini', 'parameters': {'temperature': 0.0, 'max_tokens': 1000, 'top_p': 1.0, 'frequency_penalty': 0.0, 'presence_penalty': 0.0}, 'providers': {'azure': {'api_version': '2024-02-15-preview', 'api_type': 'azure'}, 'openai': {'api_type': 'open_ai'}, 'anthropic': {'api_type': 'anthropic'}}}, 'features': {'use_caching': True, 'use_async': True, 'use_batching': True, 'enable_finnish_support': True}, 'language': 'fi'}\n",
      "DEBUG: Trying library paths: ['C:\\\\scripts\\\\Voikko\\\\libvoikko-1.dll', 'C:\\\\Program Files\\\\Voikko\\\\libvoikko-1.dll', 'C:\\\\Voikko\\\\libvoikko-1.dll']\n",
      "DEBUG: Trying dictionary paths: ['C:\\\\scripts\\\\Voikko', 'C:\\\\Program Files\\\\Voikko', 'C:\\\\Voikko']\n",
      "DEBUG: Added C:\\scripts\\Voikko to DLL search path\n",
      "INFO: Successfully initialized Voikko with path: C:\\scripts\\Voikko\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-12 14:03:42,669 - FileUtils.core.file_utils - INFO - Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Project root: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-12 14:03:42,676 - FileUtils.core.file_utils - INFO - FileUtils initialized with local storage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: FileUtils initialized with local storage\n",
      "INFO: Loaded 747 stopwords from c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\configurations\\stop_words\\fi.txt\n",
      "DEBUG: Initialized with config: {'min_word_length': 3, 'include_compounds': True}\n",
      "DEBUG: Trying library paths: ['C:\\\\scripts\\\\Voikko\\\\libvoikko-1.dll', 'C:\\\\Program Files\\\\Voikko\\\\libvoikko-1.dll', 'C:\\\\Voikko\\\\libvoikko-1.dll']\n",
      "DEBUG: Trying dictionary paths: ['C:\\\\scripts\\\\Voikko', 'C:\\\\Program Files\\\\Voikko', 'C:\\\\Voikko']\n",
      "DEBUG: Added C:\\scripts\\Voikko to DLL search path\n",
      "INFO: Successfully initialized Voikko with path: C:\\scripts\\Voikko\n",
      "INFO: Language switched to fi\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# 3. Excel processing\n",
    "print(\"\\n=== Excel Processing Demo ===\\n\")\n",
    "await demonstrate_excel_processing()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Custom format demo\n",
    "print(\"\\n=== Custom Format Demo ===\\n\")\n",
    "await demonstrate_custom_format()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Batch file analysis\n",
    "# print(\"\\n=== Batch File Analysis Demo ===\\n\")\n",
    "# await demonstrate_batch_file_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. Single Text Analysis with Different Detail Levels\n",
    "# async def demonstrate_single_analysis(text: str):\n",
    "#     \"\"\"Demonstrate analysis of single text with different detail levels.\"\"\"\n",
    "#     print(\"Analyzing text:\")\n",
    "#     print(\"-\" * 50)\n",
    "#     print(text)\n",
    "#     print(\"\\n\")\n",
    "\n",
    "#     # Perform analysis\n",
    "#     result = await analyzer.analyze(\n",
    "#         text,\n",
    "#         analysis_types=[\"keywords\", \"themes\", \"categories\"]\n",
    "#     )\n",
    "\n",
    "#     # Show summary format\n",
    "#     print(\"Summary Output:\")\n",
    "#     print(\"-\" * 50)\n",
    "#     summary = summary_formatter.format_output(\n",
    "#         results=result,\n",
    "#         analysis_types=[\"keywords\", \"themes\", \"categories\"]\n",
    "#     )\n",
    "#     for analysis_type, output in summary.items():\n",
    "#         print(f\"\\n{analysis_type.title()}:\")\n",
    "#         print(output)\n",
    "\n",
    "#     # Show detailed format\n",
    "#     print(\"\\nDetailed Output:\")\n",
    "#     print(\"-\" * 50)\n",
    "#     detailed = detailed_formatter.format_detailed_output(\n",
    "#         results=result,\n",
    "#         analysis_types=[\"keywords\", \"themes\", \"categories\"]\n",
    "#     )\n",
    "#     for analysis_type, output in detailed.items():\n",
    "#         print(f\"\\n{analysis_type.title()}:\")\n",
    "#         print(\"Summary:\", output[\"summary\"])\n",
    "#         print(\"Details:\", output[\"details\"])\n",
    "#         print(\"Metadata:\", output[\"metadata\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For notebook execution\n",
    "# if __name__ == \"__main__\":\n",
    "    # await run_demos()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semantic-analyzer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
