{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = str(Path().resolve().parent)\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "    print(f\"Added {project_root} to Python path\")\n",
    "\n",
    "# Core components\n",
    "from src.semantic_analyzer.analyzer import SemanticAnalyzer\n",
    "from src.utils.FileUtils.file_utils import FileUtils\n",
    "from src.analyzers.keyword_analyzer import KeywordAnalyzer\n",
    "from src.analyzers.theme_analyzer import ThemeAnalyzer\n",
    "from src.core.language_processing import create_text_processor\n",
    "from src.loaders.parameter_adapter import ParameterAdapter\n",
    "\n",
    "# Initialize FileUtils and set up logging\n",
    "file_utils = FileUtils()\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseTester:\n",
    "    \"\"\"Base class for analysis testing.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        from src.core.llm.factory import create_llm\n",
    "        self.file_utils = FileUtils()\n",
    "        self.llm = create_llm()  # Create LLM instance\n",
    "        self.test_texts = self._load_test_texts()\n",
    "        \n",
    "    def _load_test_texts(self) -> Dict[str, str]:\n",
    "        \"\"\"Load test texts from files.\"\"\"\n",
    "        try:\n",
    "            # Try to load from existing files\n",
    "            texts = {}\n",
    "            for lang in [\"en\", \"fi\"]:\n",
    "                df = self.file_utils.load_single_file(\n",
    "                    f\"test_content_{lang}.xlsx\",\n",
    "                    input_type=\"raw\"\n",
    "                )\n",
    "                if df is not None:\n",
    "                    for _, row in df.iterrows():\n",
    "                        key = f\"{lang}_{row['type']}\"\n",
    "                        texts[key] = row['content']\n",
    "            return texts\n",
    "                        \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not load test texts: {e}. Using defaults.\")\n",
    "            return self._create_default_texts()\n",
    "    \n",
    "    def _create_default_texts(self) -> Dict[str, str]:\n",
    "        \"\"\"Create default test texts.\"\"\"\n",
    "        return {\n",
    "            \"en_technical\": \"\"\"\n",
    "                Machine learning models are trained using large datasets.\n",
    "                Neural networks extract features through multiple layers.\n",
    "                Data preprocessing improves model performance.\n",
    "            \"\"\",\n",
    "            \"en_business\": \"\"\"\n",
    "                Q3 financial results show 15% revenue growth.\n",
    "                Customer acquisition costs decreased while retention improved.\n",
    "                Market expansion strategy targets emerging sectors.\n",
    "            \"\"\",\n",
    "            \"fi_technical\": \"\"\"\n",
    "                Ohjelmistokehittäjä työskentelee asiakasprojektissa.\n",
    "                Tekninen toteutus vaatii erityistä huomiota.\n",
    "                Tietoturva on keskeinen osa kehitystä.\n",
    "            \"\"\"\n",
    "        }\n",
    "\n",
    "    def save_test_texts(self) -> None:\n",
    "        \"\"\"Save test texts using FileUtils.\"\"\"\n",
    "        df = pd.DataFrame([\n",
    "            {\n",
    "                \"language\": key.split(\"_\")[0],\n",
    "                \"type\": key.split(\"_\")[1],\n",
    "                \"content\": content.strip()\n",
    "            }\n",
    "            for key, content in self.test_texts.items()\n",
    "        ])\n",
    "        \n",
    "        self.file_utils.save_data_to_disk(\n",
    "            data={\"texts\": df},\n",
    "            output_type=\"raw\",\n",
    "            file_name=\"test_texts\",\n",
    "            output_filetype=\"xlsx\",\n",
    "            include_timestamp=False\n",
    "        )\n",
    "\n",
    "    async def analyze_text(self, text: str, language: str, analyzer: Any) -> Dict[str, Any]:\n",
    "        \"\"\"Base method for text analysis.\"\"\"\n",
    "        try:\n",
    "            return await analyzer.analyze(text)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Analysis error: {e}\")\n",
    "            return {\"error\": str(e)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeywordTester(BaseTester):\n",
    "    \"\"\"Helper class for testing keyword analysis.\"\"\"\n",
    "    \n",
    "    async def test_statistical_analysis(self, text: str, language: str = None) -> Dict[str, Any]:\n",
    "        \"\"\"Test statistical keyword extraction.\"\"\"\n",
    "        if language is None:\n",
    "            from langdetect import detect\n",
    "            try:\n",
    "                language = detect(text)\n",
    "            except:\n",
    "                language = \"en\"\n",
    "        \n",
    "        # Create processor and analyzer\n",
    "        processor = create_text_processor(language=language)\n",
    "        analyzer = KeywordAnalyzer(\n",
    "            llm=self.llm,  # Pass LLM instance\n",
    "            config={\"weights\": {\"statistical\": 1.0, \"llm\": 0.0}},  # Statistical only\n",
    "            language_processor=processor\n",
    "        )\n",
    "        \n",
    "        return await self.analyze_text(text, language, analyzer)\n",
    "\n",
    "    async def test_llm_analysis(self, text: str, language: str = None) -> Dict[str, Any]:\n",
    "        \"\"\"Test LLM-based keyword extraction.\"\"\"\n",
    "        if language is None:\n",
    "            from langdetect import detect\n",
    "            try:\n",
    "                language = detect(text)\n",
    "            except:\n",
    "                language = \"en\"\n",
    "        \n",
    "        analyzer = KeywordAnalyzer(\n",
    "            llm=self.llm,  # Pass LLM instance\n",
    "            config={\"weights\": {\"statistical\": 0.0, \"llm\": 1.0}},  # LLM only\n",
    "            language_processor=create_text_processor(language=language)\n",
    "        )\n",
    "        \n",
    "        return await self.analyze_text(text, language, analyzer)\n",
    "\n",
    "    async def test_combined_analysis(self, text: str, language: str = None) -> Dict[str, Any]:\n",
    "        \"\"\"Test combined statistical and LLM analysis.\"\"\"\n",
    "        if language is None:\n",
    "            from langdetect import detect\n",
    "            try:\n",
    "                language = detect(text)\n",
    "            except:\n",
    "                language = \"en\"\n",
    "        \n",
    "        analyzer = KeywordAnalyzer(\n",
    "            llm=self.llm,  # Pass LLM instance\n",
    "            config={\n",
    "                \"weights\": {\"statistical\": 0.4, \"llm\": 0.6},\n",
    "                \"max_keywords\": 8,\n",
    "                \"min_confidence\": 0.3\n",
    "            },\n",
    "            language_processor=create_text_processor(language=language)\n",
    "        )\n",
    "        \n",
    "        return await self.analyze_text(text, language, analyzer)\n",
    "\n",
    "    def display_keyword_results(self, results: Dict[str, Any]) -> None:\n",
    "        \"\"\"Display keyword analysis results.\"\"\"\n",
    "        print(\"\\nKeyword Analysis Results:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        if \"error\" in results:\n",
    "            print(f\"Error: {results['error']}\")\n",
    "            return\n",
    "            \n",
    "        if \"keywords\" in results:\n",
    "            print(\"\\nKeywords:\", results[\"keywords\"])\n",
    "            \n",
    "        if \"domain_keywords\" in results:\n",
    "            print(\"\\nDomain Keywords:\")\n",
    "            for domain, keywords in results[\"domain_keywords\"].items():\n",
    "                print(f\"{domain}: {keywords}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThemeTester(BaseTester):\n",
    "    \"\"\"Helper class for testing theme analysis.\"\"\"\n",
    "    \n",
    "    async def test_theme_analysis(self, text: str, language: str = None) -> Dict[str, Any]:\n",
    "        \"\"\"Test theme analysis on text.\"\"\"\n",
    "        if language is None:\n",
    "            from langdetect import detect\n",
    "            try:\n",
    "                language = detect(text)\n",
    "            except:\n",
    "                language = \"en\"\n",
    "        \n",
    "        analyzer = ThemeAnalyzer(\n",
    "            llm=self.llm,\n",
    "            config={\n",
    "                \"max_themes\": 3,\n",
    "                \"min_confidence\": 0.3,\n",
    "                \"focus_areas\": \"business,technical\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return await self.analyze_text(text, language, analyzer)\n",
    "        \n",
    "    def display_theme_results(self, results: Any) -> None:\n",
    "        \"\"\"Display theme analysis results.\n",
    "        \n",
    "        Args:\n",
    "            results: Either a dict or ThemeOutput model\n",
    "        \"\"\"\n",
    "        print(\"\\nTheme Analysis Results:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Convert to dict if it's a pydantic model\n",
    "        if hasattr(results, \"model_dump\"):\n",
    "            results = results.model_dump()\n",
    "        elif hasattr(results, \"dict\"):\n",
    "            results = results.dict()\n",
    "            \n",
    "        # Handle error case\n",
    "        if isinstance(results, dict) and \"error\" in results:\n",
    "            print(f\"Error: {results['error']}\")\n",
    "            return\n",
    "\n",
    "        # Access theme data\n",
    "        themes_data = results.get(\"themes\", {})\n",
    "        if isinstance(themes_data, dict):\n",
    "            themes = themes_data.get(\"themes\", [])\n",
    "            descriptions = themes_data.get(\"theme_descriptions\", {})\n",
    "            confidence = themes_data.get(\"theme_confidence\", {})\n",
    "            keywords = themes_data.get(\"related_keywords\", {})\n",
    "        else:\n",
    "            themes = []\n",
    "            descriptions = {}\n",
    "            confidence = {}\n",
    "            keywords = {}\n",
    "            \n",
    "        # Display themes\n",
    "        if not themes:\n",
    "            print(\"No themes found.\")\n",
    "            return\n",
    "            \n",
    "        for theme in themes:\n",
    "            print(f\"\\nTheme: {theme}\")\n",
    "            print(f\"Description: {descriptions.get(theme, 'No description available')}\")\n",
    "            print(f\"Confidence: {confidence.get(theme, 0):.2f}\")\n",
    "            theme_keywords = keywords.get(theme, [])\n",
    "            if theme_keywords:\n",
    "                print(f\"Keywords: {', '.join(theme_keywords)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnalysisPipeline:\n",
    "    \"\"\"Complete analysis pipeline for testing multiple analyzers.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.file_utils = FileUtils()\n",
    "        self.keyword_tester = KeywordTester()\n",
    "        self.theme_tester = ThemeTester()\n",
    "        \n",
    "    async def analyze_text(self, text: str, language: str = None) -> Dict[str, Any]:\n",
    "        \"\"\"Run complete analysis pipeline on text.\"\"\"\n",
    "        if language is None:\n",
    "            from langdetect import detect\n",
    "            try:\n",
    "                language = detect(text)\n",
    "            except:\n",
    "                language = \"en\"\n",
    "        \n",
    "        # Run analyses\n",
    "        keyword_results = await self.keyword_tester.test_combined_analysis(\n",
    "            text, language=language\n",
    "        )\n",
    "        theme_results = await self.theme_tester.test_theme_analysis(\n",
    "            text, language=language\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"keywords\": keyword_results,\n",
    "            \"themes\": theme_results,\n",
    "            \"language\": language\n",
    "        }\n",
    "    \n",
    "    def display_results(self, results: Dict[str, Any]) -> None:\n",
    "        \"\"\"Display complete analysis results.\"\"\"\n",
    "        print(\"\\nComplete Analysis Results\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Display keyword results\n",
    "        print(\"\\nKeyword Analysis:\")\n",
    "        print(\"-\" * 20)\n",
    "        keyword_results = results.get(\"keywords\", {})\n",
    "        if hasattr(keyword_results, \"model_dump\"):\n",
    "            keyword_results = keyword_results.model_dump()\n",
    "        elif hasattr(keyword_results, \"dict\"):\n",
    "            keyword_results = keyword_results.dict()\n",
    "            \n",
    "        if isinstance(keyword_results, dict):\n",
    "            if \"keywords\" in keyword_results:\n",
    "                print(\"Keywords:\", keyword_results[\"keywords\"])\n",
    "                print(\"Domain Keywords:\", keyword_results.get(\"domain_keywords\", {}))\n",
    "            elif \"error\" in keyword_results:\n",
    "                print(f\"Error: {keyword_results['error']}\")\n",
    "        \n",
    "        # Display theme results\n",
    "        print(\"\\nTheme Analysis:\")\n",
    "        print(\"-\" * 20)\n",
    "        self.theme_tester.display_theme_results(results.get(\"themes\", {}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_analysis_examples():\n",
    "    \"\"\"Run example analyses on different content types.\"\"\"\n",
    "    \n",
    "    # Example texts\n",
    "    example_texts = {\n",
    "        \"Business Analysis\": \"\"\"\n",
    "            Q3 revenue increased by 15% with strong growth in enterprise sales.\n",
    "            Customer retention improved while acquisition costs decreased.\n",
    "            New market expansion initiatives are showing positive early results.\n",
    "        \"\"\",\n",
    "        \n",
    "        \"Technical Content\": \"\"\"\n",
    "            The application uses microservices architecture with containerized deployments.\n",
    "            Data processing pipeline incorporates machine learning models for prediction.\n",
    "            System monitoring ensures high availability and performance metrics.\n",
    "        \"\"\",\n",
    "        \n",
    "        \"Mixed Content\": \"\"\"\n",
    "            The IT department's cloud migration project reduced infrastructure costs by 25%.\n",
    "            DevOps implementation improved deployment frequency while maintaining quality.\n",
    "            Monthly recurring revenue from SaaS products grew steadily.\n",
    "        \"\"\"\n",
    "    }\n",
    "    \n",
    "    pipeline = AnalysisPipeline()\n",
    "    \n",
    "    for title, text in example_texts.items():\n",
    "        print(f\"\\nAnalyzing {title}\")\n",
    "        print(\"=\" * 50)\n",
    "        results = await pipeline.analyze_text(text)\n",
    "        pipeline.display_results(results)\n",
    "        print(\"\\n\" + \"-\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def debug_theme_analysis(text: str):\n",
    "    \"\"\"Debug theme analysis with detailed output.\"\"\"\n",
    "    print(\"\\nDebug Theme Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(\"\\nInput Text:\")\n",
    "    print(\"-\" * 20)\n",
    "    print(text.strip())\n",
    "    \n",
    "    # Configure logging\n",
    "    logger = logging.getLogger(\"src.analyzers.theme_analyzer\")\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    \n",
    "    # Add handler if not already present\n",
    "    if not logger.handlers:\n",
    "        handler = logging.StreamHandler()\n",
    "        formatter = logging.Formatter(\n",
    "            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        handler.setFormatter(formatter)\n",
    "        logger.addHandler(handler)\n",
    "    \n",
    "    print(\"\\nRunning Analysis...\")\n",
    "    print(\"-\" * 20)\n",
    "    \n",
    "    # Run analysis\n",
    "    theme_tester = ThemeTester()\n",
    "    results = await theme_tester.test_theme_analysis(text)\n",
    "    \n",
    "    # Display results\n",
    "    theme_tester.display_theme_results(results)\n",
    "    \n",
    "    if logger.isEnabledFor(logging.DEBUG):\n",
    "        print(\"\\nDebug Information:\")\n",
    "        print(\"-\" * 20)\n",
    "        if hasattr(results, \"model_dump\"):\n",
    "            print(json.dumps(results.model_dump(), indent=2))\n",
    "        else:\n",
    "            print(json.dumps(results, indent=2))\n",
    "    \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_environment() -> bool:\n",
    "    \"\"\"Verify notebook environment setup.\"\"\"\n",
    "    from dotenv import load_dotenv\n",
    "    \n",
    "    # Load environment variables\n",
    "    env_path = Path(project_root) / \".env\"\n",
    "    env_loaded = load_dotenv(env_path)\n",
    "    \n",
    "    # Required variables\n",
    "    required_env_vars = [\n",
    "        'OPENAI_API_KEY',\n",
    "        'ANTHROPIC_API_KEY',\n",
    "    ]\n",
    "    \n",
    "    # Basic checks\n",
    "    basic_checks = {\n",
    "        \"Project root in path\": project_root in sys.path,\n",
    "        \"Can import src\": \"src\" in sys.modules,\n",
    "        \"FileUtils initialized\": hasattr(file_utils, \"project_root\"),\n",
    "        \".env file loaded\": env_loaded,\n",
    "    }\n",
    "    \n",
    "    # Environment variable checks\n",
    "    env_var_checks = {\n",
    "        f\"{var} set\": os.getenv(var) is not None\n",
    "        for var in required_env_vars\n",
    "    }\n",
    "    \n",
    "    # Path checks\n",
    "    expected_paths = {\n",
    "        \"Raw data\": file_utils.get_data_path(\"raw\"),\n",
    "        \"Processed data\": file_utils.get_data_path(\"processed\"),\n",
    "        \"Configuration\": file_utils.get_data_path(\"configurations\"),\n",
    "        \"Main config.yaml\": Path(project_root) / \"config.yaml\"\n",
    "    }\n",
    "    \n",
    "    path_checks = {\n",
    "        f\"{name} exists\": path.exists()\n",
    "        for name, path in expected_paths.items()\n",
    "    }\n",
    "    \n",
    "    # Combine all checks\n",
    "    all_checks = {\n",
    "        **basic_checks,\n",
    "        **env_var_checks,\n",
    "        **path_checks\n",
    "    }\n",
    "    \n",
    "    print(\"Environment Check Results:\")\n",
    "    print(\"=\" * 50)\n",
    "    print()\n",
    "    \n",
    "    # Print Basic Setup section\n",
    "    print(\"Basic Setup:\")\n",
    "    print(\"-\" * 11)\n",
    "    for check, result in basic_checks.items():\n",
    "        status = \"✓\" if result else \"✗\"\n",
    "        print(f\"{status} {check}\")\n",
    "    \n",
    "    # Print Environment Variables section\n",
    "    print(\"\\nEnvironment Variables:\")\n",
    "    print(\"-\" * 21)\n",
    "    for check, result in env_var_checks.items():\n",
    "        status = \"✓\" if result else \"✗\"\n",
    "        print(f\"{status} {check}\")\n",
    "    \n",
    "    # Print Project Structure section\n",
    "    print(\"\\nProject Structure:\")\n",
    "    print(\"-\" * 17)\n",
    "    for check, result in path_checks.items():\n",
    "        status = \"✓\" if result else \"✗\"\n",
    "        print(f\"{status} {check}\")\n",
    "    \n",
    "    # Overall status\n",
    "    all_passed = all(all_checks.values())\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Environment Status:\", \"Ready ✓\" if all_passed else \"Setup needed ✗\")\n",
    "    \n",
    "    # Print setup instructions if needed\n",
    "    if not all_passed:\n",
    "        print(\"\\nSetup Instructions:\")\n",
    "        if not env_loaded:\n",
    "            print(\"- Create a .env file in the project root with required API keys\")\n",
    "        for var in required_env_vars:\n",
    "            if not os.getenv(var):\n",
    "                print(f\"- Add {var} to your .env file\")\n",
    "        for name, path in expected_paths.items():\n",
    "            if not path.exists():\n",
    "                print(f\"- Create {name} directory at {path}\")\n",
    "    \n",
    "    return all_passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Check Results:\n",
      "==================================================\n",
      "\n",
      "Basic Setup:\n",
      "-----------\n",
      "✓ Project root in path\n",
      "✓ Can import src\n",
      "✓ FileUtils initialized\n",
      "✓ .env file loaded\n",
      "\n",
      "Environment Variables:\n",
      "---------------------\n",
      "✓ OPENAI_API_KEY set\n",
      "✓ ANTHROPIC_API_KEY set\n",
      "\n",
      "Project Structure:\n",
      "-----------------\n",
      "✓ Raw data exists\n",
      "✓ Processed data exists\n",
      "✓ Configuration exists\n",
      "✓ Main config.yaml exists\n",
      "\n",
      "==================================================\n",
      "Environment Status: Ready ✓\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# First cell: Verify environment\n",
    "verify_environment()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Debug Theme Analysis\n",
      "==================================================\n",
      "\n",
      "Input Text:\n",
      "--------------------\n",
      "Q3 revenue increased by 15% with strong growth in enterprise sales.\n",
      "Customer retention improved while acquisition costs decreased.\n",
      "New market expansion initiatives are showing positive early results.\n",
      "\n",
      "Running Analysis...\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 15:16:02 - src.utils.FileUtils.file_utils - INFO - Attempting to load file: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\raw\\test_content_en.xlsx\n",
      "2024-11-12 15:16:02 - src.utils.FileUtils.file_utils - INFO - Successfully loaded Excel file: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\raw\\test_content_en.xlsx\n",
      "2024-11-12 15:16:02 - src.utils.FileUtils.file_utils - INFO - Attempting to load file: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\raw\\test_content_fi.xlsx\n",
      "2024-11-12 15:16:03 - src.utils.FileUtils.file_utils - INFO - Successfully loaded Excel file: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\raw\\test_content_fi.xlsx\n",
      "2024-11-12 15:16:04,541 - src.analyzers.theme_analyzer - DEBUG - Getting LLM analysis...\n",
      "2024-11-12 15:16:08 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-12 15:16:08,159 - src.analyzers.theme_analyzer - DEBUG - Raw LLM output content: {\n",
      "    \"themes\": [\"Revenue Growth\", \"Customer Retention\", \"Market Expansion\"],\n",
      "    \"descriptions\": {\n",
      "        \"Revenue Growth\": \"The increase in revenue by 15% indicates strong performance, particularly in enterprise sales.\",\n",
      "        \"Customer Retention\": \"Improvement in customer retention alongside a decrease in acquisition costs suggests effective customer relationship management.\",\n",
      "        \"Market Expansion\": \"Positive early results from new market expansion initiatives highlight the company's strategic growth efforts.\"\n",
      "    },\n",
      "    \"confidence\": {\n",
      "        \"Revenue Growth\": 0.90,\n",
      "        \"Customer Retention\": 0.80,\n",
      "        \"Market Expansion\": 0.75\n",
      "    },\n",
      "    \"related_keywords\": {\n",
      "        \"Revenue Growth\": [\"enterprise sales\", \"revenue increase\"],\n",
      "        \"Customer Retention\": [\"customer retention\", \"acquisition costs\"],\n",
      "        \"Market Expansion\": [\"market expansion\", \"early results\"]\n",
      "    }\n",
      "}\n",
      "2024-11-12 15:16:08,163 - src.analyzers.theme_analyzer - DEBUG - Processed 3 themes successfully\n",
      "2024-11-12 15:16:08,166 - src.analyzers.theme_analyzer - DEBUG - Processing LLM results...\n",
      "2024-11-12 15:16:08,167 - src.analyzers.theme_analyzer - DEBUG - Processed 3 themes successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Theme Analysis Results:\n",
      "--------------------------------------------------\n",
      "Error: None\n",
      "\n",
      "Debug Information:\n",
      "--------------------\n",
      "{\n",
      "  \"language\": \"en\",\n",
      "  \"error\": null,\n",
      "  \"success\": true,\n",
      "  \"themes\": [\n",
      "    {\n",
      "      \"name\": \"Revenue Growth\",\n",
      "      \"description\": \"The increase in revenue by 15% indicates strong performance, particularly in enterprise sales.\",\n",
      "      \"confidence\": 0.9,\n",
      "      \"keywords\": [\n",
      "        \"enterprise sales\",\n",
      "        \"revenue increase\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Customer Retention\",\n",
      "      \"description\": \"Improvement in customer retention alongside a decrease in acquisition costs suggests effective customer relationship management.\",\n",
      "      \"confidence\": 0.8,\n",
      "      \"keywords\": [\n",
      "        \"customer retention\",\n",
      "        \"acquisition costs\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Market Expansion\",\n",
      "      \"description\": \"Positive early results from new market expansion initiatives highlight the company's strategic growth efforts.\",\n",
      "      \"confidence\": 0.75,\n",
      "      \"keywords\": [\n",
      "        \"market expansion\",\n",
      "        \"early results\"\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"theme_descriptions\": {\n",
      "    \"Revenue Growth\": \"The increase in revenue by 15% indicates strong performance, particularly in enterprise sales.\",\n",
      "    \"Customer Retention\": \"Improvement in customer retention alongside a decrease in acquisition costs suggests effective customer relationship management.\",\n",
      "    \"Market Expansion\": \"Positive early results from new market expansion initiatives highlight the company's strategic growth efforts.\"\n",
      "  },\n",
      "  \"theme_confidence\": {\n",
      "    \"Revenue Growth\": 0.9,\n",
      "    \"Customer Retention\": 0.8,\n",
      "    \"Market Expansion\": 0.75\n",
      "  },\n",
      "  \"related_keywords\": {\n",
      "    \"Revenue Growth\": [\n",
      "      \"enterprise sales\",\n",
      "      \"revenue increase\"\n",
      "    ],\n",
      "    \"Customer Retention\": [\n",
      "      \"customer retention\",\n",
      "      \"acquisition costs\"\n",
      "    ],\n",
      "    \"Market Expansion\": [\n",
      "      \"market expansion\",\n",
      "      \"early results\"\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ThemeOutput(language='en', error=None, success=True, themes=[ThemeInfo(name='Revenue Growth', description='The increase in revenue by 15% indicates strong performance, particularly in enterprise sales.', confidence=0.9, keywords=['enterprise sales', 'revenue increase']), ThemeInfo(name='Customer Retention', description='Improvement in customer retention alongside a decrease in acquisition costs suggests effective customer relationship management.', confidence=0.8, keywords=['customer retention', 'acquisition costs']), ThemeInfo(name='Market Expansion', description=\"Positive early results from new market expansion initiatives highlight the company's strategic growth efforts.\", confidence=0.75, keywords=['market expansion', 'early results'])], theme_descriptions={'Revenue Growth': 'The increase in revenue by 15% indicates strong performance, particularly in enterprise sales.', 'Customer Retention': 'Improvement in customer retention alongside a decrease in acquisition costs suggests effective customer relationship management.', 'Market Expansion': \"Positive early results from new market expansion initiatives highlight the company's strategic growth efforts.\"}, theme_confidence={'Revenue Growth': 0.9, 'Customer Retention': 0.8, 'Market Expansion': 0.75}, related_keywords={'Revenue Growth': ['enterprise sales', 'revenue increase'], 'Customer Retention': ['customer retention', 'acquisition costs'], 'Market Expansion': ['market expansion', 'early results']})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the analysis\n",
    "text = \"\"\"\n",
    "Q3 revenue increased by 15% with strong growth in enterprise sales.\n",
    "Customer retention improved while acquisition costs decreased.\n",
    "New market expansion initiatives are showing positive early results.\n",
    "\"\"\"\n",
    "\n",
    "await debug_theme_analysis(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second cell: Run analysis tests\n",
    "async def run_analysis_examples():\n",
    "    \"\"\"Run example analyses on different content types.\"\"\"\n",
    "    \n",
    "    # Example texts\n",
    "    example_texts = {\n",
    "        \"Business Analysis\": \"\"\"\n",
    "            Q3 revenue increased by 15% with strong growth in enterprise sales.\n",
    "            Customer retention improved while acquisition costs decreased.\n",
    "            New market expansion initiatives are showing positive early results.\n",
    "        \"\"\",\n",
    "        \n",
    "        \"Technical Content\": \"\"\"\n",
    "            The application uses microservices architecture with containerized deployments.\n",
    "            Data processing pipeline incorporates machine learning models for prediction.\n",
    "            System monitoring ensures high availability and performance metrics.\n",
    "        \"\"\",\n",
    "        \n",
    "        \"Mixed Content\": \"\"\"\n",
    "            The IT department's cloud migration project reduced infrastructure costs by 25%.\n",
    "            DevOps implementation improved deployment frequency while maintaining quality.\n",
    "            Monthly recurring revenue from SaaS products grew steadily.\n",
    "        \"\"\"\n",
    "    }\n",
    "    \n",
    "    pipeline = AnalysisPipeline()\n",
    "    \n",
    "    for title, text in example_texts.items():\n",
    "        print(f\"\\nAnalyzing {title}\")\n",
    "        print(\"=\" * 50)\n",
    "        results = await pipeline.analyze_text(text)\n",
    "        pipeline.display_results(results)\n",
    "        print(\"\\n\" + \"-\" * 80 + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Third cell: Interactive analysis\n",
    "# async def analyze_custom_text(text: str, language: str = None):\n",
    "#     \"\"\"Analyze custom text with complete pipeline.\"\"\"\n",
    "#     return await analyze_text_pipeline(text, language)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run complete analysis examples\n",
    "await run_analysis_examples()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or analyze specific text\n",
    "text = \"\"\"Your text here...\"\"\"\n",
    "await analyze_custom_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "await run_analysis_examples()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or analyze specific text:\n",
    "# text = \"\"\"Your text here...\"\"\"\n",
    "# await analyze_custom_text(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semantic-analyzer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
