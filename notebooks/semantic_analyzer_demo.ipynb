{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Text Analyzer Demo Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Setup and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added /home/topi/data-science/repos/semantic-text-analyzer to Python path\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import asyncio\n",
    "import logging\n",
    "from typing import Dict, Any, Optional\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = str(Path().resolve().parent)\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "    print(f\"Added {project_root} to Python path\")\n",
    "\n",
    "# Core components\n",
    "from src.semantic_analyzer.analyzer import SemanticAnalyzer\n",
    "from src.utils.FileUtils.file_utils import FileUtils\n",
    "from src.loaders.parameter_adapter import ParameterAdapter\n",
    "from src.core.config import AnalyzerConfig\n",
    "from src.loaders.models import ParameterSet\n",
    "\n",
    "from src.analyzers.keyword_analyzer import KeywordAnalyzer\n",
    "from src.semantic_analyzer.parameters import ParameterManager\n",
    "\n",
    "# Initialize FileUtils and set up logging\n",
    "file_utils = FileUtils()\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Check Results:\n",
      "==================================================\n",
      "\n",
      "Basic Setup:\n",
      "-----------\n",
      "✓ Project root in path\n",
      "✓ Can import src\n",
      "✓ FileUtils initialized\n",
      "✓ .env file loaded\n",
      "\n",
      "Environment Variables:\n",
      "---------------------\n",
      "✓ OPENAI_API_KEY set\n",
      "✓ ANTHROPIC_API_KEY set\n",
      "\n",
      "Project Structure:\n",
      "-----------------\n",
      "✓ Raw data exists\n",
      "✓ Processed data exists\n",
      "✓ Configuration exists\n",
      "✓ Main config.yaml exists\n",
      "\n",
      "==================================================\n",
      "Environment Status: Ready ✓\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def verify_environment():\n",
    "    \"\"\"Verify that the notebook environment is properly configured.\"\"\"\n",
    "    # Load environment variables\n",
    "    from dotenv import load_dotenv\n",
    "    env_path = Path(project_root) / \".env\"\n",
    "    env_loaded = load_dotenv(env_path)\n",
    "\n",
    "    # Required environment variables\n",
    "    required_env_vars = [\n",
    "        'OPENAI_API_KEY',\n",
    "        'ANTHROPIC_API_KEY',\n",
    "    ]\n",
    "\n",
    "    # Basic checks\n",
    "    basic_checks = {\n",
    "        \"Project root in path\": project_root in sys.path,\n",
    "        \"Can import src\": \"src\" in sys.modules,\n",
    "        \"FileUtils initialized\": hasattr(file_utils, \"project_root\"),\n",
    "        \".env file loaded\": env_loaded,\n",
    "    }\n",
    "\n",
    "    # Environment variable checks\n",
    "    env_var_checks = {\n",
    "        f\"{var} set\": os.getenv(var) is not None\n",
    "        for var in required_env_vars\n",
    "    }\n",
    "\n",
    "    # Check for required paths using FileUtils\n",
    "    expected_paths = {\n",
    "        \"Raw data\": file_utils.get_data_path(\"raw\"),\n",
    "        \"Processed data\": file_utils.get_data_path(\"processed\"),\n",
    "        \"Configuration\": file_utils.get_data_path(\"configurations\"),\n",
    "        \"Main config.yaml\": Path(project_root) / \"config.yaml\"\n",
    "    }\n",
    "    \n",
    "    path_checks = {\n",
    "        f\"{name} exists\": path.exists()\n",
    "        for name, path in expected_paths.items()\n",
    "    }\n",
    "\n",
    "    # Combine all checks\n",
    "    all_checks = {\n",
    "        **basic_checks,\n",
    "        **env_var_checks,\n",
    "        **path_checks\n",
    "    }\n",
    "    \n",
    "    # Print results\n",
    "    print(\"Environment Check Results:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    def print_section(title, checks):\n",
    "        print(f\"\\n{title}:\")\n",
    "        print(\"-\" * len(title))\n",
    "        for check, result in checks.items():\n",
    "            status = \"✓\" if result else \"✗\"\n",
    "            print(f\"{status} {check}\")\n",
    "    \n",
    "    print_section(\"Basic Setup\", basic_checks)\n",
    "    print_section(\"Environment Variables\", env_var_checks)\n",
    "    print_section(\"Project Structure\", path_checks)\n",
    "    \n",
    "    # Overall status\n",
    "    all_passed = all(all_checks.values())\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Environment Status:\", \"Ready ✓\" if all_passed else \"Setup needed ✗\")\n",
    "    \n",
    "    if not all_passed:\n",
    "        print(\"\\nSetup Instructions:\")\n",
    "        if not env_loaded:\n",
    "            print(\"- Create a .env file in the project root with required API keys\")\n",
    "        for var in required_env_vars:\n",
    "            if not os.getenv(var):\n",
    "                print(f\"- Add {var} to your .env file\")\n",
    "        for name, path in expected_paths.items():\n",
    "            if not path.exists():\n",
    "                print(f\"- Create {name} directory at {path}\")\n",
    "\n",
    "    return all_passed\n",
    "\n",
    "# Run verification\n",
    "verify_environment()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 22:10:25 - src.core.language_processing.factory - INFO - Using default configuration\n",
      "2024-11-09 22:10:25 - src.core.language_processing.factory - INFO - Using default configuration\n",
      "2024-11-09 22:10:25 - src.core.language_processing.finnish - INFO - Loaded 747 stopwords from /home/topi/data-science/repos/semantic-text-analyzer/data/configurations/stop_words/fi.txt\n",
      "2024-11-09 22:10:25 - src.core.language_processing.finnish - INFO - Successfully initialized Voikko using system libraries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stopwords Directory Contents:\n",
      "Directory: /home/topi/data-science/repos/semantic-text-analyzer/data/configurations/stop_words\n",
      "\n",
      "File: fi.txt\n",
      "Number of words: 747\n",
      "Sample words (first 10):\n",
      "['aika',\n",
      " 'aikaa',\n",
      " 'aikaan',\n",
      " 'aikaisemmin',\n",
      " 'aikaisin',\n",
      " 'aikajen',\n",
      " 'aikana',\n",
      " 'aikoina',\n",
      " 'aikoo',\n",
      " 'aikovat']\n",
      "\n",
      "File: en.txt\n",
      "Number of words: 4\n",
      "Sample words (first 10):\n",
      "['a', 'an', 'and', 'the']\n",
      "\n",
      "Testing Language Processors:\n",
      "\n",
      "EN Processor:\n",
      "Number of stopwords: 242\n",
      "Sample stopwords (first 10):\n",
      "[\"'d\", \"'ll\", \"'re\", \"'s\", \"'t\", \"'ve\", 'a', 'about', 'above', 'actually']\n",
      "\n",
      "Testing common en words:\n",
      "the: stop word\n",
      "and: stop word\n",
      "is: stop word\n",
      "that: stop word\n",
      "with: stop word\n",
      "for: stop word\n",
      "\n",
      "FI Processor:\n",
      "Number of stopwords: 774\n",
      "Sample stopwords (first 10):\n",
      "['aiempi',\n",
      " 'aika',\n",
      " 'aikaa',\n",
      " 'aikaan',\n",
      " 'aikaisemmin',\n",
      " 'aikaisin',\n",
      " 'aikajen',\n",
      " 'aikana',\n",
      " 'aikoina',\n",
      " 'aikoo']\n",
      "\n",
      "Testing common fi words:\n",
      "ja: stop word\n",
      "on: stop word\n",
      "että: stop word\n",
      "joka: stop word\n",
      "tai: stop word\n",
      "vain: stop word\n"
     ]
    }
   ],
   "source": [
    "# verify_stopwords.ipynb\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from pprint import pprint\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = str(Path().resolve().parent)\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from src.core.language_processing import create_text_processor\n",
    "from src.utils.FileUtils.file_utils import FileUtils\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def verify_stopwords():\n",
    "    \"\"\"Verify stopwords for both English and Finnish.\"\"\"\n",
    "    \n",
    "    file_utils = FileUtils()\n",
    "    \n",
    "    # Check stopwords files\n",
    "    stop_words_dir = file_utils.get_data_path(\"configurations\") / \"stop_words\"\n",
    "    print(\"\\nStopwords Directory Contents:\")\n",
    "    print(f\"Directory: {stop_words_dir}\")\n",
    "    if stop_words_dir.exists():\n",
    "        for file in stop_words_dir.glob(\"*.txt\"):\n",
    "            print(f\"\\nFile: {file.name}\")\n",
    "            with open(file, 'r', encoding='utf-8') as f:\n",
    "                words = {line.strip() for line in f if line.strip()}\n",
    "                print(f\"Number of words: {len(words)}\")\n",
    "                print(\"Sample words (first 10):\")\n",
    "                pprint(sorted(list(words))[:10])\n",
    "    else:\n",
    "        print(\"Stopwords directory not found!\")\n",
    "    \n",
    "    # Test processors\n",
    "    print(\"\\nTesting Language Processors:\")\n",
    "    for lang in [\"en\", \"fi\"]:\n",
    "        print(f\"\\n{lang.upper()} Processor:\")\n",
    "        processor = create_text_processor(language=lang)\n",
    "        \n",
    "        # Check stopwords\n",
    "        if hasattr(processor, '_stop_words'):\n",
    "            print(f\"Number of stopwords: {len(processor._stop_words)}\")\n",
    "            print(\"Sample stopwords (first 10):\")\n",
    "            pprint(sorted(list(processor._stop_words))[:10])\n",
    "        \n",
    "        # Test some common words\n",
    "        test_words = {\n",
    "            \"en\": [\"the\", \"and\", \"is\", \"that\", \"with\", \"for\"],\n",
    "            \"fi\": [\"ja\", \"on\", \"että\", \"joka\", \"tai\", \"vain\"]\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nTesting common {lang} words:\")\n",
    "        for word in test_words[lang]:\n",
    "            print(f\"{word}: {'stop word' if processor.is_stop_word(word) else 'not stop word'}\")\n",
    "\n",
    "# Run verification\n",
    "verify_stopwords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_keywords.ipynb\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import asyncio\n",
    "import logging\n",
    "from typing import Dict, Any, Optional\n",
    "from pprint import pprint\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = str(Path().resolve().parent)\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Core components\n",
    "from src import KeywordAnalyzer\n",
    "from src.core.language_processing import create_text_processor\n",
    "\n",
    "# # Initialize logging\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "# logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 22:10:49 - src.core.language_processing.factory - INFO - Using default configuration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing English texts...\n",
      "\n",
      "Analyzing Text:\n",
      "==================================================\n",
      "Text: \n",
      "    Machine learning is a branch of artificial intelligence that focuses on developing systems \n",
      "   ...\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 22:11:12 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-09 22:11:12 - src.core.language_processing.factory - INFO - Using default configuration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Keywords:\n",
      "- machine\n",
      "- learn\n",
      "- branch\n",
      "- artificial\n",
      "- intelligence\n",
      "- focus\n",
      "- develop\n",
      "- system\n",
      "- make\n",
      "- decision\n",
      "\n",
      "Compound Words:\n",
      "- machine learning\n",
      "- artificial intelligence\n",
      "- deep learning\n",
      "- neural networks\n",
      "\n",
      "Language: en\n",
      "Success: True\n",
      "\n",
      "Analyzing Text:\n",
      "==================================================\n",
      "Text: \n",
      "    Natural Language Processing (NLP) combines linguistics and computer science to help \n",
      "    comput...\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 22:11:14 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-09 22:11:14 - src.core.language_processing.factory - INFO - Using default configuration\n",
      "2024-11-09 22:11:14 - src.core.language_processing.finnish - INFO - Loaded 747 stopwords from /home/topi/data-science/repos/semantic-text-analyzer/data/configurations/stop_words/fi.txt\n",
      "2024-11-09 22:11:14 - src.core.language_processing.finnish - INFO - Successfully initialized Voikko using system libraries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Keywords:\n",
      "- natural\n",
      "- language\n",
      "- process\n",
      "- nlp\n",
      "- combine\n",
      "- linguistics\n",
      "- computer\n",
      "- science\n",
      "- help\n",
      "- understand\n",
      "\n",
      "Compound Words:\n",
      "- Natural Language\n",
      "- machine translation\n",
      "\n",
      "Language: en\n",
      "Success: True\n",
      "\n",
      "Testing Finnish text...\n",
      "\n",
      "Analyzing Text:\n",
      "==================================================\n",
      "Text: \n",
      "    Koneoppiminen on tekoälyn osa-alue, joka keskittyy järjestelmiin, jotka oppivat datasta. \n",
      "    S...\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 22:11:18 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Keywords:\n",
      "- koneoppia\n",
      "- olla\n",
      "- tekoäly\n",
      "- osa-alue\n",
      "- joka\n",
      "- keskittyä\n",
      "- järjestelmä\n",
      "- oppia\n",
      "- data\n",
      "- syväoppia\n",
      "\n",
      "Compound Words:\n",
      "- koneoppiminen\n",
      "- syväoppiminen\n",
      "- neuroverkot\n",
      "\n",
      "Language: fi\n",
      "Success: True\n"
     ]
    }
   ],
   "source": [
    "# test_keywords.ipynb\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import asyncio\n",
    "import logging\n",
    "from typing import Dict, Any, Optional\n",
    "from pprint import pprint\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = str(Path().resolve().parent)\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Core components\n",
    "from src import KeywordAnalyzer\n",
    "from src.core.language_processing import create_text_processor\n",
    "from src.schemas import KeywordInfo\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Test texts\n",
    "test_texts = {\n",
    "    \"ai_ml\": \"\"\"\n",
    "    Machine learning is a branch of artificial intelligence that focuses on developing systems \n",
    "    that can learn from and make decisions based on data. Deep learning, a subset of machine \n",
    "    learning, uses neural networks with multiple layers to analyze various levels of abstraction.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"nlp\": \"\"\"\n",
    "    Natural Language Processing (NLP) combines linguistics and computer science to help \n",
    "    computers understand and process human language. Key tasks include sentiment analysis, \n",
    "    text classification, and machine translation.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"finnish\": \"\"\"\n",
    "    Koneoppiminen on tekoälyn osa-alue, joka keskittyy järjestelmiin, jotka oppivat datasta. \n",
    "    Syväoppiminen käyttää neuroverkkoja monimutkaisten mallien analysointiin.\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "async def test_keyword_analysis(text: str, language: str = \"en\"):\n",
    "    \"\"\"Test keyword extraction for a given text.\"\"\"\n",
    "    \n",
    "    # Create language processor\n",
    "    language_processor = create_text_processor(language=language)\n",
    "    \n",
    "    # Initialize analyzer with language processor\n",
    "    analyzer = KeywordAnalyzer(\n",
    "        config={\n",
    "            \"max_keywords\": 10,\n",
    "            \"min_length\": 3,\n",
    "            \"weights\": {\n",
    "                \"statistical\": 0.4,\n",
    "                \"llm\": 0.6\n",
    "            }\n",
    "        },\n",
    "        language_processor=language_processor\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nAnalyzing Text:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Text: {text[:100]}...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Run analysis\n",
    "    result = await analyzer.analyze(text)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\nExtracted Keywords:\")\n",
    "    if result.keywords:  # Check if we have keywords\n",
    "        for kw in result.keywords:\n",
    "            if isinstance(kw, KeywordInfo):\n",
    "                # Handle KeywordInfo objects\n",
    "                print(f\"- {kw.keyword:<30} (score: {kw.score:.3f})\")\n",
    "                if kw.domain:\n",
    "                    print(f\"  Domain: {kw.domain}\")\n",
    "                if kw.compound_parts:\n",
    "                    print(f\"  Parts: {', '.join(kw.compound_parts)}\")\n",
    "            else:\n",
    "                # Handle string or dict keywords\n",
    "                if isinstance(kw, dict):\n",
    "                    print(f\"- {kw.get('keyword', 'N/A'):<30} (score: {kw.get('score', 0.0):.3f})\")\n",
    "                else:\n",
    "                    print(f\"- {str(kw)}\")\n",
    "    \n",
    "    print(\"\\nCompound Words:\")\n",
    "    if result.compound_words:\n",
    "        for compound in result.compound_words:\n",
    "            print(f\"- {compound}\")\n",
    "    \n",
    "    print(\"\\nLanguage:\", result.language)\n",
    "    print(\"Success:\", result.success)\n",
    "    if result.error:\n",
    "        print(\"Error:\", result.error)\n",
    "\n",
    "async def run_tests():\n",
    "    \"\"\"Run tests for all sample texts.\"\"\"\n",
    "    print(\"\\nTesting English texts...\")\n",
    "    await test_keyword_analysis(test_texts[\"ai_ml\"], \"en\")\n",
    "    await test_keyword_analysis(test_texts[\"nlp\"], \"en\")\n",
    "    \n",
    "    print(\"\\nTesting Finnish text...\")\n",
    "    await test_keyword_analysis(test_texts[\"finnish\"], \"fi\")\n",
    "\n",
    "# Run tests\n",
    "await run_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions and Test Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotebookAnalyzer:\n",
    "    \"\"\"Helper class for running analyses in the notebook.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.file_utils = FileUtils()\n",
    "        self.test_texts = self._load_test_texts()\n",
    "        \n",
    "        # Use parameter file from configurations directory\n",
    "        self.parameter_file = self.file_utils.get_data_path(\"configurations\") / \"example_params.xlsx\"\n",
    "        \n",
    "        # Create the parameter file if it doesn't exist\n",
    "        if not self.parameter_file.exists():\n",
    "            self._create_parameter_file()\n",
    "        \n",
    "        self.param_adapter = ParameterAdapter(self.parameter_file)\n",
    "        \n",
    "        logger.info(f\"Initialized NotebookAnalyzer with {len(self.test_texts)} test texts\")\n",
    "    \n",
    "    def _load_test_texts(self) -> Dict[str, str]:\n",
    "        \"\"\"Load or create test texts.\"\"\"\n",
    "        texts = {\n",
    "            \"technical\": \"\"\"\n",
    "            Python is a high-level programming language known for its simplicity.\n",
    "            It supports multiple programming paradigms including procedural and\n",
    "            object-oriented programming.\n",
    "            \"\"\",\n",
    "            \"business\": \"\"\"\n",
    "            The company's Q3 results exceeded expectations with revenue growth of 15%.\n",
    "            Customer acquisition costs decreased while retention rates improved.\n",
    "            The board has approved a new strategic initiative focusing on expansion.\n",
    "            \"\"\",\n",
    "            \"finnish\": \"\"\"\n",
    "            Ohjelmistokehittäjä työskentelee asiakasprojektissa kehittäen\n",
    "            verkkokauppajärjestelmää. Tekninen toteutus vaatii erityistä huomiota\n",
    "            tietoturvan osalta.\n",
    "            \"\"\"\n",
    "        }\n",
    "        \n",
    "        # Save texts using FileUtils\n",
    "        df = pd.DataFrame([\n",
    "            {\"name\": name, \"content\": content.strip()}\n",
    "            for name, content in texts.items()\n",
    "        ])\n",
    "        \n",
    "        self.file_utils.save_data_to_disk(\n",
    "            data={\"texts\": df},\n",
    "            output_type=\"raw\",\n",
    "            file_name=\"test_texts\",\n",
    "            output_filetype=\"xlsx\",\n",
    "            include_timestamp=False\n",
    "        )\n",
    "        \n",
    "        return texts\n",
    "\n",
    "    def _create_parameter_file(self):\n",
    "        \"\"\"Create initial parameter Excel file.\"\"\"\n",
    "        parameters_data = {\n",
    "            'General Parameters': pd.DataFrame({\n",
    "                'parameter': [\n",
    "                    'max_kws',\n",
    "                    'max_themes',\n",
    "                    'focus_on',\n",
    "                    'language',\n",
    "                    'additional_context',\n",
    "                    'min_confidence',\n",
    "                    'include_compounds',\n",
    "                    'column_name_to_analyze'\n",
    "                ],\n",
    "                'value': [\n",
    "                    8,\n",
    "                    3,\n",
    "                    'business and financial content',\n",
    "                    'en',\n",
    "                    'Business performance analysis',\n",
    "                    0.3,\n",
    "                    True,\n",
    "                    'text'\n",
    "                ]\n",
    "            }),\n",
    "            'Excluded Keywords': pd.DataFrame({\n",
    "                'keyword': [\n",
    "                    'the', 'with', 'while', 'new',\n",
    "                    'company', 'business', 'results',\n",
    "                    'approximately', 'significantly',\n",
    "                    'current', 'various', 'multiple'\n",
    "                ],\n",
    "                'reason': [\n",
    "                    'Common word', 'Common word', 'Common word', 'Common word',\n",
    "                    'Too generic', 'Too generic', 'Too generic',\n",
    "                    'Modifier', 'Modifier',\n",
    "                    'Vague term', 'Vague term', 'Vague term'\n",
    "                ]\n",
    "            }),\n",
    "            'Categories': pd.DataFrame({\n",
    "                'category': [\n",
    "                    'business_performance',\n",
    "                    'financial_metrics',\n",
    "                    'strategy',\n",
    "                    'operations'\n",
    "                ],\n",
    "                'description': [\n",
    "                    'Business performance indicators and results',\n",
    "                    'Financial and revenue related metrics',\n",
    "                    'Strategic initiatives and planning',\n",
    "                    'Operational metrics and processes'\n",
    "                ],\n",
    "                'keywords': [\n",
    "                    'revenue,growth,performance,results,expectations',\n",
    "                    'costs,revenue,profit,margin,acquisition',\n",
    "                    'initiative,strategic,expansion,planning,board',\n",
    "                    'operations,efficiency,retention,improvement,process'\n",
    "                ],\n",
    "                'threshold': [0.6, 0.6, 0.6, 0.6]\n",
    "            })\n",
    "        }\n",
    "        \n",
    "        self.file_utils.save_data_to_disk(\n",
    "            data=parameters_data,\n",
    "            output_type=\"configurations\",\n",
    "            file_name=\"example_params\",\n",
    "            output_filetype=\"xlsx\",\n",
    "            include_timestamp=False\n",
    "        )\n",
    "\n",
    "    async def analyze_text(self, text_key: str, **kwargs):\n",
    "        \"\"\"Analyze a specific text sample.\"\"\"\n",
    "        if text_key not in self.test_texts:\n",
    "            raise ValueError(f\"Unknown text key: {text_key}. Available keys: {list(self.test_texts.keys())}\")\n",
    "        \n",
    "        # Load parameters\n",
    "        params = self.param_adapter.load_and_convert()\n",
    "        \n",
    "        # Get excluded keywords from parameter file\n",
    "        excluded_keywords = set()\n",
    "        try:\n",
    "            excluded_df = pd.read_excel(self.parameter_file, sheet_name='Excluded Keywords')\n",
    "            excluded_keywords = set(excluded_df['keyword'].dropna())\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not load excluded keywords: {e}\")\n",
    "        \n",
    "        # Create analyzer with explicit config\n",
    "        analyzer = SemanticAnalyzer(\n",
    "            parameter_file=self.parameter_file,\n",
    "            config={\n",
    "                \"excluded_keywords\": excluded_keywords,\n",
    "                \"min_keyword_length\": 3,\n",
    "                **kwargs.get('config', {})\n",
    "            },\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        # Run analysis\n",
    "        results = await analyzer.analyze(self.test_texts[text_key], **kwargs)\n",
    "        await self.display_results(results)\n",
    "        return results\n",
    "            \n",
    "    \n",
    "    @staticmethod\n",
    "    async def display_results(results: Dict[str, Any]) -> None:\n",
    "        \"\"\"Display analysis results in a formatted way.\"\"\"\n",
    "        for analysis_type, data in results.items():\n",
    "            print(f\"\\n{'='*20} {analysis_type.upper()} {'='*20}\")\n",
    "            if isinstance(data, dict):\n",
    "                for key, value in data.items():\n",
    "                    if isinstance(value, (list, dict)):\n",
    "                        print(f\"\\n{key}:\")\n",
    "                        print(value)\n",
    "                    else:\n",
    "                        print(f\"{key}: {value}\")\n",
    "            else:\n",
    "                print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 22:12:32 - src.utils.FileUtils.file_utils - INFO - Data saved to /home/topi/data-science/repos/semantic-text-analyzer/data/raw/test_texts.xlsx\n",
      "2024-11-09 22:12:32 - src.utils.FileUtils.file_utils - INFO - Successfully loaded 3 sheets from /home/topi/data-science/repos/semantic-text-analyzer/data/configurations/example_params.xlsx\n",
      "2024-11-09 22:12:32 - __main__ - INFO - Initialized NotebookAnalyzer with 3 test texts\n"
     ]
    }
   ],
   "source": [
    "# Initialize analyzer\n",
    "notebook_analyzer = NotebookAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Setup Verification ===\n",
      "\n",
      "Available test texts:\n",
      "\n",
      "technical:\n",
      "\n",
      "            Python is a high-level programming language known for its simplicity.\n",
      "            It su...\n",
      "\n",
      "business:\n",
      "\n",
      "            The company's Q3 results exceeded expectations with revenue growth of 15%.\n",
      "            ...\n",
      "\n",
      "finnish:\n",
      "\n",
      "            Ohjelmistokehittäjä työskentelee asiakasprojektissa kehittäen\n",
      "            verkkokauppaj...\n",
      "\n",
      "=== Parameter File ===\n",
      "Location: /home/topi/data-science/repos/semantic-text-analyzer/data/configurations/example_params.xlsx\n",
      "Parameters loaded: True\n"
     ]
    }
   ],
   "source": [
    "async def verify_setup():\n",
    "    \"\"\"Verify analyzer setup and available texts.\"\"\"\n",
    "    print(\"=== Setup Verification ===\")\n",
    "    print(\"\\nAvailable test texts:\")\n",
    "    for name, text in notebook_analyzer.test_texts.items():\n",
    "        print(f\"\\n{name}:\")\n",
    "        print(text[:100] + \"...\")  # Show first 100 chars\n",
    "    \n",
    "    print(\"\\n=== Parameter File ===\")\n",
    "    print(f\"Location: {notebook_analyzer.parameter_file}\")\n",
    "    print(\"Parameters loaded:\", notebook_analyzer.param_adapter is not None)\n",
    "\n",
    "# Run verification\n",
    "await verify_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify parameter loading and stopword handling\n",
    "async def verify_parameters():\n",
    "    \"\"\"Verify parameter loading and stopword handling.\"\"\"\n",
    "    # Load parameters\n",
    "    params = notebook_analyzer.param_adapter.load_and_convert()\n",
    "    \n",
    "    print(\"=== Parameter Verification ===\")\n",
    "    print(\"\\nExcluded Keywords from parameters:\")\n",
    "    if hasattr(params, 'excluded_keywords'):\n",
    "        print(sorted(list(params.excluded_keywords)))\n",
    "    else:\n",
    "        print(\"No excluded keywords in parameters\")\n",
    "    \n",
    "    # Create analyzer with parameters\n",
    "    analyzer = SemanticAnalyzer(\n",
    "        parameter_file=notebook_analyzer.parameter_file,\n",
    "        language=\"en\"\n",
    "    )\n",
    "    \n",
    "    print(\"\\n=== Language Processing Setup ===\")\n",
    "    print(f\"Language: {analyzer.text_processor.language}\")\n",
    "    print(f\"\\nStopwords (NLTK + custom):\")\n",
    "    print(f\"Total stopwords: {len(analyzer.text_processor._stop_words)}\")\n",
    "    print(\"\\nSample stopwords (first 20):\")\n",
    "    print(sorted(list(analyzer.text_processor._stop_words))[:20])\n",
    "    \n",
    "    print(f\"\\nExcluded keywords from config:\")\n",
    "    print(sorted(list(analyzer.text_processor.excluded_keywords)))\n",
    "    \n",
    "    return analyzer\n",
    "\n",
    "# Run verification\n",
    "# analyzer = await verify_parameters()\n",
    "\n",
    "# Test analysis with verified setup\n",
    "# results = await analyzer.analyze(\n",
    "#     notebook_analyzer.test_texts['business'],\n",
    "#     analysis_types=[\"keywords\", \"themes\", \"categories\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-09 22:12:48 - src.utils.FileUtils.file_utils - INFO - Successfully loaded 3 sheets from /home/topi/data-science/repos/semantic-text-analyzer/data/configurations/example_params.xlsx\n",
      "2024-11-09 22:12:48 - src.utils.FileUtils.file_utils - INFO - Successfully loaded 3 sheets from /home/topi/data-science/repos/semantic-text-analyzer/data/configurations/example_params.xlsx\n",
      "2024-11-09 22:12:48 - src.core.language_processing.factory - INFO - Using default configuration\n",
      "2024-11-09 22:12:48 - src.semantic_analyzer.analyzer - INFO - Initialized SemanticAnalyzer with language: en, using parameter file: /home/topi/data-science/repos/semantic-text-analyzer/data/configurations/example_params.xlsx\n",
      "2024-11-09 22:12:48 - src.semantic_analyzer.analyzer - INFO - Initialized SemanticAnalyzer with language: en, using parameter file: /home/topi/data-science/repos/semantic-text-analyzer/data/configurations/example_params.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Keyword Extraction Test ===\n",
      "\n",
      "Original text:\n",
      "\n",
      "            The company's Q3 results exceeded expectations with revenue growth of 15%.\n",
      "            Customer acquisition costs decreased while retention rates improved.\n",
      "            The board has approved a new strategic initiative focusing on expansion.\n",
      "            \n",
      "\n",
      "Tokens after stopword removal:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'EnglishTextProcessor' object has no attribute 'should_exclude_word'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mDomain keywords:\u001b[39m\u001b[38;5;124m\"\u001b[39m, keywords\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdomain_keywords\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Run the test\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m test_keyword_extraction()\n",
      "Cell \u001b[0;32mIn[10], line 28\u001b[0m, in \u001b[0;36mtest_keyword_extraction\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m tokens \u001b[38;5;241m=\u001b[39m analyzer\u001b[38;5;241m.\u001b[39mtext_processor\u001b[38;5;241m.\u001b[39mtokenize(text)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTokens after stopword removal:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m filtered \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens \n\u001b[1;32m     29\u001b[0m            \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m analyzer\u001b[38;5;241m.\u001b[39mtext_processor\u001b[38;5;241m.\u001b[39mshould_exclude_word(word)]\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(filtered)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Run analysis\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 29\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     26\u001b[0m tokens \u001b[38;5;241m=\u001b[39m analyzer\u001b[38;5;241m.\u001b[39mtext_processor\u001b[38;5;241m.\u001b[39mtokenize(text)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTokens after stopword removal:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m filtered \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens \n\u001b[0;32m---> 29\u001b[0m            \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m analyzer\u001b[38;5;241m.\u001b[39mtext_processor\u001b[38;5;241m.\u001b[39mshould_exclude_word(word)]\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(filtered)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Run analysis\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'EnglishTextProcessor' object has no attribute 'should_exclude_word'"
     ]
    }
   ],
   "source": [
    "async def test_keyword_extraction():\n",
    "    \"\"\"Test keyword extraction with full debug output.\"\"\"\n",
    "    text_key = \"business\"\n",
    "    text = notebook_analyzer.test_texts[text_key]\n",
    "    \n",
    "    # Create analyzer with explicit config\n",
    "    excluded_df = pd.read_excel(notebook_analyzer.parameter_file, sheet_name='Excluded Keywords')\n",
    "    excluded_keywords = set(excluded_df['keyword'].dropna())\n",
    "    \n",
    "    analyzer = SemanticAnalyzer(\n",
    "        parameter_file=notebook_analyzer.parameter_file,\n",
    "        language=\"en\",\n",
    "        config={\n",
    "            \"excluded_keywords\": excluded_keywords,\n",
    "            \"min_keyword_length\": 3,\n",
    "            \"max_keywords\": 8,\n",
    "            \"focus\": \"business metrics and performance\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(\"=== Keyword Extraction Test ===\")\n",
    "    print(\"\\nOriginal text:\")\n",
    "    print(text)\n",
    "    \n",
    "    # Show processing steps\n",
    "    tokens = analyzer.text_processor.tokenize(text)\n",
    "    print(\"\\nTokens after stopword removal:\")\n",
    "    filtered = [word for word in tokens \n",
    "               if not analyzer.text_processor.should_exclude_word(word)]\n",
    "    print(filtered)\n",
    "    \n",
    "    # Run analysis\n",
    "    results = await analyzer.analyze(\n",
    "        text, \n",
    "        analysis_types=[\"keywords\"]\n",
    "    )\n",
    "    \n",
    "    print(\"\\nExtracted Keywords:\")\n",
    "    if \"keywords\" in results:\n",
    "        keywords = results[\"keywords\"]\n",
    "        print(\"\\nKeywords:\", keywords.get(\"keywords\", []))\n",
    "        print(\"\\nCompound words:\", keywords.get(\"compound_words\", []))\n",
    "        print(\"\\nDomain keywords:\", keywords.get(\"domain_keywords\", {}))\n",
    "\n",
    "# Run the test\n",
    "await test_keyword_extraction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis with explicit stopword settings\n",
    "params = notebook_analyzer.param_adapter.load_and_convert()\n",
    "\n",
    "# Add some business-specific words to exclude\n",
    "additional_config = {\n",
    "    \"excluded_keywords\": [\"the\", \"with\", \"while\", \"new\"],  # Common words we want to exclude\n",
    "}\n",
    "\n",
    "# Run analysis\n",
    "results = await notebook_analyzer.analyze_text(\n",
    "    \"business\",\n",
    "    analysis_types=[\"keywords\", \"themes\", \"categories\"],\n",
    "    language=\"en\",\n",
    "    parameter_file=notebook_analyzer.parameter_file,\n",
    "    config=additional_config  # Add our additional config\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Analysis Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parameters and analyze business text\n",
    "params = notebook_analyzer.param_adapter.load_and_convert()\n",
    "\n",
    "# Analyze with additional settings\n",
    "results = await notebook_analyzer.analyze_text(\n",
    "    \"business\",\n",
    "    analysis_types=[\"keywords\", \"themes\", \"categories\"],\n",
    "    language=\"en\",\n",
    "    parameter_file=notebook_analyzer.parameter_file,\n",
    "    min_confidence=0.3,  # Ensure we get broader results\n",
    "    include_compounds=True,  # Explicitly enable compound words\n",
    "    focus_on=\"business and financial content\"  # Set specific focus\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results in a more readable format\n",
    "async def display_formatted_results(results):\n",
    "    \"\"\"Display results with better formatting.\"\"\"\n",
    "    for analysis_type, data in results.items():\n",
    "        print(f\"\\n{'='*20} {analysis_type.upper()} {'='*20}\")\n",
    "        if isinstance(data, dict):\n",
    "            for key, value in data.items():\n",
    "                if isinstance(value, list):\n",
    "                    print(f\"\\n{key}:\")\n",
    "                    for item in value:\n",
    "                        print(f\"  - {item}\")\n",
    "                elif isinstance(value, dict):\n",
    "                    print(f\"\\n{key}:\")\n",
    "                    for k, v in value.items():\n",
    "                        print(f\"  {k}: {v}\")\n",
    "                else:\n",
    "                    print(f\"{key}: {value}\")\n",
    "\n",
    "await display_formatted_results(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis with Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parameters and analyze business text\n",
    "params = notebook_analyzer.param_adapter.load_and_convert()\n",
    "\n",
    "results = await notebook_analyzer.analyze_text(\n",
    "    \"business\",\n",
    "    analysis_types=[\"keywords\", \"themes\", \"categories\"],\n",
    "    language=\"en\",\n",
    "    parameter_file=params\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finnish Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Finnish text\n",
    "results = await notebook_analyzer.analyze_text(\n",
    "    \"finnish\",\n",
    "    analysis_types=[\"keywords\", \"categories\"],\n",
    "    language=\"fi\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze all texts\n",
    "all_results = await notebook_analyzer.analyze_all(\n",
    "    analysis_types=[\"keywords\", \"themes\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "saved_path = notebook_analyzer.save_results(results, \"analysis_results\")\n",
    "print(f\"Results saved to: {saved_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Categories Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 19 - Code\n",
    "# Define custom categories\n",
    "categories = {\n",
    "    \"technical\": {\n",
    "        \"description\": \"Technical content\",\n",
    "        \"keywords\": [\"programming\", \"software\", \"technology\"],\n",
    "        \"threshold\": 0.7\n",
    "    },\n",
    "    \"business\": {\n",
    "        \"description\": \"Business content\",\n",
    "        \"keywords\": [\"revenue\", \"growth\", \"financial\"],\n",
    "        \"threshold\": 0.6\n",
    "    }\n",
    "}\n",
    "\n",
    "# Analyze with custom categories\n",
    "results = await notebook_analyzer.analyze_text(\n",
    "    \"technical\",\n",
    "    analysis_types=[\"categories\"],\n",
    "    categories=categories\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semantic-analyzer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
