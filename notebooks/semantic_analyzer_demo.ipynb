{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Text Analyzer Demo Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Setup and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added C:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer to Python path\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import asyncio\n",
    "import logging\n",
    "from typing import Dict, Any, Optional\n",
    "import pandas as pd\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = str(Path().resolve().parent)\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "    print(f\"Added {project_root} to Python path\")\n",
    "\n",
    "# Core components\n",
    "from src.semantic_analyzer.analyzer import SemanticAnalyzer\n",
    "from src.utils.FileUtils.file_utils import FileUtils\n",
    "from src.loaders.parameter_adapter import ParameterAdapter\n",
    "from src.core.config import AnalyzerConfig\n",
    "\n",
    "# Initialize FileUtils and set up logging\n",
    "file_utils = FileUtils()\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Check Results:\n",
      "==================================================\n",
      "\n",
      "Basic Setup:\n",
      "-----------\n",
      "✓ Project root in path\n",
      "✓ Can import src\n",
      "✓ FileUtils initialized\n",
      "✓ .env file loaded\n",
      "\n",
      "Environment Variables:\n",
      "---------------------\n",
      "✓ OPENAI_API_KEY set\n",
      "✓ ANTHROPIC_API_KEY set\n",
      "\n",
      "Project Structure:\n",
      "-----------------\n",
      "✓ Raw data exists\n",
      "✓ Processed data exists\n",
      "✓ Configuration exists\n",
      "✓ Main config.yaml exists\n",
      "\n",
      "==================================================\n",
      "Environment Status: Ready ✓\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def verify_environment():\n",
    "    \"\"\"Verify that the notebook environment is properly configured.\"\"\"\n",
    "    # Load environment variables\n",
    "    from dotenv import load_dotenv\n",
    "    env_path = Path(project_root) / \".env\"\n",
    "    env_loaded = load_dotenv(env_path)\n",
    "\n",
    "    # Required environment variables\n",
    "    required_env_vars = [\n",
    "        'OPENAI_API_KEY',\n",
    "        'ANTHROPIC_API_KEY',\n",
    "    ]\n",
    "\n",
    "    # Basic checks\n",
    "    basic_checks = {\n",
    "        \"Project root in path\": project_root in sys.path,\n",
    "        \"Can import src\": \"src\" in sys.modules,\n",
    "        \"FileUtils initialized\": hasattr(file_utils, \"project_root\"),\n",
    "        \".env file loaded\": env_loaded,\n",
    "    }\n",
    "\n",
    "    # Environment variable checks\n",
    "    env_var_checks = {\n",
    "        f\"{var} set\": os.getenv(var) is not None\n",
    "        for var in required_env_vars\n",
    "    }\n",
    "\n",
    "    # Check for required paths using FileUtils\n",
    "    expected_paths = {\n",
    "        \"Raw data\": file_utils.get_data_path(\"raw\"),\n",
    "        \"Processed data\": file_utils.get_data_path(\"processed\"),\n",
    "        \"Configuration\": file_utils.get_data_path(\"configurations\"),\n",
    "        \"Main config.yaml\": Path(project_root) / \"config.yaml\"\n",
    "    }\n",
    "    \n",
    "    path_checks = {\n",
    "        f\"{name} exists\": path.exists()\n",
    "        for name, path in expected_paths.items()\n",
    "    }\n",
    "\n",
    "    # Combine all checks\n",
    "    all_checks = {\n",
    "        **basic_checks,\n",
    "        **env_var_checks,\n",
    "        **path_checks\n",
    "    }\n",
    "    \n",
    "    # Print results\n",
    "    print(\"Environment Check Results:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    def print_section(title, checks):\n",
    "        print(f\"\\n{title}:\")\n",
    "        print(\"-\" * len(title))\n",
    "        for check, result in checks.items():\n",
    "            status = \"✓\" if result else \"✗\"\n",
    "            print(f\"{status} {check}\")\n",
    "    \n",
    "    print_section(\"Basic Setup\", basic_checks)\n",
    "    print_section(\"Environment Variables\", env_var_checks)\n",
    "    print_section(\"Project Structure\", path_checks)\n",
    "    \n",
    "    # Overall status\n",
    "    all_passed = all(all_checks.values())\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Environment Status:\", \"Ready ✓\" if all_passed else \"Setup needed ✗\")\n",
    "    \n",
    "    if not all_passed:\n",
    "        print(\"\\nSetup Instructions:\")\n",
    "        if not env_loaded:\n",
    "            print(\"- Create a .env file in the project root with required API keys\")\n",
    "        for var in required_env_vars:\n",
    "            if not os.getenv(var):\n",
    "                print(f\"- Add {var} to your .env file\")\n",
    "        for name, path in expected_paths.items():\n",
    "            if not path.exists():\n",
    "                print(f\"- Create {name} directory at {path}\")\n",
    "\n",
    "    return all_passed\n",
    "\n",
    "# Run verification\n",
    "verify_environment()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions and Test Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-08 16:01:13 - src.utils.FileUtils.file_utils - INFO - Data saved to c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\raw\\test_texts.xlsx\n",
      "2024-11-08 16:01:13 - src.loaders.parameter_adapter - ERROR - Error loading parameters: Excel file format cannot be determined, you must specify an engine manually.\n",
      "2024-11-08 16:01:13 - src.loaders.parameter_adapter - ERROR - Error loading parameters: Excel file format cannot be determined, you must specify an engine manually.\n"
     ]
    }
   ],
   "source": [
    "class NotebookAnalyzer:\n",
    "    \"\"\"Helper class for running analyses in the notebook.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.file_utils = FileUtils()\n",
    "        self.test_texts = self._load_test_texts()\n",
    "        self.param_adapter = ParameterAdapter(Path(project_root) / \"config.yaml\")\n",
    "    \n",
    "    def _load_test_texts(self) -> Dict[str, str]:\n",
    "        \"\"\"Load or create test texts.\"\"\"\n",
    "        texts = {\n",
    "            \"technical\": \"\"\"\n",
    "            Python is a high-level programming language known for its simplicity.\n",
    "            It supports multiple programming paradigms including procedural and\n",
    "            object-oriented programming.\n",
    "            \"\"\",\n",
    "            \"business\": \"\"\"\n",
    "            The company's Q3 results exceeded expectations with revenue growth of 15%.\n",
    "            Customer acquisition costs decreased while retention rates improved.\n",
    "            The board has approved a new strategic initiative focusing on expansion.\n",
    "            \"\"\",\n",
    "            \"finnish\": \"\"\"\n",
    "            Ohjelmistokehittäjä työskentelee asiakasprojektissa kehittäen\n",
    "            verkkokauppajärjestelmää. Tekninen toteutus vaatii erityistä huomiota\n",
    "            tietoturvan osalta.\n",
    "            \"\"\"\n",
    "        }\n",
    "        \n",
    "        # Save texts using FileUtils for future use\n",
    "        df = pd.DataFrame([\n",
    "            {\"name\": name, \"content\": content.strip()}\n",
    "            for name, content in texts.items()\n",
    "        ])\n",
    "        \n",
    "        saved_files, _ = self.file_utils.save_data_to_disk(\n",
    "            data={\"texts\": df},\n",
    "            output_type=\"raw\",\n",
    "            file_name=\"test_texts\",\n",
    "            output_filetype=\"xlsx\",\n",
    "            include_timestamp=False\n",
    "        )\n",
    "        \n",
    "        return texts\n",
    "    \n",
    "    async def analyze_text(self, text_key: str, **kwargs):\n",
    "        \"\"\"Analyze a specific test text.\"\"\"\n",
    "        if text_key not in self.test_texts:\n",
    "            raise ValueError(f\"Unknown text key: {text_key}. Available keys: {list(self.test_texts.keys())}\")\n",
    "        \n",
    "        analyzer = SemanticAnalyzer(**kwargs)\n",
    "        results = await analyzer.analyze(self.test_texts[text_key], **kwargs)\n",
    "        await self.display_results(results)\n",
    "        return results\n",
    "    \n",
    "    async def analyze_all(self, **kwargs):\n",
    "        \"\"\"Analyze all test texts.\"\"\"\n",
    "        analyzer = SemanticAnalyzer(**kwargs)\n",
    "        all_results = await analyzer.analyze_batch(\n",
    "            list(self.test_texts.values()),\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        for i, results in enumerate(all_results):\n",
    "            print(f\"\\n=== Text {i+1} ===\")\n",
    "            await self.display_results(results)\n",
    "        \n",
    "        return all_results\n",
    "    \n",
    "    @staticmethod\n",
    "    async def display_results(results: Dict[str, Any]) -> None:\n",
    "        \"\"\"Display analysis results in a formatted way.\"\"\"\n",
    "        for analysis_type, data in results.items():\n",
    "            print(f\"\\n=== {analysis_type.upper()} ===\")\n",
    "            if isinstance(data, dict):\n",
    "                for key, value in data.items():\n",
    "                    print(f\"{key}: {value}\")\n",
    "            else:\n",
    "                print(data)\n",
    "    \n",
    "    def save_results(self, results: Dict[str, Any], filename: str) -> Path:\n",
    "        \"\"\"Save analysis results using FileUtils.\"\"\"\n",
    "        saved_files, _ = self.file_utils.save_data_to_disk(\n",
    "            data=results,\n",
    "            output_type=\"processed\",\n",
    "            file_name=filename,\n",
    "            output_filetype=\"yaml\"\n",
    "        )\n",
    "        return Path(list(saved_files.values())[0])\n",
    "\n",
    "# Initialize analyzer\n",
    "notebook_analyzer = NotebookAnalyzer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Analysis Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-08 16:01:13 - src.core.language_processing.factory - INFO - Using default configuration\n",
      "2024-11-08 16:01:13 - src.core.language_processing.base - INFO - Loaded 4 stop words from c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\configurations\\stop_words\\en.txt\n",
      "2024-11-08 16:01:14 - src.semantic_analyzer.analyzer - INFO - Initialized SemanticAnalyzer with language: en, using parameter file: None\n",
      "2024-11-08 16:01:16 - src.semantic_analyzer.analyzer - INFO - Initialized SemanticAnalyzer with language: en, using parameter file: None\n",
      "2024-11-08 16:01:26 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-08 16:01:30 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== KEYWORDS ===\n",
      "keywords: ['python', 'high-level', 'program', 'language', 'know', 'for', 'simplicity', 'support', 'multiple', 'paradigm']\n",
      "keyword_scores: {'python': 0.18754640123200894, 'high-level': 0.6375464012320089, 'program': 0.4, 'language': 0.6675464012320089, 'know': 0.18754640123200894, 'for': 0.18754640123200894, 'simplicity': 0.6075464012320089, 'support': 0.18754640123200894, 'multiple': 0.18754640123200894, 'paradigm': 0.18754640123200894, 'include': 0.18754640123200894, 'procedural': 0.577546401232009, 'and': 0.18754640123200894, 'object-oriented': 0.5475464012320089, 'Python': 0.54, 'programming': 0.51}\n",
      "compound_words: ['high-level', 'object-oriented']\n",
      "domain_keywords: {'programming': ['Python', 'programming', 'procedural', 'object-oriented'], 'languages': ['language', 'high-level']}\n",
      "success: True\n",
      "language: en\n",
      "\n",
      "=== THEMES ===\n",
      "error: None\n",
      "success: False\n",
      "language: en\n"
     ]
    }
   ],
   "source": [
    "# Analyze technical text\n",
    "results = await notebook_analyzer.analyze_text(\n",
    "    \"technical\",\n",
    "    analysis_types=[\"keywords\", \"themes\"],\n",
    "    language=\"en\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis with Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-08 16:01:32 - src.loaders.parameter_adapter - ERROR - Error loading parameters: Excel file format cannot be determined, you must specify an engine manually.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not ParameterSet",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load parameters and analyze business text\u001b[39;00m\n\u001b[0;32m      2\u001b[0m params \u001b[38;5;241m=\u001b[39m notebook_analyzer\u001b[38;5;241m.\u001b[39mparam_adapter\u001b[38;5;241m.\u001b[39mload_and_convert()\n\u001b[1;32m----> 4\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m notebook_analyzer\u001b[38;5;241m.\u001b[39manalyze_text(\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbusiness\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m     analysis_types\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeywords\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthemes\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategories\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m      7\u001b[0m     language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      8\u001b[0m     parameter_file\u001b[38;5;241m=\u001b[39mparams\n\u001b[0;32m      9\u001b[0m )\n",
      "Cell \u001b[1;32mIn[3], line 50\u001b[0m, in \u001b[0;36mNotebookAnalyzer.analyze_text\u001b[1;34m(self, text_key, **kwargs)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_texts:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown text key: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Available keys: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_texts\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 50\u001b[0m analyzer \u001b[38;5;241m=\u001b[39m SemanticAnalyzer(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     51\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m analyzer\u001b[38;5;241m.\u001b[39manalyze(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_texts[text_key], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplay_results(results)\n",
      "File \u001b[1;32m~\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\src\\semantic_analyzer\\analyzer.py:39\u001b[0m, in \u001b[0;36mSemanticAnalyzer.__init__\u001b[1;34m(self, config, file_utils, llm, language, categories, parameter_file, **kwargs)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Initialize semantic analyzer.\"\"\"\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Load parameters first\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameter_adapter \u001b[38;5;241m=\u001b[39m \u001b[43mParameterAdapter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparameter_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameter_adapter\u001b[38;5;241m.\u001b[39mload_and_convert()\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Initialize core components\u001b[39;00m\n",
      "File \u001b[1;32m~\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\src\\loaders\\parameter_adapter.py:123\u001b[0m, in \u001b[0;36mParameterAdapter.__init__\u001b[1;34m(self, file_path)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, file_path: Optional[Union[\u001b[38;5;28mstr\u001b[39m, Path]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    122\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Initialize adapter with parameter file path.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path \u001b[38;5;241m=\u001b[39m \u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m file_path \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_and_convert()\n\u001b[0;32m    125\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitialized ParameterAdapter with file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\tja\\AppData\\Local\\miniconda3\\envs\\semantic-analyzer\\lib\\pathlib.py:1082\u001b[0m, in \u001b[0;36mPath.__new__\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m Path:\n\u001b[0;32m   1081\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m WindowsPath \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnt\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m PosixPath\n\u001b[1;32m-> 1082\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_parts\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1083\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flavour\u001b[38;5;241m.\u001b[39mis_supported:\n\u001b[0;32m   1084\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot instantiate \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m on your system\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1085\u001b[0m                               \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,))\n",
      "File \u001b[1;32mc:\\Users\\tja\\AppData\\Local\\miniconda3\\envs\\semantic-analyzer\\lib\\pathlib.py:707\u001b[0m, in \u001b[0;36mPurePath._from_parts\u001b[1;34m(cls, args, init)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_from_parts\u001b[39m(\u001b[38;5;28mcls\u001b[39m, args, init\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;66;03m# We need to call _parse_args on the instance, so as to get the\u001b[39;00m\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;66;03m# right flavour.\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m--> 707\u001b[0m     drv, root, parts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    708\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_drv \u001b[38;5;241m=\u001b[39m drv\n\u001b[0;32m    709\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_root \u001b[38;5;241m=\u001b[39m root\n",
      "File \u001b[1;32mc:\\Users\\tja\\AppData\\Local\\miniconda3\\envs\\semantic-analyzer\\lib\\pathlib.py:691\u001b[0m, in \u001b[0;36mPurePath._parse_args\u001b[1;34m(cls, args)\u001b[0m\n\u001b[0;32m    689\u001b[0m     parts \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39m_parts\n\u001b[0;32m    690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 691\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    692\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    693\u001b[0m         \u001b[38;5;66;03m# Force-cast str subclasses to str (issue #21127)\u001b[39;00m\n\u001b[0;32m    694\u001b[0m         parts\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mstr\u001b[39m(a))\n",
      "\u001b[1;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not ParameterSet"
     ]
    }
   ],
   "source": [
    "# Load parameters and analyze business text\n",
    "params = notebook_analyzer.param_adapter.load_and_convert()\n",
    "\n",
    "results = await notebook_analyzer.analyze_text(\n",
    "    \"business\",\n",
    "    analysis_types=[\"keywords\", \"themes\", \"categories\"],\n",
    "    language=\"en\",\n",
    "    parameter_file=params\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finnish Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Finnish text\n",
    "results = await notebook_analyzer.analyze_text(\n",
    "    \"finnish\",\n",
    "    analysis_types=[\"keywords\", \"categories\"],\n",
    "    language=\"fi\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze all texts\n",
    "all_results = await notebook_analyzer.analyze_all(\n",
    "    analysis_types=[\"keywords\", \"themes\"]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "saved_path = notebook_analyzer.save_results(results, \"analysis_results\")\n",
    "print(f\"Results saved to: {saved_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Categories Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 19 - Code\n",
    "# Define custom categories\n",
    "categories = {\n",
    "    \"technical\": {\n",
    "        \"description\": \"Technical content\",\n",
    "        \"keywords\": [\"programming\", \"software\", \"technology\"],\n",
    "        \"threshold\": 0.7\n",
    "    },\n",
    "    \"business\": {\n",
    "        \"description\": \"Business content\",\n",
    "        \"keywords\": [\"revenue\", \"growth\", \"financial\"],\n",
    "        \"threshold\": 0.6\n",
    "    }\n",
    "}\n",
    "\n",
    "# Analyze with custom categories\n",
    "results = await notebook_analyzer.analyze_text(\n",
    "    \"technical\",\n",
    "    analysis_types=[\"categories\"],\n",
    "    categories=categories\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semantic-analyzer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
