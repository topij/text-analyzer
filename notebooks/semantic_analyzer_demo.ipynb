{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Text Analyzer Demo Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Setup and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added C:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer to Python path\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import asyncio\n",
    "import logging\n",
    "from typing import Dict, Any, Optional\n",
    "import pandas as pd\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = str(Path().resolve().parent)\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "    print(f\"Added {project_root} to Python path\")\n",
    "\n",
    "# Core components\n",
    "from src.semantic_analyzer.analyzer import SemanticAnalyzer\n",
    "from src.utils.FileUtils.file_utils import FileUtils\n",
    "from src.loaders.parameter_adapter import ParameterAdapter\n",
    "from src.core.config import AnalyzerConfig\n",
    "from src.loaders.models import ParameterSet\n",
    "\n",
    "# Initialize FileUtils and set up logging\n",
    "file_utils = FileUtils()\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Check Results:\n",
      "==================================================\n",
      "\n",
      "Basic Setup:\n",
      "-----------\n",
      "✓ Project root in path\n",
      "✓ Can import src\n",
      "✓ FileUtils initialized\n",
      "✓ .env file loaded\n",
      "\n",
      "Environment Variables:\n",
      "---------------------\n",
      "✓ OPENAI_API_KEY set\n",
      "✓ ANTHROPIC_API_KEY set\n",
      "\n",
      "Project Structure:\n",
      "-----------------\n",
      "✓ Raw data exists\n",
      "✓ Processed data exists\n",
      "✓ Configuration exists\n",
      "✓ Main config.yaml exists\n",
      "\n",
      "==================================================\n",
      "Environment Status: Ready ✓\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def verify_environment():\n",
    "    \"\"\"Verify that the notebook environment is properly configured.\"\"\"\n",
    "    # Load environment variables\n",
    "    from dotenv import load_dotenv\n",
    "    env_path = Path(project_root) / \".env\"\n",
    "    env_loaded = load_dotenv(env_path)\n",
    "\n",
    "    # Required environment variables\n",
    "    required_env_vars = [\n",
    "        'OPENAI_API_KEY',\n",
    "        'ANTHROPIC_API_KEY',\n",
    "    ]\n",
    "\n",
    "    # Basic checks\n",
    "    basic_checks = {\n",
    "        \"Project root in path\": project_root in sys.path,\n",
    "        \"Can import src\": \"src\" in sys.modules,\n",
    "        \"FileUtils initialized\": hasattr(file_utils, \"project_root\"),\n",
    "        \".env file loaded\": env_loaded,\n",
    "    }\n",
    "\n",
    "    # Environment variable checks\n",
    "    env_var_checks = {\n",
    "        f\"{var} set\": os.getenv(var) is not None\n",
    "        for var in required_env_vars\n",
    "    }\n",
    "\n",
    "    # Check for required paths using FileUtils\n",
    "    expected_paths = {\n",
    "        \"Raw data\": file_utils.get_data_path(\"raw\"),\n",
    "        \"Processed data\": file_utils.get_data_path(\"processed\"),\n",
    "        \"Configuration\": file_utils.get_data_path(\"configurations\"),\n",
    "        \"Main config.yaml\": Path(project_root) / \"config.yaml\"\n",
    "    }\n",
    "    \n",
    "    path_checks = {\n",
    "        f\"{name} exists\": path.exists()\n",
    "        for name, path in expected_paths.items()\n",
    "    }\n",
    "\n",
    "    # Combine all checks\n",
    "    all_checks = {\n",
    "        **basic_checks,\n",
    "        **env_var_checks,\n",
    "        **path_checks\n",
    "    }\n",
    "    \n",
    "    # Print results\n",
    "    print(\"Environment Check Results:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    def print_section(title, checks):\n",
    "        print(f\"\\n{title}:\")\n",
    "        print(\"-\" * len(title))\n",
    "        for check, result in checks.items():\n",
    "            status = \"✓\" if result else \"✗\"\n",
    "            print(f\"{status} {check}\")\n",
    "    \n",
    "    print_section(\"Basic Setup\", basic_checks)\n",
    "    print_section(\"Environment Variables\", env_var_checks)\n",
    "    print_section(\"Project Structure\", path_checks)\n",
    "    \n",
    "    # Overall status\n",
    "    all_passed = all(all_checks.values())\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Environment Status:\", \"Ready ✓\" if all_passed else \"Setup needed ✗\")\n",
    "    \n",
    "    if not all_passed:\n",
    "        print(\"\\nSetup Instructions:\")\n",
    "        if not env_loaded:\n",
    "            print(\"- Create a .env file in the project root with required API keys\")\n",
    "        for var in required_env_vars:\n",
    "            if not os.getenv(var):\n",
    "                print(f\"- Add {var} to your .env file\")\n",
    "        for name, path in expected_paths.items():\n",
    "            if not path.exists():\n",
    "                print(f\"- Create {name} directory at {path}\")\n",
    "\n",
    "    return all_passed\n",
    "\n",
    "# Run verification\n",
    "verify_environment()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions and Test Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotebookAnalyzer:\n",
    "    \"\"\"Helper class for running analyses in the notebook.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.file_utils = FileUtils()\n",
    "        self.test_texts = self._load_test_texts()\n",
    "        \n",
    "        # Use parameter file from configurations directory\n",
    "        self.parameter_file = self.file_utils.get_data_path(\"configurations\") / \"example_params.xlsx\"\n",
    "        \n",
    "        # Create the parameter file if it doesn't exist\n",
    "        if not self.parameter_file.exists():\n",
    "            self._create_parameter_file()\n",
    "        \n",
    "        self.param_adapter = ParameterAdapter(self.parameter_file)\n",
    "        \n",
    "        logger.info(f\"Initialized NotebookAnalyzer with {len(self.test_texts)} test texts\")\n",
    "    \n",
    "    def _load_test_texts(self) -> Dict[str, str]:\n",
    "        \"\"\"Load or create test texts.\"\"\"\n",
    "        texts = {\n",
    "            \"technical\": \"\"\"\n",
    "            Python is a high-level programming language known for its simplicity.\n",
    "            It supports multiple programming paradigms including procedural and\n",
    "            object-oriented programming.\n",
    "            \"\"\",\n",
    "            \"business\": \"\"\"\n",
    "            The company's Q3 results exceeded expectations with revenue growth of 15%.\n",
    "            Customer acquisition costs decreased while retention rates improved.\n",
    "            The board has approved a new strategic initiative focusing on expansion.\n",
    "            \"\"\",\n",
    "            \"finnish\": \"\"\"\n",
    "            Ohjelmistokehittäjä työskentelee asiakasprojektissa kehittäen\n",
    "            verkkokauppajärjestelmää. Tekninen toteutus vaatii erityistä huomiota\n",
    "            tietoturvan osalta.\n",
    "            \"\"\"\n",
    "        }\n",
    "        \n",
    "        # Save texts using FileUtils\n",
    "        df = pd.DataFrame([\n",
    "            {\"name\": name, \"content\": content.strip()}\n",
    "            for name, content in texts.items()\n",
    "        ])\n",
    "        \n",
    "        self.file_utils.save_data_to_disk(\n",
    "            data={\"texts\": df},\n",
    "            output_type=\"raw\",\n",
    "            file_name=\"test_texts\",\n",
    "            output_filetype=\"xlsx\",\n",
    "            include_timestamp=False\n",
    "        )\n",
    "        \n",
    "        return texts\n",
    "\n",
    "    def _create_parameter_file(self):\n",
    "        \"\"\"Create initial parameter Excel file.\"\"\"\n",
    "        parameters_data = {\n",
    "            'General Parameters': pd.DataFrame({\n",
    "                'parameter': [\n",
    "                    'max_kws',\n",
    "                    'max_themes',\n",
    "                    'focus_on',\n",
    "                    'language',\n",
    "                    'additional_context',\n",
    "                    'min_confidence',\n",
    "                    'include_compounds',\n",
    "                    'column_name_to_analyze'\n",
    "                ],\n",
    "                'value': [\n",
    "                    8,\n",
    "                    3,\n",
    "                    'business and financial content',\n",
    "                    'en',\n",
    "                    'Business performance analysis',\n",
    "                    0.3,\n",
    "                    True,\n",
    "                    'text'\n",
    "                ]\n",
    "            }),\n",
    "            'Excluded Keywords': pd.DataFrame({\n",
    "                'keyword': [\n",
    "                    'the', 'with', 'while', 'new',\n",
    "                    'company', 'business', 'results',\n",
    "                    'approximately', 'significantly',\n",
    "                    'current', 'various', 'multiple'\n",
    "                ],\n",
    "                'reason': [\n",
    "                    'Common word', 'Common word', 'Common word', 'Common word',\n",
    "                    'Too generic', 'Too generic', 'Too generic',\n",
    "                    'Modifier', 'Modifier',\n",
    "                    'Vague term', 'Vague term', 'Vague term'\n",
    "                ]\n",
    "            }),\n",
    "            'Categories': pd.DataFrame({\n",
    "                'category': [\n",
    "                    'business_performance',\n",
    "                    'financial_metrics',\n",
    "                    'strategy',\n",
    "                    'operations'\n",
    "                ],\n",
    "                'description': [\n",
    "                    'Business performance indicators and results',\n",
    "                    'Financial and revenue related metrics',\n",
    "                    'Strategic initiatives and planning',\n",
    "                    'Operational metrics and processes'\n",
    "                ],\n",
    "                'keywords': [\n",
    "                    'revenue,growth,performance,results,expectations',\n",
    "                    'costs,revenue,profit,margin,acquisition',\n",
    "                    'initiative,strategic,expansion,planning,board',\n",
    "                    'operations,efficiency,retention,improvement,process'\n",
    "                ],\n",
    "                'threshold': [0.6, 0.6, 0.6, 0.6]\n",
    "            })\n",
    "        }\n",
    "        \n",
    "        self.file_utils.save_data_to_disk(\n",
    "            data=parameters_data,\n",
    "            output_type=\"configurations\",\n",
    "            file_name=\"example_params\",\n",
    "            output_filetype=\"xlsx\",\n",
    "            include_timestamp=False\n",
    "        )\n",
    "\n",
    "    async def analyze_text(self, text_key: str, **kwargs):\n",
    "        \"\"\"Analyze a specific text sample.\"\"\"\n",
    "        if text_key not in self.test_texts:\n",
    "            raise ValueError(f\"Unknown text key: {text_key}. Available keys: {list(self.test_texts.keys())}\")\n",
    "        \n",
    "        # Load parameters\n",
    "        params = self.param_adapter.load_and_convert()\n",
    "        \n",
    "        # Get excluded keywords from parameter file\n",
    "        excluded_keywords = set()\n",
    "        try:\n",
    "            excluded_df = pd.read_excel(self.parameter_file, sheet_name='Excluded Keywords')\n",
    "            excluded_keywords = set(excluded_df['keyword'].dropna())\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not load excluded keywords: {e}\")\n",
    "        \n",
    "        # Create analyzer with explicit config\n",
    "        analyzer = SemanticAnalyzer(\n",
    "            parameter_file=self.parameter_file,\n",
    "            config={\n",
    "                \"excluded_keywords\": excluded_keywords,\n",
    "                \"min_keyword_length\": 3,\n",
    "                **kwargs.get('config', {})\n",
    "            },\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        # Run analysis\n",
    "        results = await analyzer.analyze(self.test_texts[text_key], **kwargs)\n",
    "        await self.display_results(results)\n",
    "        return results\n",
    "            \n",
    "    \n",
    "    @staticmethod\n",
    "    async def display_results(results: Dict[str, Any]) -> None:\n",
    "        \"\"\"Display analysis results in a formatted way.\"\"\"\n",
    "        for analysis_type, data in results.items():\n",
    "            print(f\"\\n{'='*20} {analysis_type.upper()} {'='*20}\")\n",
    "            if isinstance(data, dict):\n",
    "                for key, value in data.items():\n",
    "                    if isinstance(value, (list, dict)):\n",
    "                        print(f\"\\n{key}:\")\n",
    "                        print(value)\n",
    "                    else:\n",
    "                        print(f\"{key}: {value}\")\n",
    "            else:\n",
    "                print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-08 20:20:23 - src.utils.FileUtils.file_utils - INFO - Data saved to c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\raw\\test_texts.xlsx\n",
      "2024-11-08 20:20:23 - src.utils.FileUtils.file_utils - INFO - Successfully loaded 4 sheets from c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\configurations\\example_params.xlsx\n",
      "2024-11-08 20:20:23 - __main__ - INFO - Initialized NotebookAnalyzer with 3 test texts\n"
     ]
    }
   ],
   "source": [
    "# Initialize analyzer\n",
    "notebook_analyzer = NotebookAnalyzer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Setup Verification ===\n",
      "\n",
      "Available test texts:\n",
      "\n",
      "technical:\n",
      "\n",
      "            Python is a high-level programming language known for its simplicity.\n",
      "            It su...\n",
      "\n",
      "business:\n",
      "\n",
      "            The company's Q3 results exceeded expectations with revenue growth of 15%.\n",
      "            ...\n",
      "\n",
      "finnish:\n",
      "\n",
      "            Ohjelmistokehittäjä työskentelee asiakasprojektissa kehittäen\n",
      "            verkkokauppaj...\n",
      "\n",
      "=== Parameter File ===\n",
      "Location: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\configurations\\example_params.xlsx\n",
      "Parameters loaded: True\n"
     ]
    }
   ],
   "source": [
    "async def verify_setup():\n",
    "    \"\"\"Verify analyzer setup and available texts.\"\"\"\n",
    "    print(\"=== Setup Verification ===\")\n",
    "    print(\"\\nAvailable test texts:\")\n",
    "    for name, text in notebook_analyzer.test_texts.items():\n",
    "        print(f\"\\n{name}:\")\n",
    "        print(text[:100] + \"...\")  # Show first 100 chars\n",
    "    \n",
    "    print(\"\\n=== Parameter File ===\")\n",
    "    print(f\"Location: {notebook_analyzer.parameter_file}\")\n",
    "    print(\"Parameters loaded:\", notebook_analyzer.param_adapter is not None)\n",
    "\n",
    "# Run verification\n",
    "await verify_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify parameter loading and stopword handling\n",
    "async def verify_parameters():\n",
    "    \"\"\"Verify parameter loading and stopword handling.\"\"\"\n",
    "    # Load parameters\n",
    "    params = notebook_analyzer.param_adapter.load_and_convert()\n",
    "    \n",
    "    print(\"=== Parameter Verification ===\")\n",
    "    print(\"\\nExcluded Keywords from parameters:\")\n",
    "    if hasattr(params, 'excluded_keywords'):\n",
    "        print(sorted(list(params.excluded_keywords)))\n",
    "    else:\n",
    "        print(\"No excluded keywords in parameters\")\n",
    "    \n",
    "    # Create analyzer with parameters\n",
    "    analyzer = SemanticAnalyzer(\n",
    "        parameter_file=notebook_analyzer.parameter_file,\n",
    "        language=\"en\"\n",
    "    )\n",
    "    \n",
    "    print(\"\\n=== Language Processing Setup ===\")\n",
    "    print(f\"Language: {analyzer.text_processor.language}\")\n",
    "    print(f\"\\nStopwords (NLTK + custom):\")\n",
    "    print(f\"Total stopwords: {len(analyzer.text_processor._stop_words)}\")\n",
    "    print(\"\\nSample stopwords (first 20):\")\n",
    "    print(sorted(list(analyzer.text_processor._stop_words))[:20])\n",
    "    \n",
    "    print(f\"\\nExcluded keywords from config:\")\n",
    "    print(sorted(list(analyzer.text_processor.excluded_keywords)))\n",
    "    \n",
    "    return analyzer\n",
    "\n",
    "# Run verification\n",
    "# analyzer = await verify_parameters()\n",
    "\n",
    "# Test analysis with verified setup\n",
    "# results = await analyzer.analyze(\n",
    "#     notebook_analyzer.test_texts['business'],\n",
    "#     analysis_types=[\"keywords\", \"themes\", \"categories\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-08 20:20:44 - src.utils.FileUtils.file_utils - INFO - Successfully loaded 4 sheets from c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\configurations\\example_params.xlsx\n",
      "2024-11-08 20:20:44 - src.utils.FileUtils.file_utils - INFO - Successfully loaded 4 sheets from c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\configurations\\example_params.xlsx\n",
      "2024-11-08 20:20:44 - src.core.language_processing.factory - INFO - Using default configuration\n",
      "2024-11-08 20:20:46 - src.semantic_analyzer.analyzer - INFO - Initialized SemanticAnalyzer with language: en, using parameter file: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\configurations\\example_params.xlsx\n",
      "2024-11-08 20:20:47 - src.semantic_analyzer.analyzer - INFO - Initialized SemanticAnalyzer with language: en, using parameter file: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\configurations\\example_params.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Keyword Extraction Test ===\n",
      "\n",
      "Original text:\n",
      "\n",
      "            The company's Q3 results exceeded expectations with revenue growth of 15%.\n",
      "            Customer acquisition costs decreased while retention rates improved.\n",
      "            The board has approved a new strategic initiative focusing on expansion.\n",
      "            \n",
      "\n",
      "Tokens after stopword removal:\n",
      "['company', \"'s\", 'Q3', 'results', 'exceeded', 'expectations', 'revenue', 'growth', '15', '%', '.', 'Customer', 'acquisition', 'costs', 'decreased', 'retention', 'rates', 'improved', '.', 'board', 'approved', 'new', 'strategic', 'initiative', 'focusing', 'expansion', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-08 20:20:56 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Keywords:\n",
      "\n",
      "Keywords: ['company', 'result', 'exceed', 'expectation', 'revenue', 'growth', 'customer', 'acquisition']\n",
      "\n",
      "Compound words: ['customer+acquisition', 'retention+rates', 'strategic+initiative']\n",
      "\n",
      "Domain keywords: {'business': ['Q3', 'results', 'revenue', 'growth', 'acquisition', 'retention', 'strategic', 'initiative']}\n"
     ]
    }
   ],
   "source": [
    "async def test_keyword_extraction():\n",
    "    \"\"\"Test keyword extraction with full debug output.\"\"\"\n",
    "    text_key = \"business\"\n",
    "    text = notebook_analyzer.test_texts[text_key]\n",
    "    \n",
    "    # Create analyzer with explicit config\n",
    "    excluded_df = pd.read_excel(notebook_analyzer.parameter_file, sheet_name='Excluded Keywords')\n",
    "    excluded_keywords = set(excluded_df['keyword'].dropna())\n",
    "    \n",
    "    analyzer = SemanticAnalyzer(\n",
    "        parameter_file=notebook_analyzer.parameter_file,\n",
    "        language=\"en\",\n",
    "        config={\n",
    "            \"excluded_keywords\": excluded_keywords,\n",
    "            \"min_keyword_length\": 3,\n",
    "            \"max_keywords\": 8,\n",
    "            \"focus\": \"business metrics and performance\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(\"=== Keyword Extraction Test ===\")\n",
    "    print(\"\\nOriginal text:\")\n",
    "    print(text)\n",
    "    \n",
    "    # Show processing steps\n",
    "    tokens = analyzer.text_processor.tokenize(text)\n",
    "    print(\"\\nTokens after stopword removal:\")\n",
    "    filtered = [word for word in tokens \n",
    "               if not analyzer.text_processor.should_exclude_word(word)]\n",
    "    print(filtered)\n",
    "    \n",
    "    # Run analysis\n",
    "    results = await analyzer.analyze(\n",
    "        text, \n",
    "        analysis_types=[\"keywords\"]\n",
    "    )\n",
    "    \n",
    "    print(\"\\nExtracted Keywords:\")\n",
    "    if \"keywords\" in results:\n",
    "        keywords = results[\"keywords\"]\n",
    "        print(\"\\nKeywords:\", keywords.get(\"keywords\", []))\n",
    "        print(\"\\nCompound words:\", keywords.get(\"compound_words\", []))\n",
    "        print(\"\\nDomain keywords:\", keywords.get(\"domain_keywords\", {}))\n",
    "\n",
    "# Run the test\n",
    "await test_keyword_extraction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-08 20:21:13 - src.utils.FileUtils.file_utils - INFO - Successfully loaded 4 sheets from c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\configurations\\example_params.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-08 20:21:13 - src.utils.FileUtils.file_utils - INFO - Successfully loaded 4 sheets from c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\configurations\\example_params.xlsx\n",
      "2024-11-08 20:21:13 - src.utils.FileUtils.file_utils - INFO - Successfully loaded 4 sheets from c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\configurations\\example_params.xlsx\n",
      "2024-11-08 20:21:13 - src.core.language_processing.factory - INFO - Using default configuration\n",
      "2024-11-08 20:21:15 - src.semantic_analyzer.analyzer - INFO - Initialized SemanticAnalyzer with language: en, using parameter file: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\configurations\\example_params.xlsx\n",
      "2024-11-08 20:21:16 - src.semantic_analyzer.analyzer - INFO - Initialized SemanticAnalyzer with language: en, using parameter file: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\configurations\\example_params.xlsx\n",
      "2024-11-08 20:21:19 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-08 20:21:25 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-08 20:21:26 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== KEYWORDS ====================\n",
      "\n",
      "keywords:\n",
      "['company', 'result', 'exceed', 'expectation', 'revenue', 'growth', 'customer', 'acquisition']\n",
      "\n",
      "keyword_scores:\n",
      "{'company': 0.4, 'result': 0.4, 'exceed': 0.4, 'expectation': 0.4, 'revenue': 0.9400000000000001, 'growth': 0.88, 'customer': 0.4, 'acquisition': 0.8200000000000001, 'cost': 0.4, 'decrease': 0.4, 'retention': 0.8200000000000001, 'rate': 0.4, 'improve': 0.4, 'board': 0.4, 'approve': 0.4, 'new': 0.4, 'strategic': 0.4, 'initiative': 0.4, 'focus': 0.4, 'expansion': 0.4, 'Q3': 0.54, 'results': 0.51, 'exceeded': 0.48, 'expectations': 0.44999999999999996}\n",
      "\n",
      "compound_words:\n",
      "['customer+acquisition', 'retention+rates', 'strategic+initiative']\n",
      "\n",
      "domain_keywords:\n",
      "{'business': ['Q3', 'results', 'revenue', 'growth', 'acquisition', 'retention', 'strategic', 'initiative']}\n",
      "success: True\n",
      "language: en\n",
      "\n",
      "==================== THEMES ====================\n",
      "error: None\n",
      "success: False\n",
      "language: en\n",
      "\n",
      "==================== CATEGORIES ====================\n",
      "\n",
      "categories:\n",
      "[]\n",
      "\n",
      "explanations:\n",
      "{}\n",
      "\n",
      "evidence:\n",
      "{}\n",
      "success: True\n",
      "language: en\n"
     ]
    }
   ],
   "source": [
    "# Cell 4 - Run Analysis\n",
    "# Analyze business text\n",
    "results = await notebook_analyzer.analyze_text(\n",
    "    \"business\",\n",
    "    analysis_types=[\"keywords\", \"themes\", \"categories\"],\n",
    "    language=\"en\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis with explicit stopword settings\n",
    "params = notebook_analyzer.param_adapter.load_and_convert()\n",
    "\n",
    "# Add some business-specific words to exclude\n",
    "additional_config = {\n",
    "    \"excluded_keywords\": [\"the\", \"with\", \"while\", \"new\"],  # Common words we want to exclude\n",
    "}\n",
    "\n",
    "# Run analysis\n",
    "results = await notebook_analyzer.analyze_text(\n",
    "    \"business\",\n",
    "    analysis_types=[\"keywords\", \"themes\", \"categories\"],\n",
    "    language=\"en\",\n",
    "    parameter_file=notebook_analyzer.parameter_file,\n",
    "    config=additional_config  # Add our additional config\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Analysis Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-08 19:47:18 - src.utils.FileUtils.file_utils - INFO - Successfully loaded 4 sheets from c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\configurations\\example_params.xlsx\n",
      "2024-11-08 19:47:18 - src.utils.FileUtils.file_utils - INFO - Successfully loaded 4 sheets from c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\configurations\\example_params.xlsx\n",
      "2024-11-08 19:47:18 - src.utils.FileUtils.file_utils - INFO - Successfully loaded 4 sheets from c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\configurations\\example_params.xlsx\n",
      "2024-11-08 19:47:18 - src.core.language_processing.factory - INFO - Using default configuration\n",
      "2024-11-08 19:47:19 - src.semantic_analyzer.analyzer - INFO - Initialized SemanticAnalyzer with language: en, using parameter file: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\configurations\\example_params.xlsx\n",
      "2024-11-08 19:47:21 - src.semantic_analyzer.analyzer - INFO - Initialized SemanticAnalyzer with language: en, using parameter file: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\configurations\\example_params.xlsx\n",
      "2024-11-08 19:47:31 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-08 19:47:36 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-08 19:47:38 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== KEYWORDS ===\n",
      "keywords: ['the', 'company', 'result', 'exceed', 'expectation', 'with', 'revenue', 'growth']\n",
      "keyword_scores: {'the': 0.4, 'company': 0.23300844255160014, 'result': 0.23300844255160014, 'exceed': 0.23300844255160014, 'expectation': 0.23300844255160014, 'with': 0.23300844255160014, 'revenue': 0.23300844255160014, 'growth': 0.23300844255160014, 'customer': 0.23300844255160014, 'acquisition': 0.23300844255160014, 'cost': 0.23300844255160014, 'decrease': 0.23300844255160014, 'while': 0.23300844255160014, 'retention': 0.23300844255160014, 'rate': 0.23300844255160014, 'improve': 0.23300844255160014, 'board': 0.23300844255160014, 'have': 0.23300844255160014, 'approve': 0.23300844255160014, 'new': 0.23300844255160014, 'strategic': 0.23300844255160014, 'initiative': 0.23300844255160014, 'focus': 0.23300844255160014, 'expansion': 0.6530084425516001, 'Q3 results': 0.54, 'revenue growth': 0.51, 'customer acquisition': 0.48, 'retention rates': 0.44999999999999996, 'strategic initiative': 0.48}\n",
      "compound_words: ['customer acquisition', 'retention rates', 'strategic initiative']\n",
      "domain_keywords: {'business': ['Q3 results', 'revenue growth', 'customer acquisition', 'retention rates', 'strategic initiative', 'expansion']}\n",
      "success: True\n",
      "language: en\n",
      "\n",
      "=== THEMES ===\n",
      "error: None\n",
      "success: False\n",
      "language: en\n",
      "\n",
      "=== CATEGORIES ===\n",
      "categories: []\n",
      "explanations: {}\n",
      "evidence: {}\n",
      "success: True\n",
      "language: en\n"
     ]
    }
   ],
   "source": [
    "# Load parameters and analyze business text\n",
    "params = notebook_analyzer.param_adapter.load_and_convert()\n",
    "\n",
    "# Analyze with additional settings\n",
    "results = await notebook_analyzer.analyze_text(\n",
    "    \"business\",\n",
    "    analysis_types=[\"keywords\", \"themes\", \"categories\"],\n",
    "    language=\"en\",\n",
    "    parameter_file=notebook_analyzer.parameter_file,\n",
    "    min_confidence=0.3,  # Ensure we get broader results\n",
    "    include_compounds=True,  # Explicitly enable compound words\n",
    "    focus_on=\"business and financial content\"  # Set specific focus\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== KEYWORDS ====================\n",
      "\n",
      "keywords:\n",
      "  - the\n",
      "  - company\n",
      "  - result\n",
      "  - exceed\n",
      "  - expectation\n",
      "  - with\n",
      "  - revenue\n",
      "  - growth\n",
      "\n",
      "keyword_scores:\n",
      "  the: 0.4\n",
      "  company: 0.23300844255160014\n",
      "  result: 0.23300844255160014\n",
      "  exceed: 0.23300844255160014\n",
      "  expectation: 0.23300844255160014\n",
      "  with: 0.23300844255160014\n",
      "  revenue: 0.23300844255160014\n",
      "  growth: 0.23300844255160014\n",
      "  customer: 0.23300844255160014\n",
      "  acquisition: 0.23300844255160014\n",
      "  cost: 0.23300844255160014\n",
      "  decrease: 0.23300844255160014\n",
      "  while: 0.23300844255160014\n",
      "  retention: 0.23300844255160014\n",
      "  rate: 0.23300844255160014\n",
      "  improve: 0.23300844255160014\n",
      "  board: 0.23300844255160014\n",
      "  have: 0.23300844255160014\n",
      "  approve: 0.23300844255160014\n",
      "  new: 0.23300844255160014\n",
      "  strategic: 0.23300844255160014\n",
      "  initiative: 0.23300844255160014\n",
      "  focus: 0.23300844255160014\n",
      "  expansion: 0.6530084425516001\n",
      "  Q3 results: 0.54\n",
      "  revenue growth: 0.51\n",
      "  customer acquisition: 0.48\n",
      "  retention rates: 0.44999999999999996\n",
      "  strategic initiative: 0.48\n",
      "\n",
      "compound_words:\n",
      "  - customer acquisition\n",
      "  - retention rates\n",
      "  - strategic initiative\n",
      "\n",
      "domain_keywords:\n",
      "  business: ['Q3 results', 'revenue growth', 'customer acquisition', 'retention rates', 'strategic initiative', 'expansion']\n",
      "success: True\n",
      "language: en\n",
      "\n",
      "==================== THEMES ====================\n",
      "error: None\n",
      "success: False\n",
      "language: en\n",
      "\n",
      "==================== CATEGORIES ====================\n",
      "\n",
      "categories:\n",
      "\n",
      "explanations:\n",
      "\n",
      "evidence:\n",
      "success: True\n",
      "language: en\n"
     ]
    }
   ],
   "source": [
    "# Display results in a more readable format\n",
    "async def display_formatted_results(results):\n",
    "    \"\"\"Display results with better formatting.\"\"\"\n",
    "    for analysis_type, data in results.items():\n",
    "        print(f\"\\n{'='*20} {analysis_type.upper()} {'='*20}\")\n",
    "        if isinstance(data, dict):\n",
    "            for key, value in data.items():\n",
    "                if isinstance(value, list):\n",
    "                    print(f\"\\n{key}:\")\n",
    "                    for item in value:\n",
    "                        print(f\"  - {item}\")\n",
    "                elif isinstance(value, dict):\n",
    "                    print(f\"\\n{key}:\")\n",
    "                    for k, v in value.items():\n",
    "                        print(f\"  {k}: {v}\")\n",
    "                else:\n",
    "                    print(f\"{key}: {value}\")\n",
    "\n",
    "await display_formatted_results(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis with Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-08 19:47:38 - src.utils.FileUtils.file_utils - INFO - Successfully loaded 4 sheets from c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\configurations\\example_params.xlsx\n",
      "2024-11-08 19:47:38 - src.utils.FileUtils.file_utils - INFO - Successfully loaded 4 sheets from c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\configurations\\example_params.xlsx\n",
      "2024-11-08 19:47:38 - src.utils.FileUtils.file_utils - INFO - Successfully loaded 4 sheets from c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\configurations\\example_params.xlsx\n",
      "2024-11-08 19:47:38 - src.core.language_processing.factory - INFO - Using default configuration\n",
      "2024-11-08 19:47:39 - src.semantic_analyzer.analyzer - INFO - Initialized SemanticAnalyzer with language: en, using parameter file: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\configurations\\example_params.xlsx\n",
      "2024-11-08 19:47:41 - src.semantic_analyzer.analyzer - INFO - Initialized SemanticAnalyzer with language: en, using parameter file: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\configurations\\example_params.xlsx\n",
      "2024-11-08 19:47:44 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-08 19:47:48 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-08 19:47:48 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== KEYWORDS ===\n",
      "keywords: ['the', 'company', 'result', 'exceed', 'expectation', 'with', 'revenue', 'growth']\n",
      "keyword_scores: {'the': 0.4, 'company': 0.23300844255160014, 'result': 0.23300844255160014, 'exceed': 0.23300844255160014, 'expectation': 0.23300844255160014, 'with': 0.23300844255160014, 'revenue': 0.23300844255160014, 'growth': 0.23300844255160014, 'customer': 0.23300844255160014, 'acquisition': 0.23300844255160014, 'cost': 0.23300844255160014, 'decrease': 0.23300844255160014, 'while': 0.23300844255160014, 'retention': 0.23300844255160014, 'rate': 0.23300844255160014, 'improve': 0.23300844255160014, 'board': 0.23300844255160014, 'have': 0.23300844255160014, 'approve': 0.23300844255160014, 'new': 0.23300844255160014, 'strategic': 0.23300844255160014, 'initiative': 0.23300844255160014, 'focus': 0.23300844255160014, 'expansion': 0.6530084425516001, 'Q3 results': 0.54, 'revenue growth': 0.51, 'customer acquisition': 0.48, 'retention rates': 0.44999999999999996, 'strategic initiative': 0.48}\n",
      "compound_words: ['customer acquisition', 'retention rates', 'strategic initiative']\n",
      "domain_keywords: {'business': ['Q3 results', 'revenue growth', 'customer acquisition', 'retention rates', 'strategic initiative', 'expansion']}\n",
      "success: True\n",
      "language: en\n",
      "\n",
      "=== THEMES ===\n",
      "error: None\n",
      "success: False\n",
      "language: en\n",
      "\n",
      "=== CATEGORIES ===\n",
      "categories: []\n",
      "explanations: {}\n",
      "evidence: {}\n",
      "success: True\n",
      "language: en\n"
     ]
    }
   ],
   "source": [
    "# Load parameters and analyze business text\n",
    "params = notebook_analyzer.param_adapter.load_and_convert()\n",
    "\n",
    "results = await notebook_analyzer.analyze_text(\n",
    "    \"business\",\n",
    "    analysis_types=[\"keywords\", \"themes\", \"categories\"],\n",
    "    language=\"en\",\n",
    "    parameter_file=params\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finnish Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-08 19:47:49 - src.core.language_processing.factory - INFO - Using default configuration\n",
      "2024-11-08 19:47:49 - src.core.language_processing.finnish - INFO - Successfully initialized Voikko with path: C:/scripts/Voikko\n",
      "2024-11-08 19:47:49 - src.core.language_processing.finnish - INFO - Loaded 847 Finnish stopwords from c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\configurations\\stop_words\\fi.txt\n",
      "2024-11-08 19:47:50 - src.semantic_analyzer.analyzer - INFO - Initialized SemanticAnalyzer with language: fi, using parameter file: None\n",
      "2024-11-08 19:47:51 - src.semantic_analyzer.analyzer - INFO - Initialized SemanticAnalyzer with language: fi, using parameter file: None\n",
      "2024-11-08 19:47:54 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-08 19:47:55 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== KEYWORDS ===\n",
      "keywords: ['ohjelmistokehittäjä', 'työskennellä', 'asiakasprojekti', 'kehittää', 'verkkokauppajärjestelmä', 'tekninen', 'toteutus', 'vaatia', 'erityinen', 'huomio']\n",
      "keyword_scores: {'ohjelmistokehittäjä': 0.9400000000000001, 'työskennellä': 0.88, 'asiakasprojekti': 0.91, 'kehittää': 0.9400000000000001, 'verkkokauppajärjestelmä': 0.97, 'tekninen': 0.4, 'toteutus': 0.8200000000000001, 'vaatia': 0.4, 'erityinen': 0.4, 'huomio': 0.79, 'tietoturpa': 0.4, 'osa': 0.4, 'tietoturva': 0.44999999999999996}\n",
      "compound_words: ['verkkokauppajärjestelmä', 'asiakasprojekti']\n",
      "domain_keywords: {'software_development': ['ohjelmistokehittäjä', 'kehittää', 'toteutus', 'tietoturva']}\n",
      "success: True\n",
      "language: fi\n",
      "\n",
      "=== CATEGORIES ===\n",
      "categories: []\n",
      "explanations: {}\n",
      "evidence: {}\n",
      "success: True\n",
      "language: fi\n"
     ]
    }
   ],
   "source": [
    "# Analyze Finnish text\n",
    "results = await notebook_analyzer.analyze_text(\n",
    "    \"finnish\",\n",
    "    analysis_types=[\"keywords\", \"categories\"],\n",
    "    language=\"fi\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-08 19:47:55 - src.core.language_processing.factory - INFO - Using default configuration\n",
      "2024-11-08 19:47:56 - src.semantic_analyzer.analyzer - INFO - Initialized SemanticAnalyzer with language: en, using parameter file: None\n",
      "2024-11-08 19:47:57 - src.semantic_analyzer.analyzer - INFO - Initialized SemanticAnalyzer with language: en, using parameter file: None\n",
      "2024-11-08 19:48:00 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-08 19:48:01 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-08 19:48:02 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-08 19:48:03 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-08 19:48:03 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-08 19:48:04 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Text 1 ===\n",
      "\n",
      "=== KEYWORDS ===\n",
      "keywords: ['python', 'high-level', 'program', 'language', 'know', 'for', 'simplicity', 'support', 'multiple', 'paradigm']\n",
      "keyword_scores: {'python': 0.18754640123200894, 'high-level': 0.6375464012320089, 'program': 0.4, 'language': 0.6675464012320089, 'know': 0.18754640123200894, 'for': 0.18754640123200894, 'simplicity': 0.6075464012320089, 'support': 0.18754640123200894, 'multiple': 0.18754640123200894, 'paradigm': 0.18754640123200894, 'include': 0.18754640123200894, 'procedural': 0.577546401232009, 'and': 0.18754640123200894, 'object-oriented': 0.5475464012320089, 'Python': 0.54, 'programming': 0.51}\n",
      "compound_words: ['high-level', 'object-oriented']\n",
      "domain_keywords: {'programming': ['Python', 'programming', 'procedural', 'object-oriented'], 'languages': ['language', 'high-level']}\n",
      "success: True\n",
      "language: en\n",
      "\n",
      "=== THEMES ===\n",
      "error: None\n",
      "success: False\n",
      "language: en\n",
      "\n",
      "=== Text 2 ===\n",
      "\n",
      "=== KEYWORDS ===\n",
      "keywords: ['the', 'company', 'result', 'exceed', 'expectation', 'with', 'revenue', 'growth', 'customer', 'acquisition']\n",
      "keyword_scores: {'the': 0.4, 'company': 0.23300844255160014, 'result': 0.23300844255160014, 'exceed': 0.23300844255160014, 'expectation': 0.23300844255160014, 'with': 0.23300844255160014, 'revenue': 0.7730084425516002, 'growth': 0.7430084425516001, 'customer': 0.6830084425516001, 'acquisition': 0.6530084425516001, 'cost': 0.23300844255160014, 'decrease': 0.23300844255160014, 'while': 0.23300844255160014, 'retention': 0.6530084425516001, 'rate': 0.23300844255160014, 'improve': 0.23300844255160014, 'board': 0.23300844255160014, 'have': 0.23300844255160014, 'approve': 0.23300844255160014, 'new': 0.23300844255160014, 'strategic': 0.23300844255160014, 'initiative': 0.23300844255160014, 'focus': 0.23300844255160014, 'expansion': 0.23300844255160014, 'Q3': 0.54, 'results': 0.51, 'exceeded': 0.48, 'expectations': 0.48, 'costs': 0.39}\n",
      "compound_words: ['customer acquisition']\n",
      "domain_keywords: {'business': ['Q3', 'results', 'revenue', 'growth', 'customer acquisition', 'retention']}\n",
      "success: True\n",
      "language: en\n",
      "\n",
      "=== THEMES ===\n",
      "error: None\n",
      "success: False\n",
      "language: en\n",
      "\n",
      "=== Text 3 ===\n",
      "\n",
      "=== KEYWORDS ===\n",
      "keywords: ['ohjelmistokehittäjä', 'työskentelee', 'asiakasprojektissa', 'kehittäen', 'verkkokauppajärjestelmää', 'tekninen', 'toteutus', 'vaatii', 'erityistä', 'huomiota']\n",
      "keyword_scores: {'ohjelmistokehittäjä': 0.9400000000000001, 'työskentelee': 0.88, 'asiakasprojektissa': 0.91, 'kehittäen': 0.88, 'verkkokauppajärjestelmää': 0.9400000000000001, 'tekninen': 0.4, 'toteutus': 0.8200000000000001, 'vaatii': 0.4, 'erityistä': 0.4, 'huomiota': 0.76, 'tietoturvan': 0.4, 'osalta': 0.4, 'tietoturva': 0.44999999999999996}\n",
      "compound_words: ['verkkokauppajärjestelmää']\n",
      "domain_keywords: {'software_development': ['ohjelmistokehittäjä', 'verkkokauppajärjestelmää', 'tietoturva']}\n",
      "success: True\n",
      "language: en\n",
      "\n",
      "=== THEMES ===\n",
      "error: None\n",
      "success: False\n",
      "language: fi\n"
     ]
    }
   ],
   "source": [
    "# Analyze all texts\n",
    "all_results = await notebook_analyzer.analyze_all(\n",
    "    analysis_types=[\"keywords\", \"themes\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-08 19:48:04 - src.utils.FileUtils.file_utils - INFO - Data saved to c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\processed\\analysis_results_20241108_194804.yaml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\processed\\analysis_results_20241108_194804.yaml\n"
     ]
    }
   ],
   "source": [
    "# Save results\n",
    "saved_path = notebook_analyzer.save_results(results, \"analysis_results\")\n",
    "print(f\"Results saved to: {saved_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Categories Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-08 19:48:04 - src.core.language_processing.factory - INFO - Using default configuration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-08 19:48:06 - src.semantic_analyzer.analyzer - INFO - Initialized SemanticAnalyzer with language: en, using parameter file: None\n",
      "2024-11-08 19:48:07 - src.semantic_analyzer.analyzer - INFO - Initialized SemanticAnalyzer with language: en, using parameter file: None\n",
      "2024-11-08 19:48:08 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CATEGORIES ===\n",
      "categories: []\n",
      "explanations: {}\n",
      "evidence: {}\n",
      "success: True\n",
      "language: en\n"
     ]
    }
   ],
   "source": [
    "# Cell 19 - Code\n",
    "# Define custom categories\n",
    "categories = {\n",
    "    \"technical\": {\n",
    "        \"description\": \"Technical content\",\n",
    "        \"keywords\": [\"programming\", \"software\", \"technology\"],\n",
    "        \"threshold\": 0.7\n",
    "    },\n",
    "    \"business\": {\n",
    "        \"description\": \"Business content\",\n",
    "        \"keywords\": [\"revenue\", \"growth\", \"financial\"],\n",
    "        \"threshold\": 0.6\n",
    "    }\n",
    "}\n",
    "\n",
    "# Analyze with custom categories\n",
    "results = await notebook_analyzer.analyze_text(\n",
    "    \"technical\",\n",
    "    analysis_types=[\"categories\"],\n",
    "    categories=categories\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semantic-analyzer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
