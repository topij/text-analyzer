{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added C:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer to Python path\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import asyncio\n",
    "import logging\n",
    "from typing import Dict, Any, List, Tuple\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = str(Path().resolve().parent)\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "    print(f\"Added {project_root} to Python path\")\n",
    "\n",
    "# Core components\n",
    "from src.semantic_analyzer.analyzer import SemanticAnalyzer\n",
    "from src.utils.FileUtils.file_utils import FileUtils\n",
    "from src.analyzers.keyword_analyzer import KeywordAnalyzer\n",
    "from src.core.language_processing import create_text_processor\n",
    "from src.loaders.parameter_adapter import ParameterAdapter\n",
    "\n",
    "# Initialize FileUtils and set up logging\n",
    "file_utils = FileUtils()\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Check Results:\n",
      "==================================================\n",
      "\n",
      "Basic Setup:\n",
      "-----------\n",
      "✓ Project root in path\n",
      "✓ Can import src\n",
      "✓ FileUtils initialized\n",
      "✓ .env file loaded\n",
      "\n",
      "Environment Variables:\n",
      "---------------------\n",
      "✓ OPENAI_API_KEY set\n",
      "✓ ANTHROPIC_API_KEY set\n",
      "\n",
      "Project Structure:\n",
      "-----------------\n",
      "✓ Raw data exists\n",
      "✓ Processed data exists\n",
      "✓ Configuration exists\n",
      "✓ Main config.yaml exists\n",
      "\n",
      "==================================================\n",
      "Environment Status: Ready ✓\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def verify_environment():\n",
    "    \"\"\"Verify that the notebook environment is properly configured.\"\"\"\n",
    "    # Load environment variables\n",
    "    from dotenv import load_dotenv\n",
    "    env_path = Path(project_root) / \".env\"\n",
    "    env_loaded = load_dotenv(env_path)\n",
    "\n",
    "    # Required environment variables\n",
    "    required_env_vars = [\n",
    "        'OPENAI_API_KEY',\n",
    "        'ANTHROPIC_API_KEY',\n",
    "    ]\n",
    "\n",
    "    # Basic checks\n",
    "    basic_checks = {\n",
    "        \"Project root in path\": project_root in sys.path,\n",
    "        \"Can import src\": \"src\" in sys.modules,\n",
    "        \"FileUtils initialized\": hasattr(file_utils, \"project_root\"),\n",
    "        \".env file loaded\": env_loaded,\n",
    "    }\n",
    "\n",
    "    # Environment variable checks\n",
    "    env_var_checks = {\n",
    "        f\"{var} set\": os.getenv(var) is not None\n",
    "        for var in required_env_vars\n",
    "    }\n",
    "\n",
    "    # Check for required paths using FileUtils\n",
    "    expected_paths = {\n",
    "        \"Raw data\": file_utils.get_data_path(\"raw\"),\n",
    "        \"Processed data\": file_utils.get_data_path(\"processed\"),\n",
    "        \"Configuration\": file_utils.get_data_path(\"configurations\"),\n",
    "        \"Main config.yaml\": Path(project_root) / \"config.yaml\"\n",
    "    }\n",
    "    \n",
    "    path_checks = {\n",
    "        f\"{name} exists\": path.exists()\n",
    "        for name, path in expected_paths.items()\n",
    "    }\n",
    "\n",
    "    # Combine all checks\n",
    "    all_checks = {\n",
    "        **basic_checks,\n",
    "        **env_var_checks,\n",
    "        **path_checks\n",
    "    }\n",
    "    \n",
    "    # Print results\n",
    "    print(\"Environment Check Results:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    def print_section(title, checks):\n",
    "        print(f\"\\n{title}:\")\n",
    "        print(\"-\" * len(title))\n",
    "        for check, result in checks.items():\n",
    "            status = \"✓\" if result else \"✗\"\n",
    "            print(f\"{status} {check}\")\n",
    "    \n",
    "    print_section(\"Basic Setup\", basic_checks)\n",
    "    print_section(\"Environment Variables\", env_var_checks)\n",
    "    print_section(\"Project Structure\", path_checks)\n",
    "    \n",
    "    # Overall status\n",
    "    all_passed = all(all_checks.values())\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Environment Status:\", \"Ready ✓\" if all_passed else \"Setup needed ✗\")\n",
    "    \n",
    "    if not all_passed:\n",
    "        print(\"\\nSetup Instructions:\")\n",
    "        if not env_loaded:\n",
    "            print(\"- Create a .env file in the project root with required API keys\")\n",
    "        for var in required_env_vars:\n",
    "            if not os.getenv(var):\n",
    "                print(f\"- Add {var} to your .env file\")\n",
    "        for name, path in expected_paths.items():\n",
    "            if not path.exists():\n",
    "                print(f\"- Create {name} directory at {path}\")\n",
    "\n",
    "    return all_passed\n",
    "\n",
    "# Run verification\n",
    "verify_environment()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Tester classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeywordTester:\n",
    "    \"\"\"Helper class for testing keyword analysis components.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.file_utils = FileUtils()\n",
    "        self.test_texts = self._load_test_texts()\n",
    "        \n",
    "    def _load_test_texts(self) -> Dict[str, str]:\n",
    "        \"\"\"Load or create test texts.\"\"\"\n",
    "        texts = {\n",
    "            \"technical\": \"\"\"\n",
    "                Python is a high-level programming language known for its simplicity.\n",
    "                It supports multiple programming paradigms including procedural and\n",
    "                object-oriented programming.\n",
    "            \"\"\",\n",
    "            \"business\": \"\"\"\n",
    "                The company's Q3 results exceeded expectations with revenue growth of 15%.\n",
    "                Customer acquisition costs decreased while retention rates improved.\n",
    "                The board has approved a new strategic initiative focusing on expansion.\n",
    "            \"\"\",\n",
    "            \"finnish\": \"\"\"\n",
    "                Ohjelmistokehittäjä työskentelee asiakasprojektissa kehittäen uusia \n",
    "                ominaisuuksia verkkokauppajärjestelmään. Tekninen toteutus vaatii\n",
    "                erityistä huomiota tietoturvan osalta.\n",
    "            \"\"\"\n",
    "        }\n",
    "        \n",
    "        # Save test texts using FileUtils\n",
    "        df = pd.DataFrame([\n",
    "            {\"name\": name, \"content\": content.strip()}\n",
    "            for name, content in texts.items()\n",
    "        ])\n",
    "        \n",
    "        self.file_utils.save_data_to_disk(\n",
    "            data={\"texts\": df},\n",
    "            output_type=\"raw\",\n",
    "            file_name=\"test_texts\",\n",
    "            output_filetype=\"xlsx\",\n",
    "            include_timestamp=False\n",
    "        )\n",
    "        \n",
    "        return texts\n",
    "\n",
    "    async def test_statistical_analysis(self, text: str, language: str = None):\n",
    "        \"\"\"Test statistical keyword extraction.\"\"\"\n",
    "        if language is None:\n",
    "            # Try to detect language or use default\n",
    "            from langdetect import detect\n",
    "            try:\n",
    "                language = detect(text)\n",
    "            except:\n",
    "                language = \"en\"\n",
    "        \n",
    "        # Create processor and analyzer\n",
    "        processor = create_text_processor(language=language)\n",
    "        analyzer = KeywordAnalyzer(\n",
    "            config={\"weights\": {\"statistical\": 1.0, \"llm\": 0.0}},  # Statistical only\n",
    "            language_processor=processor\n",
    "        )\n",
    "        \n",
    "        results = await analyzer.analyze(text)\n",
    "        return results\n",
    "\n",
    "    async def test_llm_analysis(self, text: str, language: str = None):\n",
    "        \"\"\"Test LLM-based keyword extraction.\"\"\"\n",
    "        if language is None:\n",
    "            # Try to detect language or use default\n",
    "            from langdetect import detect\n",
    "            try:\n",
    "                language = detect(text)\n",
    "            except:\n",
    "                language = \"en\"\n",
    "        \n",
    "        analyzer = KeywordAnalyzer(\n",
    "            config={\"weights\": {\"statistical\": 0.0, \"llm\": 1.0}},  # LLM only\n",
    "            language_processor=create_text_processor(language=language)\n",
    "        )\n",
    "        \n",
    "        results = await analyzer.analyze(text)\n",
    "        return results\n",
    "\n",
    "    async def test_combined_analysis(self, text: str, language: str = None):\n",
    "        \"\"\"Test combined statistical and LLM analysis.\"\"\"\n",
    "        if language is None:\n",
    "            # Try to detect language or use default\n",
    "            from langdetect import detect\n",
    "            try:\n",
    "                language = detect(text)\n",
    "            except:\n",
    "                language = \"en\"\n",
    "        \n",
    "        analyzer = KeywordAnalyzer(\n",
    "            config={\n",
    "                \"weights\": {\"statistical\": 0.4, \"llm\": 0.6},\n",
    "                \"max_keywords\": 10,\n",
    "                \"min_confidence\": 0.3\n",
    "            },\n",
    "            language_processor=create_text_processor(language=language)\n",
    "        )\n",
    "        \n",
    "        results = await analyzer.analyze(text)\n",
    "        return results\n",
    "\n",
    "    # async def test_combined_analysis(self, text_key: str):\n",
    "    #     \"\"\"Test combined statistical and LLM analysis.\"\"\"\n",
    "    #     text = self.test_texts[text_key]\n",
    "    #     language = \"fi\" if text_key == \"finnish\" else \"en\"\n",
    "        \n",
    "    #     analyzer = KeywordAnalyzer(\n",
    "    #         config={\n",
    "    #             \"weights\": {\"statistical\": 0.4, \"llm\": 0.6},\n",
    "    #             \"max_keywords\": 10,\n",
    "    #             \"min_confidence\": 0.3\n",
    "    #         },\n",
    "    #         language_processor=create_text_processor(language=language)\n",
    "    #     )\n",
    "        \n",
    "    #     print(f\"\\nTesting Combined Analysis for {text_key}:\")\n",
    "    #     print(\"=\" * 50)\n",
    "    #     print(f\"Text: {text[:100]}...\")\n",
    "        \n",
    "    #     results = await analyzer.analyze(text)\n",
    "        \n",
    "    #     print(\"\\nCombined Keywords:\")\n",
    "    #     if hasattr(results, \"keywords\"):\n",
    "    #         for kw in results.keywords:\n",
    "    #             print(f\"- {kw}\")\n",
    "        \n",
    "    #     return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentTester:\n",
    "    \"\"\"Helper class for testing with different content types.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.file_utils = FileUtils()\n",
    "        self.test_texts = self._load_test_content()\n",
    "        self.keyword_tester = KeywordTester()\n",
    "        \n",
    "    def _load_test_content(self) -> Dict[str, Dict[str, List[str]]]:\n",
    "        \"\"\"Load test content from files.\"\"\"\n",
    "        content = {}\n",
    "        \n",
    "        for lang in [\"en\", \"fi\"]:\n",
    "            try:\n",
    "                # Load content using FileUtils\n",
    "                df = self.file_utils.load_single_file(\n",
    "                    f\"test_content_{lang}.xlsx\",\n",
    "                    input_type=\"raw\"\n",
    "                )\n",
    "                \n",
    "                # Group by content type\n",
    "                content[lang] = {}\n",
    "                for content_type, group in df.groupby(\"type\"):\n",
    "                    content[lang][content_type] = group[\"content\"].tolist()\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not load test content for {lang}: {e}\")\n",
    "                content[lang] = {}\n",
    "        \n",
    "        return content\n",
    "\n",
    "    async def test_content_type(\n",
    "        self, \n",
    "        language: str, \n",
    "        content_type: str, \n",
    "        analyzer: KeywordAnalyzer,\n",
    "        show_comparison: bool = True\n",
    "    ) -> List[Any]:\n",
    "        \"\"\"Test analysis for specific content type with optional comparison.\"\"\"\n",
    "        if not self.test_texts.get(language, {}).get(content_type):\n",
    "            logger.warning(f\"No {content_type} content available for {language}\")\n",
    "            return []\n",
    "            \n",
    "        results = []\n",
    "        comparison_results = []\n",
    "        texts = self.test_texts[language][content_type]\n",
    "        \n",
    "        logger.info(f\"Processing {len(texts)} texts for {language} {content_type}\")\n",
    "        \n",
    "        for i, text in enumerate(texts, 1):\n",
    "            try:\n",
    "                logger.debug(f\"Processing text {i}/{len(texts)}\")\n",
    "                \n",
    "                if show_comparison:\n",
    "                    print(f\"\\nText {i}:\")\n",
    "                    print(\"-\" * 50)\n",
    "                    print(f\"Content: {text[:100]}...\")\n",
    "                    \n",
    "                    # Run comparison analysis\n",
    "                    stat_results = await self.keyword_tester.test_statistical_analysis(\n",
    "                        text, language=language\n",
    "                    )\n",
    "                    llm_results = await self.keyword_tester.test_llm_analysis(\n",
    "                        text, language=language\n",
    "                    )\n",
    "                    combined_results = await analyzer.analyze(text)\n",
    "                    \n",
    "                    # Print comparison\n",
    "                    print(\"\\nResults Comparison:\")\n",
    "                    print(\"-\" * 30)\n",
    "                    print(\"Statistical:\", stat_results.keywords if hasattr(stat_results, \"keywords\") else [])\n",
    "                    print(\"LLM:\", llm_results.keywords if hasattr(llm_results, \"keywords\") else [])\n",
    "                    print(\"Combined:\", combined_results.keywords if hasattr(combined_results, \"keywords\") else [])\n",
    "                    \n",
    "                    results.append(combined_results)\n",
    "                    comparison_results.append((stat_results, llm_results, combined_results))\n",
    "                else:\n",
    "                    # Just run normal analysis\n",
    "                    result = await analyzer.analyze(text)\n",
    "                    results.append(result)\n",
    "                    \n",
    "                    print(f\"\\nText {i}:\")\n",
    "                    print(\"Keywords:\", result.keywords if hasattr(result, \"keywords\") else [])\n",
    "                    print(\"Domain Keywords:\", result.domain_keywords if hasattr(result, \"domain_keywords\") else {})\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing text {i}: {e}\")\n",
    "                results.append(None)\n",
    "                if show_comparison:\n",
    "                    comparison_results.append((None, None, None))\n",
    "            \n",
    "        return comparison_results if show_comparison else results\n",
    "    \n",
    "    async def analyze_text_with_comparison(\n",
    "        self,\n",
    "        text: str,\n",
    "        language: str = \"en\"\n",
    "    ) -> Tuple[Any, Any, Any]:\n",
    "        \"\"\"Analyze a single text with comparison of different methods.\"\"\"\n",
    "        try:\n",
    "            print(\"\\nRunning Analysis with Comparison:\")\n",
    "            print(\"=\" * 50)\n",
    "            print(f\"Text: {text[:100]}...\")\n",
    "            \n",
    "            # Run all analysis types\n",
    "            stat_results = await self.keyword_tester.test_statistical_analysis(\n",
    "                text, language=language\n",
    "            )\n",
    "            llm_results = await self.keyword_tester.test_llm_analysis(\n",
    "                text, language=language\n",
    "            )\n",
    "            combined_results = await self.keyword_tester.test_combined_analysis(\n",
    "                text, language=language\n",
    "            )\n",
    "            \n",
    "            # Print comparison\n",
    "            print(\"\\nResults Comparison:\")\n",
    "            print(\"-\" * 30)\n",
    "            print(\"Statistical:\", stat_results.keywords if hasattr(stat_results, \"keywords\") else [])\n",
    "            print(\"LLM:\", llm_results.keywords if hasattr(llm_results, \"keywords\") else [])\n",
    "            print(\"Combined:\", combined_results.keywords if hasattr(combined_results, \"keywords\") else [])\n",
    "            \n",
    "            return stat_results, llm_results, combined_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error analyzing text: {e}\")\n",
    "            return None, None, None\n",
    "\n",
    "    def get_content_types(self, language: str) -> List[str]:\n",
    "        \"\"\"Get available content types for a language.\"\"\"\n",
    "        return list(self.test_texts.get(language, {}).keys())\n",
    "\n",
    "    def get_text_count(self, language: str, content_type: str) -> int:\n",
    "        \"\"\"Get number of texts for a language and content type.\"\"\"\n",
    "        return len(self.test_texts.get(language, {}).get(content_type, []))\n",
    "\n",
    "    async def analyze_single_text(\n",
    "        self,\n",
    "        text: str,\n",
    "        language: str,\n",
    "        analyzer: KeywordAnalyzer\n",
    "    ) -> Any:\n",
    "        \"\"\"Analyze a single text and display results.\"\"\"\n",
    "        try:\n",
    "            result = await analyzer.analyze(text)\n",
    "            \n",
    "            print(\"\\nAnalysis Results:\")\n",
    "            print(\"Keywords:\", result.keywords if hasattr(result, \"keywords\") else [])\n",
    "            print(\"Domain Keywords:\", result.domain_keywords if hasattr(result, \"domain_keywords\") else {})\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error analyzing text: {e}\")\n",
    "            return None\n",
    "\n",
    "async def run_content_tests(show_comparison: bool = True):\n",
    "    \"\"\"Run tests for all content types.\n",
    "    \n",
    "    Args:\n",
    "        show_comparison: If True, shows comparison between statistical, LLM, and combined results\n",
    "    \"\"\"\n",
    "    from src.loaders.parameter_adapter import ParameterAdapter\n",
    "    \n",
    "    tester = ContentTester()\n",
    "    file_utils = FileUtils()\n",
    "    \n",
    "    # Load main config from project root\n",
    "    config_path = Path(file_utils.project_root) / \"config.yaml\"\n",
    "    try:\n",
    "        logger.info(f\"Loading config from: {config_path}\")\n",
    "        main_config = file_utils.load_yaml(config_path)\n",
    "        lang_configs = main_config.get(\"languages\", {})\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Could not load main config from {config_path}: {e}\")\n",
    "        lang_configs = {}\n",
    "    \n",
    "    # Load parameters using ParameterAdapter\n",
    "    en_params = ParameterAdapter(\n",
    "        file_utils.get_data_path(\"configurations\") / \"parameters_en.xlsx\"\n",
    "    ).parameters\n",
    "    \n",
    "    fi_params = ParameterAdapter(\n",
    "        file_utils.get_data_path(\"configurations\") / \"parameters_fi.xlsx\"\n",
    "    ).parameters\n",
    "    \n",
    "    # Create analyzers with parameters\n",
    "    en_analyzer = KeywordAnalyzer(\n",
    "        config={\n",
    "            **en_params.general.model_dump(),  # Convert to dict\n",
    "            \"weights\": {\"statistical\": 0.4, \"llm\": 0.6},\n",
    "            \"max_keywords\": 8\n",
    "        },\n",
    "        language_processor=create_text_processor(\n",
    "            language=\"en\",\n",
    "            config=lang_configs.get(\"en\", {})\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    fi_analyzer = KeywordAnalyzer(\n",
    "        config={\n",
    "            **fi_params.general.model_dump(),  # Convert to dict\n",
    "            \"weights\": {\"statistical\": 0.4, \"llm\": 0.6},\n",
    "            \"max_keywords\": 8\n",
    "        },\n",
    "        language_processor=create_text_processor(\n",
    "            language=\"fi\",\n",
    "            config=lang_configs.get(\"fi\", {})\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Test English content\n",
    "    print(\"Testing English content:\")\n",
    "    print(\"=\" * 50)\n",
    "    for content_type in tester.get_content_types(\"en\"):\n",
    "        print(f\"\\nTesting {content_type} content:\")\n",
    "        results[f\"en_{content_type}\"] = await tester.test_content_type(\n",
    "            \"en\", \n",
    "            content_type, \n",
    "            en_analyzer,\n",
    "            show_comparison=show_comparison\n",
    "        )\n",
    "    \n",
    "    # Test Finnish content\n",
    "    print(\"\\nTesting Finnish content:\")\n",
    "    print(\"=\" * 50)\n",
    "    for content_type in tester.get_content_types(\"fi\"):\n",
    "        print(f\"\\nTesting {content_type} content:\")\n",
    "        results[f\"fi_{content_type}\"] = await tester.test_content_type(\n",
    "            \"fi\", \n",
    "            content_type, \n",
    "            fi_analyzer,\n",
    "            show_comparison=show_comparison\n",
    "        )\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Helper function for single text analysis\n",
    "async def analyze_single_text(text: str, language: str = \"en\", show_comparison: bool = True):\n",
    "    \"\"\"Analyze a single text with optional comparison.\n",
    "    \n",
    "    Args:\n",
    "        text: Text to analyze\n",
    "        language: Language code ('en' or 'fi')\n",
    "        show_comparison: If True, shows comparison between different analysis methods\n",
    "    \"\"\"\n",
    "    tester = ContentTester()\n",
    "    \n",
    "    if show_comparison:\n",
    "        return await tester.analyze_text_with_comparison(text, language)\n",
    "    else:\n",
    "        return await tester.analyze_single_text(text, language)\n",
    "\n",
    "\n",
    "\n",
    "# Run in notebook:\n",
    "# All tests:\n",
    "# results = await run_content_tests()\n",
    "\n",
    "# Single text analysis:\n",
    "# result = await analyze_text(\"Your text here\", language=\"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-11 16:50:32 - src.core.language_processing.factory - INFO - Using default configuration\n",
      "2024-11-11 16:50:32 - src.core.language_processing.english - INFO - Initialized English processor with 831 stopwords\n",
      "2024-11-11 16:50:32 - src.core.language_processing.english - INFO - Initialized English processor with 831 stopwords\n",
      "2024-11-11 16:50:44 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analysis Results:\n",
      "==================================================\n",
      "\n",
      "Keywords found:\n",
      "- Machine Learning     (score: 0.68)\n",
      "- Artificial Intelligence (score: 0.67)\n",
      "- Deep Learning        (score: 0.66)\n",
      "- Neural Networks      (score: 0.65)\n",
      "- Data Analysis        (score: 0.63)\n",
      "- Business Transformation (score: 0.63)\n",
      "- AI Technology        (score: 0.61)\n",
      "- Large Datasets       (score: 0.60)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "async def test_position_aware_keywords():\n",
    "    \"\"\"Test position-aware keyword extraction.\"\"\"\n",
    "    \n",
    "    # Text with clearly positioned keywords\n",
    "    text = \"\"\"Machine Learning Applications\n",
    "    \n",
    "    Artificial intelligence and deep learning models are transforming businesses.\n",
    "    These neural networks help companies analyze large datasets efficiently.\n",
    "    Machine learning solutions provide valuable insights.\n",
    "    \n",
    "    In conclusion, AI technology continues to evolve rapidly.\"\"\"\n",
    "    \n",
    "    # Initialize components\n",
    "    file_utils = FileUtils()\n",
    "    processor = create_text_processor(language=\"en\")\n",
    "    \n",
    "    # Create LLM instance\n",
    "    from src.core.llm.factory import create_llm\n",
    "    llm = create_llm()\n",
    "    \n",
    "    analyzer = KeywordAnalyzer(\n",
    "        llm=llm,  # Pass the LLM instance\n",
    "        config={\n",
    "            \"weights\": {\"statistical\": 0.4, \"llm\": 0.6},\n",
    "            \"max_keywords\": 8,\n",
    "            \"position_weights\": {\n",
    "                \"title\": 1.5,\n",
    "                \"first_para\": 1.3,\n",
    "                \"last_para\": 1.2,\n",
    "                \"body\": 1.0\n",
    "            }\n",
    "        },\n",
    "        language_processor=processor\n",
    "    )\n",
    "    \n",
    "    # Analyze text\n",
    "    results = await analyzer.analyze(text)\n",
    "    \n",
    "    print(\"\\nAnalysis Results:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if results.success:\n",
    "        print(\"\\nKeywords found:\")\n",
    "        for kw in results.keywords:\n",
    "            print(f\"- {kw.keyword:<20} (score: {kw.score:.2f})\")\n",
    "            \n",
    "        if hasattr(results, \"domain_keywords\") and results.domain_keywords:\n",
    "            print(\"\\nDomain grouping:\")\n",
    "            for domain, words in results.domain_keywords.items():\n",
    "                print(f\"{domain}: {', '.join(words)}\")\n",
    "    else:\n",
    "        print(f\"Analysis failed: {results.error}\")\n",
    "        \n",
    "    return results\n",
    "\n",
    "# Run the test\n",
    "results = await test_position_aware_keywords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-11 16:50:45 - src.core.language_processing.factory - INFO - Using default configuration\n",
      "2024-11-11 16:50:45 - src.core.language_processing.english - INFO - Initialized English processor with 831 stopwords\n",
      "2024-11-11 16:50:45 - src.core.language_processing.english - INFO - Initialized English processor with 831 stopwords\n",
      "2024-11-11 16:50:50 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Business Text Analysis:\n",
      "==================================================\n",
      "\n",
      "Keywords by domain:\n",
      "\n",
      "Financial:\n",
      "- financial (1.00)\n",
      "- review (1.00)\n",
      "- revenue (1.00)\n",
      "- growth (1.00)\n",
      "- market expansion (0.64)\n",
      "- operating margins (0.63)\n",
      "- cost optimization (0.63)\n",
      "\n",
      "Metrics:\n",
      "- performance (1.00)\n"
     ]
    }
   ],
   "source": [
    "async def test_business_keywords():\n",
    "    \"\"\"Test keyword extraction with business text.\"\"\"\n",
    "    \n",
    "    text = \"\"\"Q3 Financial Performance Review\n",
    "    \n",
    "    Revenue growth exceeded expectations with 15% year-over-year increase.\n",
    "    Operating margins improved due to cost optimization initiatives.\n",
    "    Customer acquisition metrics show positive trends.\n",
    "    \n",
    "    Looking ahead, market expansion remains our strategic priority.\"\"\"\n",
    "    \n",
    "    from src.core.llm.factory import create_llm\n",
    "    processor = create_text_processor(language=\"en\")\n",
    "    llm = create_llm()\n",
    "    \n",
    "    analyzer = KeywordAnalyzer(\n",
    "        llm=llm,\n",
    "        config={\n",
    "            \"weights\": {\"statistical\": 0.4, \"llm\": 0.6},\n",
    "            \"max_keywords\": 8,\n",
    "            \"domain_keywords\": {\n",
    "                \"financial\": [\"revenue\", \"margins\", \"cost\", \"growth\"],\n",
    "                \"business\": [\"strategic\", \"market\", \"customer\", \"acquisition\"],\n",
    "                \"metrics\": [\"performance\", \"trends\", \"optimization\"]\n",
    "            },\n",
    "            \"position_weights\": {\n",
    "                \"title\": 1.5,\n",
    "                \"first_para\": 1.3,\n",
    "                \"last_para\": 1.2,\n",
    "                \"body\": 1.0\n",
    "            }\n",
    "        },\n",
    "        language_processor=processor\n",
    "    )\n",
    "    \n",
    "    results = await analyzer.analyze(text)\n",
    "    \n",
    "    print(\"\\nBusiness Text Analysis:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if results.success:\n",
    "        print(\"\\nKeywords by domain:\")\n",
    "        domains = {}\n",
    "        for kw in results.keywords:\n",
    "            domain = kw.domain or \"general\"\n",
    "            if domain not in domains:\n",
    "                domains[domain] = []\n",
    "            domains[domain].append(f\"{kw.keyword} ({kw.score:.2f})\")\n",
    "            \n",
    "        for domain, keywords in domains.items():\n",
    "            print(f\"\\n{domain.title()}:\")\n",
    "            for kw in keywords:\n",
    "                print(f\"- {kw}\")\n",
    "    else:\n",
    "        print(f\"Analysis failed: {results.error}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run business test\n",
    "business_results = await test_business_keywords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-11 16:50:50 - src.core.language_processing.factory - INFO - Using default configuration\n",
      "2024-11-11 16:50:50 - src.core.language_processing.english - INFO - Initialized English processor with 831 stopwords\n",
      "2024-11-11 16:50:50 - src.core.language_processing.english - INFO - Initialized English processor with 831 stopwords\n",
      "2024-11-11 16:50:55 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Improved Keyword Analysis:\n",
      "==================================================\n",
      "\n",
      "Clustered Keywords:\n",
      "\n",
      "technical:\n",
      "- Machine Learning          (score: 0.68)\n",
      "- Artificial Intelligence   (score: 0.68)\n",
      "- big data                  (score: 0.65)\n",
      "\n",
      "business:\n",
      "- ROI                       (score: 0.61)\n",
      "- performance indicators    (score: 0.58)\n",
      "\n",
      "technical:\n",
      "- algorithms                (score: 0.54)\n",
      "\n",
      "business:\n",
      "- business transformation   (score: 0.50)\n",
      "\n",
      "General:\n",
      "- learn                     (score: 0.48)\n"
     ]
    }
   ],
   "source": [
    "async def test_keyword_improvements():\n",
    "    \"\"\"Test improved keyword extraction with clustering.\"\"\"\n",
    "    \n",
    "    text = \"\"\"AI and Machine Learning Applications\n",
    "    \n",
    "    Artificial Intelligence (AI) and ML models are transforming business.\n",
    "    These machine learning algorithms help analyze big data effectively.\n",
    "    The ROI on AI investments has been significant.\n",
    "    \n",
    "    Key performance indicators (KPIs) show positive results.\"\"\"\n",
    "    \n",
    "    from src.core.llm.factory import create_llm\n",
    "    processor = create_text_processor(language=\"en\")\n",
    "    llm = create_llm()\n",
    "    \n",
    "    analyzer = KeywordAnalyzer(\n",
    "        llm=llm,\n",
    "        config={\n",
    "            \"weights\": {\"statistical\": 0.4, \"llm\": 0.6},\n",
    "            \"max_keywords\": 8,\n",
    "            \"domain_keywords\": {\n",
    "                \"technical\": [\"ai\", \"artificial intelligence\", \"machine learning\", \"ml\", \"algorithm\"],\n",
    "                \"business\": [\"roi\", \"return on investment\", \"kpi\", \"performance indicator\"]\n",
    "            },\n",
    "            \"clustering\": {\n",
    "                \"similarity_threshold\": 0.85,\n",
    "                \"max_cluster_size\": 3,\n",
    "                \"boost_factor\": 1.2\n",
    "            }\n",
    "        },\n",
    "        language_processor=processor\n",
    "    )\n",
    "    \n",
    "    results = await analyzer.analyze(text)\n",
    "    \n",
    "    print(\"\\nImproved Keyword Analysis:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if results.success:\n",
    "        print(\"\\nClustered Keywords:\")\n",
    "        current_domain = None\n",
    "        for kw in results.keywords:\n",
    "            if kw.domain != current_domain:\n",
    "                current_domain = kw.domain\n",
    "                print(f\"\\n{current_domain or 'General'}:\")\n",
    "            print(f\"- {kw.keyword:<25} (score: {kw.score:.2f})\")\n",
    "    else:\n",
    "        print(f\"Analysis failed: {results.error}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run test\n",
    "results = await test_keyword_improvements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_keyword_improvements():\n",
    "    \"\"\"Test improved keyword extraction with clustering.\"\"\"\n",
    "    \n",
    "    text = \"\"\"AI and Machine Learning Applications\n",
    "    \n",
    "    Artificial Intelligence (AI) and ML models are transforming business.\n",
    "    These machine learning algorithms help analyze big data effectively.\n",
    "    The ROI on AI investments has been significant.\n",
    "    \n",
    "    Key performance indicators (KPIs) show positive results.\"\"\"\n",
    "    \n",
    "    from src.core.llm.factory import create_llm\n",
    "    processor = create_text_processor(language=\"en\")\n",
    "    llm = create_llm()\n",
    "    \n",
    "    analyzer = KeywordAnalyzer(\n",
    "        llm=llm,\n",
    "        config={\n",
    "            \"weights\": {\"statistical\": 0.4, \"llm\": 0.6},\n",
    "            \"max_keywords\": 8,\n",
    "            \"domain_keywords\": {\n",
    "                \"technical\": [\"ai\", \"artificial intelligence\", \"machine learning\", \"ml\", \"algorithm\"],\n",
    "                \"business\": [\"roi\", \"return on investment\", \"kpi\", \"performance indicator\"]\n",
    "            },\n",
    "            \"clustering\": {\n",
    "                \"similarity_threshold\": 0.85,\n",
    "                \"max_cluster_size\": 3,\n",
    "                \"boost_factor\": 1.2\n",
    "            }\n",
    "        },\n",
    "        language_processor=processor\n",
    "    )\n",
    "    \n",
    "    results = await analyzer.analyze(text)\n",
    "    \n",
    "    print(\"\\nImproved Keyword Analysis:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if results.success:\n",
    "        print(\"\\nClustered Keywords:\")\n",
    "        current_domain = None\n",
    "        for kw in results.keywords:\n",
    "            if kw.domain != current_domain:\n",
    "                current_domain = kw.domain\n",
    "                print(f\"\\n{current_domain or 'General'}:\")\n",
    "            print(f\"- {kw.keyword:<25} (score: {kw.score:.2f})\")\n",
    "    else:\n",
    "        print(f\"Analysis failed: {results.error}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run test\n",
    "results = await test_keyword_improvements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-11 16:58:00 - src.core.language_processing.factory - INFO - Using default configuration\n",
      "2024-11-11 16:58:00 - src.core.language_processing.finnish - INFO - Loaded 747 stopwords from c:\\Users\\tja\\OneDrive - Rastor-instituutti ry\\Tiedostot\\Rastor-instituutti\\kehittäminen\\analytiikka\\repos\\semantic-text-analyzer\\data\\configurations\\stop_words\\fi.txt\n",
      "2024-11-11 16:58:00 - src.core.language_processing.finnish - INFO - Detected platform: win32\n",
      "Exception ignored in: <function Voikko.__del__ at 0x0000025DFAA439D0>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\tja\\AppData\\Local\\miniconda3\\envs\\semantic-analyzer\\lib\\site-packages\\libvoikko.py\", line 446, in __del__\n",
      "    self.terminate()\n",
      "  File \"c:\\Users\\tja\\AppData\\Local\\miniconda3\\envs\\semantic-analyzer\\lib\\site-packages\\libvoikko.py\", line 476, in terminate\n",
      "    if self.__handle:\n",
      "AttributeError: 'Voikko' object has no attribute '_Voikko__handle'\n",
      "2024-11-11 16:58:00 - src.core.language_processing.finnish - INFO - Added C:\\scripts\\Voikko to DLL search path\n",
      "2024-11-11 16:58:00 - src.core.language_processing.finnish - INFO - Verifying Voikko installation...\n",
      "2024-11-11 16:58:00 - src.core.language_processing.finnish - INFO - DLL exists: True (C:\\scripts\\Voikko\\libvoikko-1.dll)\n",
      "2024-11-11 16:58:00 - src.core.language_processing.finnish - INFO - Found dictionary version 5 at: C:\\scripts\\Voikko\\voikko\\5\\mor-standard\n",
      "2024-11-11 16:58:00 - src.core.language_processing.finnish - INFO - Successfully initialized Voikko with path: C:\\scripts\\Voikko\n",
      "2024-11-11 16:58:06 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Suomenkielinen avainsana-analyysi:\n",
      "==================================================\n",
      "\n",
      "Ryhmitellyt avainsanat:\n",
      "\n",
      "liiketoiminta:\n",
      "- liiketoiminta                  (tulos: 1.00)\n",
      "\n",
      "tekninen:\n",
      "- tekoäly                        (tulos: 1.00)\n",
      "\n",
      "technical:\n",
      "- verkkokauppajärjestelmä        (tulos: 1.00)\n",
      "- koneoppimismallit              (tulos: 0.65)\n",
      "\n",
      "business:\n",
      "- asiakastyytyväisyys            (tulos: 0.63)\n",
      "\n",
      "technical:\n",
      "- automaation                    (tulos: 0.61)\n",
      "- tietoturvaratkaisut            (tulos: 0.60)\n",
      "- ohjelmistokehittäjät           (tulos: 0.60)\n"
     ]
    }
   ],
   "source": [
    "async def test_finnish_keywords():\n",
    "    \"\"\"Test improved keyword extraction with Finnish text.\"\"\"\n",
    "    \n",
    "    text = \"\"\"Tekoälyn Hyödyntäminen Liiketoiminnassa\n",
    "    \n",
    "    Ohjelmistokehittäjät hyödyntävät koneoppimismalleja asiakasprojekteissa.\n",
    "    Verkkokauppajärjestelmän tehokkuus on parantunut automaation avulla.\n",
    "    Tietoturvaratkaisut ovat keskeinen osa teknistä toteutusta.\n",
    "    \n",
    "    Liiketoiminnan mittarit osoittavat asiakastyytyväisyyden parantuneen merkittävästi.\"\"\"\n",
    "    \n",
    "    from src.core.llm.factory import create_llm\n",
    "    processor = create_text_processor(language=\"fi\")\n",
    "    llm = create_llm()\n",
    "    \n",
    "    analyzer = KeywordAnalyzer(\n",
    "        llm=llm,\n",
    "        config={\n",
    "            \"weights\": {\"statistical\": 0.4, \"llm\": 0.6},\n",
    "            \"max_keywords\": 8,\n",
    "            \"domain_keywords\": {\n",
    "                \"tekninen\": [\"tekoäly\", \"koneoppiminen\", \"ohjelmisto\", \"tietoturva\", \"automaatio\"],\n",
    "                \"liiketoiminta\": [\"asiakastyytyväisyys\", \"liiketoiminta\", \"mittarit\", \"verkkokauppa\"]\n",
    "            },\n",
    "            \"clustering\": {\n",
    "                \"similarity_threshold\": 0.85,\n",
    "                \"max_cluster_size\": 3,\n",
    "                \"boost_factor\": 1.2\n",
    "            }\n",
    "        },\n",
    "        language_processor=processor\n",
    "    )\n",
    "    \n",
    "    results = await analyzer.analyze(text)\n",
    "    \n",
    "    print(\"\\nSuomenkielinen avainsana-analyysi:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if results.success:\n",
    "        print(\"\\nRyhmitellyt avainsanat:\")\n",
    "        current_domain = None\n",
    "        for kw in results.keywords:\n",
    "            if kw.domain != current_domain:\n",
    "                current_domain = kw.domain\n",
    "                print(f\"\\n{current_domain or 'Yleinen'}:\")\n",
    "            print(f\"- {kw.keyword:<30} (tulos: {kw.score:.2f})\")\n",
    "            \n",
    "        if hasattr(results, \"domain_keywords\") and results.domain_keywords:\n",
    "            print(\"\\nAvainsanat toimialueittain:\")\n",
    "            for domain, words in results.domain_keywords.items():\n",
    "                print(f\"\\n{domain}:\")\n",
    "                print(\", \".join(words))\n",
    "    else:\n",
    "        print(f\"Analyysi epäonnistui: {results.error}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run Finnish test\n",
    "results = await test_finnish_keywords()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_keyword_comparison(stat_keywords: List[str], llm_keywords: List[str], combined_keywords: List[str]) -> Dict:\n",
    "    \"\"\"Analyze and compare keywords from different methods.\"\"\"\n",
    "    # Find overlaps and unique keywords\n",
    "    all_keywords = set(stat_keywords) | set(llm_keywords) | set(combined_keywords)\n",
    "    \n",
    "    analysis = {\n",
    "        \"all_methods\": set(stat_keywords) & set(llm_keywords) & set(combined_keywords),\n",
    "        \"stat_llm_only\": set(stat_keywords) & set(llm_keywords) - set(combined_keywords),\n",
    "        \"stat_combined_only\": set(stat_keywords) & set(combined_keywords) - set(llm_keywords),\n",
    "        \"llm_combined_only\": set(llm_keywords) & set(combined_keywords) - set(stat_keywords),\n",
    "        \"stat_only\": set(stat_keywords) - set(llm_keywords) - set(combined_keywords),\n",
    "        \"llm_only\": set(llm_keywords) - set(stat_keywords) - set(combined_keywords),\n",
    "        \"combined_only\": set(combined_keywords) - set(stat_keywords) - set(llm_keywords)\n",
    "    }\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "def display_comparison_results(text: str, stat_results: Any, llm_results: Any, combined_results: Any):\n",
    "    \"\"\"Display enhanced comparison of analysis results.\"\"\"\n",
    "    print(\"\\nOriginal Text:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(text.strip())\n",
    "    \n",
    "    print(\"\\nResults Comparison:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Get keywords from each method\n",
    "    stat_kw = stat_results.keywords if hasattr(stat_results, \"keywords\") else []\n",
    "    llm_kw = llm_results.keywords if hasattr(llm_results, \"keywords\") else []\n",
    "    combined_kw = combined_results.keywords if hasattr(combined_results, \"keywords\") else []\n",
    "    \n",
    "    # Analyze overlaps\n",
    "    analysis = analyze_keyword_comparison(stat_kw, llm_kw, combined_kw)\n",
    "    \n",
    "    # Display basic results\n",
    "    print(\"\\nKeywords by Method:\")\n",
    "    print(\"Statistical:\", stat_kw)\n",
    "    print(\"LLM:\", llm_kw)\n",
    "    print(\"Combined:\", combined_kw)\n",
    "    \n",
    "    # Display analysis\n",
    "    print(\"\\nKeyword Analysis:\")\n",
    "    print(\"Found by all methods:\", sorted(analysis[\"all_methods\"]))\n",
    "    print(\"Statistical & LLM only:\", sorted(analysis[\"stat_llm_only\"]))\n",
    "    print(\"Statistical & Combined only:\", sorted(analysis[\"stat_combined_only\"]))\n",
    "    print(\"LLM & Combined only:\", sorted(analysis[\"llm_combined_only\"]))\n",
    "    print(\"Statistical only:\", sorted(analysis[\"stat_only\"]))\n",
    "    print(\"LLM only:\", sorted(analysis[\"llm_only\"]))\n",
    "    print(\"Combined only:\", sorted(analysis[\"combined_only\"]))\n",
    "    \n",
    "    # Display domain-specific insights\n",
    "    print(\"\\nDomain Analysis:\")\n",
    "    if hasattr(combined_results, \"domain_keywords\"):\n",
    "        for domain, keywords in combined_results.domain_keywords.items():\n",
    "            print(f\"{domain}:\", keywords)\n",
    "    \n",
    "    # Provide insights\n",
    "    print(\"\\nInsights:\")\n",
    "    print(\"- Statistical method found\", len(stat_kw), \"keywords\")\n",
    "    print(\"- LLM method found\", len(llm_kw), \"keywords\")\n",
    "    print(\"- Combined method found\", len(combined_kw), \"keywords\")\n",
    "    print(\"- Agreement between all methods:\", len(analysis[\"all_methods\"]), \"keywords\")\n",
    "    \n",
    "    # Calculate Jaccard similarity\n",
    "    def jaccard_similarity(set1, set2):\n",
    "        if not set1 or not set2:\n",
    "            return 0\n",
    "        return len(set1 & set2) / len(set1 | set2)\n",
    "    \n",
    "    print(\"\\nSimilarity Analysis:\")\n",
    "    print(\"Statistical vs LLM:\", f\"{jaccard_similarity(set(stat_kw), set(llm_kw)):.2f}\")\n",
    "    print(\"Statistical vs Combined:\", f\"{jaccard_similarity(set(stat_kw), set(combined_kw)):.2f}\")\n",
    "    print(\"LLM vs Combined:\", f\"{jaccard_similarity(set(llm_kw), set(combined_kw)):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- uncomment as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all tests with comparison (default)\n",
    "# results = await run_content_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all tests without comparison\n",
    "# results = await run_content_tests(show_comparison=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze a single text without comparison\n",
    "# results = await analyze_single_text(\n",
    "#     \"\"\"Your text here...\"\"\",\n",
    "#     language=\"en\",\n",
    "#     show_comparison=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = await run_content_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze a single text with comparison\n",
    "# results = await analyze_single_text(\n",
    "#     \"\"\"Strategic partnerships drive innovation and market penetration. Investment in R&amp;D resulted in three new product launches. Sales performance exceeded targets in key market segments. \"\"\",\n",
    "#     language=\"en\",\n",
    "#     show_comparison=True\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or analyze a single text with comparison\n",
    "\n",
    "text = \"\"\"Strategic partnerships drive innovation and market penetration. \n",
    "Investment in R&D resulted in three new product launches. \n",
    "Sales performance exceeded targets in key market segments.\"\"\"\n",
    "\n",
    "tester = ContentTester()\n",
    "stat_resuls, llm_resuls, combined_resuls = await tester.analyze_text_with_comparison(\n",
    "    text,\n",
    "    language=\"en\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "\n",
    "\n",
    "# display_comparison_results(text, stat_resuls, llm_results, combined_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = await run_content_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test technical text\n",
    "# await test_text(\"technical\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test business text\n",
    "# await test_text(\"business\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Finnish text\n",
    "# await test_text(\"finnish\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semantic-analyzer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
