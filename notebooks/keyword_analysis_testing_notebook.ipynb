{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added /home/topi/data-science/repos/semantic-text-analyzer to Python path\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import asyncio\n",
    "import logging\n",
    "from typing import Dict, Any, Optional\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = str(Path().resolve().parent)\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "    print(f\"Added {project_root} to Python path\")\n",
    "\n",
    "# Core components\n",
    "from src.semantic_analyzer.analyzer import SemanticAnalyzer\n",
    "from src.utils.FileUtils.file_utils import FileUtils\n",
    "from src.analyzers.keyword_analyzer import KeywordAnalyzer\n",
    "from src.core.language_processing import create_text_processor\n",
    "\n",
    "# Initialize FileUtils and set up logging\n",
    "file_utils = FileUtils()\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_environment():\n",
    "    \"\"\"Verify that the notebook environment is properly configured.\"\"\"\n",
    "    # Load environment variables\n",
    "    from dotenv import load_dotenv\n",
    "    env_path = Path(project_root) / \".env\"\n",
    "    env_loaded = load_dotenv(env_path)\n",
    "\n",
    "    # Required environment variables\n",
    "    required_env_vars = [\n",
    "        'OPENAI_API_KEY',\n",
    "        'ANTHROPIC_API_KEY',\n",
    "    ]\n",
    "\n",
    "    # Basic checks\n",
    "    basic_checks = {\n",
    "        \"Project root in path\": project_root in sys.path,\n",
    "        \"Can import src\": \"src\" in sys.modules,\n",
    "        \"FileUtils initialized\": hasattr(file_utils, \"project_root\"),\n",
    "        \".env file loaded\": env_loaded,\n",
    "    }\n",
    "\n",
    "    # Environment variable checks\n",
    "    env_var_checks = {\n",
    "        f\"{var} set\": os.getenv(var) is not None\n",
    "        for var in required_env_vars\n",
    "    }\n",
    "\n",
    "    # Check for required paths using FileUtils\n",
    "    expected_paths = {\n",
    "        \"Raw data\": file_utils.get_data_path(\"raw\"),\n",
    "        \"Processed data\": file_utils.get_data_path(\"processed\"),\n",
    "        \"Configuration\": file_utils.get_data_path(\"configurations\"),\n",
    "        \"Main config.yaml\": Path(project_root) / \"config.yaml\"\n",
    "    }\n",
    "    \n",
    "    path_checks = {\n",
    "        f\"{name} exists\": path.exists()\n",
    "        for name, path in expected_paths.items()\n",
    "    }\n",
    "\n",
    "    # Combine all checks\n",
    "    all_checks = {\n",
    "        **basic_checks,\n",
    "        **env_var_checks,\n",
    "        **path_checks\n",
    "    }\n",
    "    \n",
    "    # Print results\n",
    "    print(\"Environment Check Results:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    def print_section(title, checks):\n",
    "        print(f\"\\n{title}:\")\n",
    "        print(\"-\" * len(title))\n",
    "        for check, result in checks.items():\n",
    "            status = \"✓\" if result else \"✗\"\n",
    "            print(f\"{status} {check}\")\n",
    "    \n",
    "    print_section(\"Basic Setup\", basic_checks)\n",
    "    print_section(\"Environment Variables\", env_var_checks)\n",
    "    print_section(\"Project Structure\", path_checks)\n",
    "    \n",
    "    # Overall status\n",
    "    all_passed = all(all_checks.values())\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Environment Status:\", \"Ready ✓\" if all_passed else \"Setup needed ✗\")\n",
    "    \n",
    "    if not all_passed:\n",
    "        print(\"\\nSetup Instructions:\")\n",
    "        if not env_loaded:\n",
    "            print(\"- Create a .env file in the project root with required API keys\")\n",
    "        for var in required_env_vars:\n",
    "            if not os.getenv(var):\n",
    "                print(f\"- Add {var} to your .env file\")\n",
    "        for name, path in expected_paths.items():\n",
    "            if not path.exists():\n",
    "                print(f\"- Create {name} directory at {path}\")\n",
    "\n",
    "    return all_passed\n",
    "\n",
    "# Run verification\n",
    "# verify_environment()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeywordTester:\n",
    "    \"\"\"Helper class for testing keyword analysis components.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.file_utils = FileUtils()\n",
    "        self.test_texts = self._load_test_texts()\n",
    "        \n",
    "    def _load_test_texts(self) -> Dict[str, str]:\n",
    "        \"\"\"Load or create test texts.\"\"\"\n",
    "        texts = {\n",
    "            \"technical\": \"\"\"\n",
    "                Python is a high-level programming language known for its simplicity.\n",
    "                It supports multiple programming paradigms including procedural and\n",
    "                object-oriented programming.\n",
    "            \"\"\",\n",
    "            \"business\": \"\"\"\n",
    "                The company's Q3 results exceeded expectations with revenue growth of 15%.\n",
    "                Customer acquisition costs decreased while retention rates improved.\n",
    "                The board has approved a new strategic initiative focusing on expansion.\n",
    "            \"\"\",\n",
    "            \"finnish\": \"\"\"\n",
    "                Ohjelmistokehittäjä työskentelee asiakasprojektissa kehittäen uusia \n",
    "                ominaisuuksia verkkokauppajärjestelmään. Tekninen toteutus vaatii\n",
    "                erityistä huomiota tietoturvan osalta.\n",
    "            \"\"\"\n",
    "        }\n",
    "        \n",
    "        # Save test texts using FileUtils\n",
    "        df = pd.DataFrame([\n",
    "            {\"name\": name, \"content\": content.strip()}\n",
    "            for name, content in texts.items()\n",
    "        ])\n",
    "        \n",
    "        self.file_utils.save_data_to_disk(\n",
    "            data={\"texts\": df},\n",
    "            output_type=\"raw\",\n",
    "            file_name=\"test_texts\",\n",
    "            output_filetype=\"xlsx\",\n",
    "            include_timestamp=False\n",
    "        )\n",
    "        \n",
    "        return texts\n",
    "\n",
    "    async def test_statistical_analysis(self, text_key: str):\n",
    "        \"\"\"Test statistical keyword extraction.\"\"\"\n",
    "        text = self.test_texts[text_key]\n",
    "        language = \"fi\" if text_key == \"finnish\" else \"en\"\n",
    "        \n",
    "        # Create processor and analyzer\n",
    "        processor = create_text_processor(language=language)\n",
    "        analyzer = KeywordAnalyzer(\n",
    "            config={\"weights\": {\"statistical\": 1.0, \"llm\": 0.0}},  # Statistical only\n",
    "            language_processor=processor\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nTesting Statistical Analysis for {text_key}:\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Text: {text[:100]}...\")\n",
    "        \n",
    "        # Show processing steps\n",
    "        tokens = processor.tokenize(text)\n",
    "        print(\"\\nTokens:\")\n",
    "        print(tokens)\n",
    "        \n",
    "        # Get statistical keywords\n",
    "        results = await analyzer.analyze(text)\n",
    "        print(\"\\nStatistical Keywords:\")\n",
    "        if hasattr(results, \"keywords\"):\n",
    "            for kw in results.keywords[:10]:\n",
    "                print(f\"- {kw}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "    async def test_llm_analysis(self, text_key: str):\n",
    "        \"\"\"Test LLM-based keyword extraction.\"\"\"\n",
    "        text = self.test_texts[text_key]\n",
    "        language = \"fi\" if text_key == \"finnish\" else \"en\"\n",
    "        \n",
    "        analyzer = KeywordAnalyzer(\n",
    "            config={\"weights\": {\"statistical\": 0.0, \"llm\": 1.0}},  # LLM only\n",
    "            language_processor=create_text_processor(language=language)\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nTesting LLM Analysis for {text_key}:\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Text: {text[:100]}...\")\n",
    "        \n",
    "        results = await analyzer.analyze(text)\n",
    "        print(\"\\nLLM Keywords:\")\n",
    "        if hasattr(results, \"keywords\"):\n",
    "            for kw in results.keywords[:10]:\n",
    "                print(f\"- {kw}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "    async def test_combined_analysis(self, text_key: str):\n",
    "        \"\"\"Test combined statistical and LLM analysis.\"\"\"\n",
    "        text = self.test_texts[text_key]\n",
    "        language = \"fi\" if text_key == \"finnish\" else \"en\"\n",
    "        \n",
    "        analyzer = KeywordAnalyzer(\n",
    "            config={\n",
    "                \"weights\": {\"statistical\": 0.4, \"llm\": 0.6},\n",
    "                \"max_keywords\": 10,\n",
    "                \"min_confidence\": 0.3\n",
    "            },\n",
    "            language_processor=create_text_processor(language=language)\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nTesting Combined Analysis for {text_key}:\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Text: {text[:100]}...\")\n",
    "        \n",
    "        results = await analyzer.analyze(text)\n",
    "        \n",
    "        print(\"\\nCombined Keywords:\")\n",
    "        if hasattr(results, \"keywords\"):\n",
    "            for kw in results.keywords:\n",
    "                print(f\"- {kw}\")\n",
    "        \n",
    "        return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 21:12:45 - src.utils.FileUtils.file_utils - INFO - Data saved to /home/topi/data-science/repos/semantic-text-analyzer/data/raw/test_texts.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Initialize tester\n",
    "tester = KeywordTester()\n",
    "\n",
    "# Create test functions that can be run in notebook cells\n",
    "async def test_text(text_key: str):\n",
    "    \"\"\"Run all tests for a specific text.\"\"\"\n",
    "    print(f\"\\nTesting {text_key} text:\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Test each analysis type\n",
    "    stat_results = await tester.test_statistical_analysis(text_key)\n",
    "    llm_results = await tester.test_llm_analysis(text_key)\n",
    "    combined_results = await tester.test_combined_analysis(text_key)\n",
    "    \n",
    "    # Compare results\n",
    "    print(\"\\nResults Comparison:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Statistical:\", stat_results.keywords if hasattr(stat_results, \"keywords\") else [])\n",
    "    print(\"LLM:\", llm_results.keywords if hasattr(llm_results, \"keywords\") else [])\n",
    "    print(\"Combined:\", combined_results.keywords if hasattr(combined_results, \"keywords\") else [])\n",
    "    \n",
    "    return stat_results, llm_results, combined_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First verify stopwords are properly loaded\n",
    "def verify_stopwords():\n",
    "    \"\"\"Verify stopwords for both English and Finnish.\"\"\"\n",
    "    \n",
    "    # Check stopwords files\n",
    "    stop_words_dir = file_utils.get_data_path(\"configurations\") / \"stop_words\"\n",
    "    print(\"\\nStopwords Directory Contents:\")\n",
    "    print(f\"Directory: {stop_words_dir}\")\n",
    "    if stop_words_dir.exists():\n",
    "        for file in stop_words_dir.glob(\"*.txt\"):\n",
    "            print(f\"\\nFile: {file.name}\")\n",
    "            with open(file, 'r', encoding='utf-8') as f:\n",
    "                words = {line.strip() for line in f if line.strip()}\n",
    "                print(f\"Number of words: {len(words)}\")\n",
    "                print(\"Sample words (first 10):\")\n",
    "                pprint(sorted(list(words))[:10])\n",
    "    else:\n",
    "        print(\"Stopwords directory not found!\")\n",
    "    \n",
    "    # Test processors\n",
    "    print(\"\\nTesting Language Processors:\")\n",
    "    for lang in [\"en\", \"fi\"]:\n",
    "        print(f\"\\n{lang.upper()} Processor:\")\n",
    "        processor = create_text_processor(language=lang)\n",
    "        \n",
    "        # Check stopwords\n",
    "        if hasattr(processor, '_stop_words'):\n",
    "            print(f\"Number of stopwords: {len(processor._stop_words)}\")\n",
    "            print(\"Sample stopwords (first 10):\")\n",
    "            pprint(sorted(list(processor._stop_words))[:10])\n",
    "        \n",
    "        # Test some common words\n",
    "        test_words = {\n",
    "            \"en\": [\"the\", \"and\", \"is\", \"that\", \"with\", \"for\"],\n",
    "            \"fi\": [\"ja\", \"on\", \"että\", \"joka\", \"tai\", \"vain\"]\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nTesting common {lang} words:\")\n",
    "        for word in test_words[lang]:\n",
    "            print(f\"{word}: {'stop word' if processor.is_stop_word(word) else 'not stop word'}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_keyword_extraction():\n",
    "    \"\"\"Test keyword extraction with full debug output.\"\"\"\n",
    "    text_key = \"business\"\n",
    "    text = tester.test_texts[text_key]\n",
    "    \n",
    "    # Create language processor first\n",
    "    language_processor = create_text_processor(language=\"en\")\n",
    "    \n",
    "    # Create analyzer with explicit config\n",
    "    try:\n",
    "        # Use FileUtils to load excluded keywords\n",
    "        excluded_data = file_utils.load_excel_sheets(\n",
    "            file_utils.get_data_path(\"configurations\") / \"example_params.xlsx\"\n",
    "        )\n",
    "        excluded_df = excluded_data.get(\"Excluded Keywords\", pd.DataFrame())\n",
    "        excluded_keywords = set(excluded_df['keyword'].dropna()) if not excluded_df.empty else set()\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Could not load excluded keywords: {e}\")\n",
    "        excluded_keywords = set()\n",
    "    \n",
    "    analyzer = KeywordAnalyzer(\n",
    "        config={\n",
    "            \"excluded_keywords\": excluded_keywords,\n",
    "            \"min_keyword_length\": 3,\n",
    "            \"max_keywords\": 8,\n",
    "            \"focus\": \"business metrics and performance\"\n",
    "        },\n",
    "        language_processor=language_processor\n",
    "    )\n",
    "    \n",
    "    print(\"=== Keyword Extraction Test ===\")\n",
    "    print(\"\\nOriginal text:\")\n",
    "    print(text)\n",
    "    \n",
    "    # Use the language processor directly for preprocessing steps\n",
    "    print(\"\\nProcessing Steps:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Show tokenization\n",
    "    tokens = language_processor.tokenize(text)\n",
    "    print(\"\\nTokens:\")\n",
    "    print(tokens)\n",
    "    \n",
    "    # Show filtered tokens\n",
    "    filtered = [word for word in tokens \n",
    "               if language_processor.should_keep_word(word)]\n",
    "    print(\"\\nFiltered tokens (after stopword removal):\")\n",
    "    print(filtered)\n",
    "    \n",
    "    # Show base forms\n",
    "    base_forms = [language_processor.get_base_form(word) for word in filtered]\n",
    "    print(\"\\nBase forms:\")\n",
    "    print(base_forms)\n",
    "    \n",
    "    # Run analysis\n",
    "    print(\"\\nRunning Analysis:\")\n",
    "    print(\"-\" * 50)\n",
    "    results = await analyzer.analyze(text)\n",
    "    \n",
    "    print(\"\\nExtracted Keywords:\")\n",
    "    if hasattr(results, \"keywords\"):\n",
    "        print(\"\\nKeywords:\", results.keywords)\n",
    "        print(\"\\nKeyword Scores:\", results.keyword_scores if hasattr(results, \"keyword_scores\") else {})\n",
    "        print(\"\\nCompound Words:\", results.compound_words if hasattr(results, \"compound_words\") else [])\n",
    "        print(\"\\nDomain Keywords:\", results.domain_keywords if hasattr(results, \"domain_keywords\") else {})\n",
    "    else:\n",
    "        print(\"No keywords found in results\")\n",
    "        pprint(results)\n",
    "    \n",
    "    return results\n",
    "\n",
    "async def run_tests():\n",
    "    \"\"\"Run all tests in sequence.\"\"\"\n",
    "    print(\"\\nVerifying environment and components...\")\n",
    "    if not verify_environment():\n",
    "        print(\"Environment verification failed!\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nVerifying stopwords and language processing...\")\n",
    "    verify_stopwords()\n",
    "    \n",
    "    print(\"\\nTesting keyword extraction...\")\n",
    "    results = await test_keyword_extraction()\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 21:12:45 - src.core.language_processing.factory - INFO - Using default configuration\n",
      "2024-11-10 21:12:45 - src.core.language_processing.english - INFO - Initialized English processor with 831 stopwords\n",
      "2024-11-10 21:12:45 - src.core.language_processing.english - INFO - Initialized English processor with 831 stopwords\n",
      "2024-11-10 21:12:45 - src.core.language_processing.factory - INFO - Using default configuration\n",
      "2024-11-10 21:12:45 - src.core.language_processing.finnish - INFO - Loaded 747 stopwords from /home/topi/data-science/repos/semantic-text-analyzer/data/configurations/stop_words/fi.txt\n",
      "2024-11-10 21:12:45 - src.core.language_processing.finnish - INFO - Successfully initialized Voikko using system libraries\n",
      "2024-11-10 21:12:45 - src.core.language_processing.factory - INFO - Using default configuration\n",
      "2024-11-10 21:12:45 - src.core.language_processing.english - INFO - Initialized English processor with 831 stopwords\n",
      "2024-11-10 21:12:45 - src.core.language_processing.english - INFO - Initialized English processor with 831 stopwords\n",
      "2024-11-10 21:12:45 - src.utils.FileUtils.file_utils - INFO - Successfully loaded 3 sheets from /home/topi/data-science/repos/semantic-text-analyzer/data/configurations/example_params.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verifying environment and components...\n",
      "Environment Check Results:\n",
      "==================================================\n",
      "\n",
      "Basic Setup:\n",
      "-----------\n",
      "✓ Project root in path\n",
      "✓ Can import src\n",
      "✓ FileUtils initialized\n",
      "✓ .env file loaded\n",
      "\n",
      "Environment Variables:\n",
      "---------------------\n",
      "✓ OPENAI_API_KEY set\n",
      "✓ ANTHROPIC_API_KEY set\n",
      "\n",
      "Project Structure:\n",
      "-----------------\n",
      "✓ Raw data exists\n",
      "✓ Processed data exists\n",
      "✓ Configuration exists\n",
      "✓ Main config.yaml exists\n",
      "\n",
      "==================================================\n",
      "Environment Status: Ready ✓\n",
      "\n",
      "Verifying stopwords and language processing...\n",
      "\n",
      "Stopwords Directory Contents:\n",
      "Directory: /home/topi/data-science/repos/semantic-text-analyzer/data/configurations/stop_words\n",
      "\n",
      "File: fi.txt\n",
      "Number of words: 747\n",
      "Sample words (first 10):\n",
      "['aiemmin',\n",
      " 'aika',\n",
      " 'aikaa',\n",
      " 'aikaan',\n",
      " 'aikaisemmin',\n",
      " 'aikaisin',\n",
      " 'aikajen',\n",
      " 'aikana',\n",
      " 'aikoina',\n",
      " 'aikoo']\n",
      "\n",
      "File: en.txt\n",
      "Number of words: 733\n",
      "Sample words (first 10):\n",
      "['a',\n",
      " 'abaft',\n",
      " 'abafter',\n",
      " 'abaftest',\n",
      " 'about',\n",
      " 'abouter',\n",
      " 'aboutest',\n",
      " 'above',\n",
      " 'abover',\n",
      " 'abovest']\n",
      "\n",
      "File: stop-words-finnish.txt\n",
      "Number of words: 747\n",
      "Sample words (first 10):\n",
      "['aika',\n",
      " 'aikaa',\n",
      " 'aikaan',\n",
      " 'aikaisemmin',\n",
      " 'aikaisin',\n",
      " 'aikajen',\n",
      " 'aikana',\n",
      " 'aikoina',\n",
      " 'aikoo',\n",
      " 'aikovat']\n",
      "\n",
      "Testing Language Processors:\n",
      "\n",
      "EN Processor:\n",
      "Number of stopwords: 831\n",
      "Sample stopwords (first 10):\n",
      "[\"'d\", \"'ll\", \"'re\", \"'s\", \"'t\", \"'ve\", 'a', 'abaft', 'abafter', 'abaftest']\n",
      "\n",
      "Testing common en words:\n",
      "the: stop word\n",
      "and: stop word\n",
      "is: stop word\n",
      "that: stop word\n",
      "with: stop word\n",
      "for: stop word\n",
      "\n",
      "FI Processor:\n",
      "Number of stopwords: 774\n",
      "Sample stopwords (first 10):\n",
      "['aiemmin',\n",
      " 'aiempi',\n",
      " 'aika',\n",
      " 'aikaa',\n",
      " 'aikaan',\n",
      " 'aikaisemmin',\n",
      " 'aikaisin',\n",
      " 'aikajen',\n",
      " 'aikana',\n",
      " 'aikoina']\n",
      "\n",
      "Testing common fi words:\n",
      "ja: stop word\n",
      "on: stop word\n",
      "että: stop word\n",
      "joka: stop word\n",
      "tai: stop word\n",
      "vain: stop word\n",
      "\n",
      "Testing keyword extraction...\n",
      "=== Keyword Extraction Test ===\n",
      "\n",
      "Original text:\n",
      "\n",
      "                The company's Q3 results exceeded expectations with revenue growth of 15%.\n",
      "                Customer acquisition costs decreased while retention rates improved.\n",
      "                The board has approved a new strategic initiative focusing on expansion.\n",
      "            \n",
      "\n",
      "Processing Steps:\n",
      "--------------------------------------------------\n",
      "\n",
      "Tokens:\n",
      "['company', 'Q3', 'results', 'exceeded', 'expectations', 'revenue', 'growth', '15', 'Customer', 'acquisition', 'costs', 'decreased', 'retention', 'rates', 'improved', 'board', 'approved', 'strategic', 'initiative', 'focusing', 'expansion']\n",
      "\n",
      "Filtered tokens (after stopword removal):\n",
      "['company', 'results', 'exceeded', 'expectations', 'revenue', 'growth', 'Customer', 'acquisition', 'costs', 'decreased', 'retention', 'rates', 'improved', 'board', 'approved', 'strategic', 'initiative', 'focusing', 'expansion']\n",
      "\n",
      "Base forms:\n",
      "['company', 'result', 'exceed', 'expectation', 'revenue', 'growth', 'customer', 'acquisition', 'cost', 'decrease', 'retention', 'rate', 'improve', 'board', 'approve', 'strategic', 'initiative', 'focus', 'expansion']\n",
      "\n",
      "Running Analysis:\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 21:12:55 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Keywords:\n",
      "\n",
      "Keywords: ['expansion', 'Q3 results', 'revenue growth', 'customer acquisition', 'retention rates']\n",
      "\n",
      "Keyword Scores: {'expansion': 0.5682605452846603, 'Q3 results': 0.54, 'revenue growth': 0.54, 'customer acquisition': 0.48, 'retention rates': 0.48}\n",
      "\n",
      "Compound Words: ['customer acquisition', 'retention rates', 'strategic initiative']\n",
      "\n",
      "Domain Keywords: {'business_metrics': ['revenue growth', 'customer acquisition', 'retention rates', 'strategic initiative']}\n"
     ]
    }
   ],
   "source": [
    "# In notebook, you can now run:\n",
    "results = await run_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 21:12:56 - src.core.language_processing.factory - INFO - Using default configuration\n",
      "2024-11-10 21:12:56 - src.core.language_processing.english - INFO - Initialized English processor with 831 stopwords\n",
      "2024-11-10 21:12:56 - src.core.language_processing.english - INFO - Initialized English processor with 831 stopwords\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing technical text:\n",
      "======================================================================\n",
      "\n",
      "Testing Statistical Analysis for technical:\n",
      "==================================================\n",
      "Text: \n",
      "                Python is a high-level programming language known for its simplicity.\n",
      "             ...\n",
      "\n",
      "Tokens:\n",
      "['Python', 'high-level', 'programming', 'language', 'known', 'simplicity', 'supports', 'multiple', 'programming', 'paradigms', 'including', 'procedural', 'object-oriented', 'programming']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 21:12:58 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-10 21:12:58 - src.core.language_processing.factory - INFO - Using default configuration\n",
      "2024-11-10 21:12:58 - src.core.language_processing.english - INFO - Initialized English processor with 831 stopwords\n",
      "2024-11-10 21:12:58 - src.core.language_processing.english - INFO - Initialized English processor with 831 stopwords\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statistical Keywords:\n",
      "- program\n",
      "- high-level\n",
      "- simplicity\n",
      "- language\n",
      "\n",
      "Testing LLM Analysis for technical:\n",
      "==================================================\n",
      "Text: \n",
      "                Python is a high-level programming language known for its simplicity.\n",
      "             ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 21:13:00 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-10 21:13:00 - src.core.language_processing.factory - INFO - Using default configuration\n",
      "2024-11-10 21:13:00 - src.core.language_processing.english - INFO - Initialized English processor with 831 stopwords\n",
      "2024-11-10 21:13:00 - src.core.language_processing.english - INFO - Initialized English processor with 831 stopwords\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LLM Keywords:\n",
      "- Python\n",
      "- programming\n",
      "- language\n",
      "- high-level\n",
      "\n",
      "Testing Combined Analysis for technical:\n",
      "==================================================\n",
      "Text: \n",
      "                Python is a high-level programming language known for its simplicity.\n",
      "             ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 21:13:03 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combined Keywords:\n",
      "- language\n",
      "- high-level\n",
      "- simplicity\n",
      "- Python\n",
      "\n",
      "Results Comparison:\n",
      "--------------------------------------------------\n",
      "Statistical: ['program', 'high-level', 'simplicity', 'language']\n",
      "LLM: ['Python', 'programming', 'language', 'high-level']\n",
      "Combined: ['language', 'high-level', 'simplicity', 'Python']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(KeywordOutput(language='en', error=None, success=True, keywords=['program', 'high-level', 'simplicity', 'language'], keyword_scores={'program': 1.0, 'high-level': 0.8236741949136424, 'simplicity': 0.8236741949136424, 'language': 0.7547440479624554}, compound_words=['high-level', 'object-oriented', 'procedural'], domain_keywords={'programming': ['Python', 'programming', 'language', 'high-level', 'simplicity', 'object-oriented', 'procedural']}),\n",
       " KeywordOutput(language='en', error=None, success=True, keywords=['Python', 'programming', 'language', 'high-level'], keyword_scores={'Python': 0.9, 'programming': 0.85, 'language': 0.8, 'high-level': 0.75}, compound_words=['high-level', 'object-oriented'], domain_keywords={'programming': ['Python', 'programming', 'language', 'high-level', 'simplicity', 'object-oriented', 'procedural']}),\n",
       " KeywordOutput(language='en', error=None, success=True, keywords=['language', 'high-level', 'simplicity', 'Python'], keyword_scores={'language': 0.7818976191849822, 'high-level': 0.7794696779654569, 'simplicity': 0.7494696779654569, 'Python': 0.54}, compound_words=['high-level', 'object-oriented'], domain_keywords={'programming': ['Python', 'programming', 'procedural', 'object-oriented'], 'languages': ['language', 'high-level']}))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test technical text\n",
    "await test_text(\"technical\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 20:06:59 - src.core.language_processing.factory - INFO - Using default configuration\n",
      "2024-11-10 20:06:59 - src.core.language_processing.english - INFO - Initialized English processor with 831 stopwords\n",
      "2024-11-10 20:06:59 - src.core.language_processing.english - INFO - Initialized English processor with 831 stopwords\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing business text:\n",
      "======================================================================\n",
      "\n",
      "Testing Statistical Analysis for business:\n",
      "==================================================\n",
      "Text: \n",
      "                The company's Q3 results exceeded expectations with revenue growth of 15%.\n",
      "        ...\n",
      "\n",
      "Tokens:\n",
      "['company', 'Q3', 'results', 'exceeded', 'expectations', 'revenue', 'growth', '15', 'Customer', 'acquisition', 'costs', 'decreased', 'retention', 'rates', 'improved', 'board', 'approved', 'strategic', 'initiative', 'focusing', 'expansion']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 20:07:01 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-10 20:07:01 - src.core.language_processing.factory - INFO - Using default configuration\n",
      "2024-11-10 20:07:01 - src.core.language_processing.english - INFO - Initialized English processor with 831 stopwords\n",
      "2024-11-10 20:07:01 - src.core.language_processing.english - INFO - Initialized English processor with 831 stopwords\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statistical Keywords:\n",
      "- expectation\n",
      "- company\n",
      "- revenue\n",
      "- result\n",
      "- exceed\n",
      "\n",
      "Testing LLM Analysis for business:\n",
      "==================================================\n",
      "Text: \n",
      "                The company's Q3 results exceeded expectations with revenue growth of 15%.\n",
      "        ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 20:07:03 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-10 20:07:03 - src.core.language_processing.factory - INFO - Using default configuration\n",
      "2024-11-10 20:07:03 - src.core.language_processing.english - INFO - Initialized English processor with 831 stopwords\n",
      "2024-11-10 20:07:03 - src.core.language_processing.english - INFO - Initialized English processor with 831 stopwords\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LLM Keywords:\n",
      "- Q3 results\n",
      "- revenue growth\n",
      "- customer acquisition\n",
      "- strategic initiative\n",
      "- retention rates\n",
      "\n",
      "Testing Combined Analysis for business:\n",
      "==================================================\n",
      "Text: \n",
      "                The company's Q3 results exceeded expectations with revenue growth of 15%.\n",
      "        ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 20:07:06 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combined Keywords:\n",
      "- revenue\n",
      "- growth\n",
      "- retention\n",
      "- customer\n",
      "- acquisition\n",
      "\n",
      "Results Comparison:\n",
      "--------------------------------------------------\n",
      "Statistical: ['expectation', 'company', 'revenue', 'result', 'exceed']\n",
      "LLM: ['Q3 results', 'revenue growth', 'customer acquisition', 'strategic initiative', 'retention rates']\n",
      "Combined: ['revenue', 'growth', 'retention', 'customer', 'acquisition']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(KeywordOutput(language='en', error=None, success=True, keywords=['expectation', 'company', 'revenue', 'result', 'exceed'], keyword_scores={'expectation': 1.0, 'company': 0.8368288369533894, 'revenue': 0.8368288369533894, 'result': 0.783091851446946, 'exceed': 0.783091851446946}, compound_words=['customer acquisition', 'strategic initiative'], domain_keywords={'business': ['revenue', 'growth', 'acquisition', 'retention', 'expansion']}),\n",
       " KeywordOutput(language='en', error=None, success=True, keywords=['Q3 results', 'revenue growth', 'customer acquisition', 'strategic initiative', 'retention rates'], keyword_scores={'Q3 results': 0.9, 'revenue growth': 0.85, 'customer acquisition': 0.8, 'strategic initiative': 0.8, 'retention rates': 0.75}, compound_words=['customer acquisition', 'strategic initiative'], domain_keywords={'business': ['revenue', 'growth', 'acquisition', 'retention', 'expansion']}),\n",
       " KeywordOutput(language='en', error=None, success=True, keywords=['revenue', 'growth', 'retention', 'customer', 'acquisition'], keyword_scores={'revenue': 0.8747315347813558, 'growth': 0.7632367405787783, 'retention': 0.5867931134452429, 'customer': 0.5791610791311865, 'acquisition': 0.5700000000000001}, compound_words=['customer acquisition', 'retention rates', 'strategic initiative'], domain_keywords={'business': ['revenue', 'growth', 'customer acquisition', 'retention']}))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test business text\n",
    "await test_text(\"business\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 16:26:02 - src.core.language_processing.factory - INFO - Using default configuration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing finnish text:\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 16:26:02 - src.core.language_processing.finnish - INFO - Loaded 747 stopwords from /home/topi/data-science/repos/semantic-text-analyzer/data/configurations/stop_words/fi.txt\n",
      "2024-11-10 16:26:02 - src.core.language_processing.finnish - INFO - Successfully initialized Voikko using system libraries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Statistical Analysis for finnish:\n",
      "==================================================\n",
      "Text: \n",
      "                Ohjelmistokehittäjä työskentelee asiakasprojektissa kehittäen uusia \n",
      "              ...\n",
      "\n",
      "Tokens:\n",
      "['Ohjelmistokehittäjä', 'työskentelee', 'asiakasprojektissa', 'kehittäen', 'uusia', 'ominaisuuksia', 'verkkokauppajärjestelmään', 'Tekninen', 'toteutus', 'vaatii', 'erityistä', 'huomiota', 'tietoturvan', 'osalta']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 16:26:06 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-10 16:26:06 - src.core.language_processing.factory - INFO - Using default configuration\n",
      "2024-11-10 16:26:06 - src.core.language_processing.finnish - INFO - Loaded 747 stopwords from /home/topi/data-science/repos/semantic-text-analyzer/data/configurations/stop_words/fi.txt\n",
      "2024-11-10 16:26:06 - src.core.language_processing.finnish - INFO - Successfully initialized Voikko using system libraries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statistical Keywords:\n",
      "- ohjelmistokehittäjä\n",
      "- työskennellä\n",
      "- asiakasprojekti\n",
      "- kehittää\n",
      "- uusi\n",
      "- ominaisuus\n",
      "- verkkokauppajärjestelmä\n",
      "- tekninen\n",
      "\n",
      "Testing LLM Analysis for finnish:\n",
      "==================================================\n",
      "Text: \n",
      "                Ohjelmistokehittäjä työskentelee asiakasprojektissa kehittäen uusia \n",
      "              ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 16:26:09 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-11-10 16:26:09 - src.core.language_processing.factory - INFO - Using default configuration\n",
      "2024-11-10 16:26:09 - src.core.language_processing.finnish - INFO - Loaded 747 stopwords from /home/topi/data-science/repos/semantic-text-analyzer/data/configurations/stop_words/fi.txt\n",
      "2024-11-10 16:26:09 - src.core.language_processing.finnish - INFO - Successfully initialized Voikko using system libraries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LLM Keywords:\n",
      "- ohjelmistokehittäjä\n",
      "- työskennellä\n",
      "- asiakasprojekti\n",
      "- kehittää\n",
      "- uusi\n",
      "- ominaisuus\n",
      "- verkkokauppajärjestelmä\n",
      "- tekninen\n",
      "\n",
      "Testing Combined Analysis for finnish:\n",
      "==================================================\n",
      "Text: \n",
      "                Ohjelmistokehittäjä työskentelee asiakasprojektissa kehittäen uusia \n",
      "              ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-10 16:26:12 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combined Keywords:\n",
      "- ohjelmistokehittäjä\n",
      "- työskennellä\n",
      "- asiakasprojekti\n",
      "- kehittää\n",
      "- uusi\n",
      "- ominaisuus\n",
      "- verkkokauppajärjestelmä\n",
      "- tekninen\n",
      "- toteutus\n",
      "- vaatia\n",
      "\n",
      "Results Comparison:\n",
      "--------------------------------------------------\n",
      "Statistical: ['ohjelmistokehittäjä', 'työskennellä', 'asiakasprojekti', 'kehittää', 'uusi', 'ominaisuus', 'verkkokauppajärjestelmä', 'tekninen']\n",
      "LLM: ['ohjelmistokehittäjä', 'työskennellä', 'asiakasprojekti', 'kehittää', 'uusi', 'ominaisuus', 'verkkokauppajärjestelmä', 'tekninen']\n",
      "Combined: ['ohjelmistokehittäjä', 'työskennellä', 'asiakasprojekti', 'kehittää', 'uusi', 'ominaisuus', 'verkkokauppajärjestelmä', 'tekninen', 'toteutus', 'vaatia']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(KeywordOutput(language='fi', error=None, success=True, keywords=['ohjelmistokehittäjä', 'työskennellä', 'asiakasprojekti', 'kehittää', 'uusi', 'ominaisuus', 'verkkokauppajärjestelmä', 'tekninen'], keyword_scores={'ohjelmistokehittäjä': 1.0, 'työskennellä': 1.0, 'asiakasprojekti': 1.0, 'kehittää': 1.0, 'uusi': 1.0, 'ominaisuus': 1.0, 'verkkokauppajärjestelmä': 1.0, 'tekninen': 1.0, 'toteutus': 1.0, 'vaatia': 1.0, 'eritty': 1.0, 'huomio': 1.0, 'tietoturpa': 1.0, 'osa': 1.0, 'tietoturva': 0.0}, compound_words=['verkkokauppajärjestelmä'], domain_keywords={'software_development': ['ohjelmistokehittäjä', 'kehittää', 'toteutus', 'tietoturva']}),\n",
       " KeywordOutput(language='fi', error=None, success=True, keywords=['ohjelmistokehittäjä', 'työskennellä', 'asiakasprojekti', 'kehittää', 'uusi', 'ominaisuus', 'verkkokauppajärjestelmä', 'tekninen'], keyword_scores={'ohjelmistokehittäjä': 0.9, 'työskennellä': 0.0, 'asiakasprojekti': 0.85, 'kehittää': 0.75, 'uusi': 0.0, 'ominaisuus': 0.7, 'verkkokauppajärjestelmä': 0.8, 'tekninen': 0.0, 'toteutus': 0.65, 'vaatia': 0.0, 'eritty': 0.0, 'huomio': 0.0, 'tietoturpa': 0.0, 'osa': 0.0, 'tietoturva': 0.6}, compound_words=['verkkokauppajärjestelmä'], domain_keywords={'software_development': ['ohjelmistokehittäjä', 'kehittää', 'toteutus', 'tietoturva']}),\n",
       " KeywordOutput(language='fi', error=None, success=True, keywords=['ohjelmistokehittäjä', 'työskennellä', 'asiakasprojekti', 'kehittää', 'uusi', 'ominaisuus', 'verkkokauppajärjestelmä', 'tekninen', 'toteutus', 'vaatia'], keyword_scores={'ohjelmistokehittäjä': 0.9400000000000001, 'työskennellä': 0.4, 'asiakasprojekti': 0.91, 'kehittää': 0.85, 'uusi': 0.4, 'ominaisuus': 0.8200000000000001, 'verkkokauppajärjestelmä': 0.88, 'tekninen': 0.4, 'toteutus': 0.79, 'vaatia': 0.4, 'eritty': 0.4, 'huomio': 0.4, 'tietoturpa': 0.4, 'osa': 0.4, 'tietoturva': 0.36}, compound_words=['verkkokauppajärjestelmä'], domain_keywords={'ohjelmistokehitys': ['ohjelmistokehittäjä', 'kehittää', 'toteutus', 'tietoturva']}))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Finnish text\n",
    "await test_text(\"finnish\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semantic-analyzer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
