{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Import required modules\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any, Tuple, Union\n",
        "import logging\n",
        "import asyncio\n",
        "\n",
        "# Add project root to Python path if needed\n",
        "project_root = str(Path().resolve().parent)\n",
        "if project_root not in sys.path:\n",
        "    sys.path.append(project_root)\n",
        "\n",
        "# Import necessary components\n",
        "# from src.nb_helpers.environment import setup_notebook_env, verify_environment\n",
        "from src.semantic_analyzer import SemanticAnalyzer\n",
        "from src.core.config import AnalyzerConfig\n",
        "\n",
        "from src.core.language_processing import create_text_processor\n",
        "from src.core.llm.factory import create_llm\n",
        "from src.loaders.parameter_handler import ParameterHandler\n",
        "from src.analyzers.keyword_analyzer import KeywordAnalyzer\n",
        "from src.analyzers.theme_analyzer import ThemeAnalyzer\n",
        "from src.analyzers.category_analyzer import CategoryAnalyzer\n",
        "\n",
        "import FileUtils\n"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1733728391942
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from scripts.azure.setup_helpers import setup_voikko\n",
        "\n",
        "from src.az_helpers.setup_azure import init_azure_ml\n",
        "from src.az_helpers.az_environment import setup_notebook_env, verify_environment\n",
        "\n",
        "# Set up environment and logging\n",
        "setup_notebook_env(log_level=\"DEBUG\")\n",
        "verify_environment()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "2024-12-09 07:13:19,443 - FileUtils.core.file_utils - INFO - Project root: /mnt/batch/tasks/shared/LS_root/mounts/clusters/basic-cpu/code/Users/topi.jarvinen/semantic-text-analyzer\n2024-12-09 07:13:19,735 - FileUtils.core.file_utils - INFO - FileUtils initialized with local storage\n2024-12-09 07:13:20,004 - FileUtils.core.file_utils - INFO - Project root: /mnt/batch/tasks/shared/LS_root/mounts/clusters/basic-cpu/code/Users/topi.jarvinen/semantic-text-analyzer\n2024-12-09 07:13:20,013 - FileUtils.core.file_utils - INFO - FileUtils initialized with local storage\nEnvironment Check Results:\n==================================================\n✓ Project root in path\n✓ FileUtils initialized\n✓ .env file loaded\n✓ OPENAI_API_KEY set\n✓ ANTHROPIC_API_KEY set\n✓ Raw data exists\n✓ Processed data exists\n✓ Configuration exists\n✓ Main config.yaml exists\n\n==================================================\nEnvironment Status: Ready ✓\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 2,
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733728401136
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Reset logging configuration for Voikko setup\n",
        "# logging.getLogger().setLevel(logging.DEBUG)\n",
        "# for handler in logging.getLogger().handlers:\n",
        "#     handler.setLevel(logging.DEBUG)\n",
        "\n",
        "# # Also set debug level specifically for the setup helper logger\n",
        "# logging.getLogger(\"scripts.azure.setup_helpers\").setLevel(logging.DEBUG)\n",
        "\n",
        "# from scripts.azure.setup_helpers import setup_voikko\n",
        "\n",
        "# # Run setup with explicit project root\n",
        "# if setup_voikko(project_root=project_root):\n",
        "#     print(\"Voikko ready\")\n",
        "# else:\n",
        "#     print(\"Using fallback mode\")"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733728401723
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # Run setup with explicit project root\n",
        "# if setup_voikko(project_root=project_root):\n",
        "#     print(\"Voikko ready\")\n",
        "# else:\n",
        "#     print(\"Using fallback mode\")"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733728402359
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In analyzer_demo_local_nb.ipynb and azure_notebook.ipynb\n",
        "# from src.core.llm.factory import create_llm\n",
        "\n",
        "# Setup\n",
        "# config = AnalyzerConfig()\n",
        "# llm = create_llm(config=config)\n",
        "# analyzer = SemanticAnalyzer(llm=llm)"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1733728402782
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(FileUtils.__version__)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "0.5.3\n"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1733728402981
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up environment and logging\n",
        "setup_notebook_env(log_level=\"DEBUG\")\n",
        "verify_environment()\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "2024-12-09 07:13:21,577 - FileUtils.core.file_utils - INFO - Project root: /mnt/batch/tasks/shared/LS_root/mounts/clusters/basic-cpu/code/Users/topi.jarvinen/semantic-text-analyzer\n2024-12-09 07:13:21,578 - FileUtils.core.file_utils - INFO - FileUtils initialized with local storage\n2024-12-09 07:13:21,870 - FileUtils.core.file_utils - INFO - Project root: /mnt/batch/tasks/shared/LS_root/mounts/clusters/basic-cpu/code/Users/topi.jarvinen/semantic-text-analyzer\n2024-12-09 07:13:21,946 - FileUtils.core.file_utils - INFO - FileUtils initialized with local storage\nEnvironment Check Results:\n==================================================\n✓ Project root in path\n✓ FileUtils initialized\n✓ .env file loaded\n✓ OPENAI_API_KEY set\n✓ ANTHROPIC_API_KEY set\n✓ Raw data exists\n✓ Processed data exists\n✓ Configuration exists\n✓ Main config.yaml exists\n\n==================================================\nEnvironment Status: Ready ✓\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1733728403256
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test data to use\n",
        "test_texts = {\n",
        "    \"en\": {\n",
        "        \"technical\": \"\"\"Machine learning models are trained using large datasets to recognize patterns. \n",
        "                     The neural network architecture includes multiple layers for feature extraction. \n",
        "                     Data preprocessing and feature engineering are crucial steps.\"\"\",\n",
        "        \"business\": \"\"\"Q3 financial results show 15% revenue growth and improved profit margins. \n",
        "                    Customer acquisition costs decreased while retention rates increased. \n",
        "                    Market expansion strategy focuses on emerging technology sectors.\"\"\"\n",
        "    },\n",
        "    \"fi\": {\n",
        "        \"technical\": \"\"\"Koneoppimismalleja koulutetaan suurilla datajoukolla tunnistamaan kaavoja. \n",
        "                     Neuroverkon arkkitehtuuri sisältää useita kerroksia piirteiden erottamiseen. \n",
        "                     Datan esikäsittely ja piirteiden suunnittelu ovat keskeisiä vaiheita.\"\"\",\n",
        "        \"business\": \"\"\"Q3 taloudelliset tulokset osoittavat 15% liikevaihdon kasvun ja parantuneet katteet. \n",
        "                    Asiakashankinnan kustannukset laskivat ja asiakaspysyvyys parani. \n",
        "                    Markkinalaajennusstrategia keskittyy nouseviin teknologiasektoreihin.\"\"\"\n",
        "    }\n",
        "}\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1733728403446
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_texts['en']['technical']"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": "'Machine learning models are trained using large datasets to recognize patterns. \\n                     The neural network architecture includes multiple layers for feature extraction. \\n                     Data preprocessing and feature engineering are crucial steps.'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733728403659
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "     # llm = create_llm()\n",
        "config = AnalyzerConfig()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "2024-12-09 07:13:22,723 - FileUtils.core.file_utils - INFO - Project root: /mnt/batch/tasks/shared/LS_root/mounts/clusters/basic-cpu/code/Users/topi.jarvinen/semantic-text-analyzer\n2024-12-09 07:13:22,724 - FileUtils.core.file_utils - INFO - FileUtils initialized with local storage\n"
        }
      ],
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1733728403888
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async def test_individual_analyzer(\n",
        "    analyzer: Union[KeywordAnalyzer, ThemeAnalyzer, CategoryAnalyzer], \n",
        "    text: str, \n",
        "    analyzer_type: str,\n",
        "    provider: str = 'openai'\n",
        "):\n",
        "    \"\"\"Test individual analyzer component.\"\"\"\n",
        "    print(f\"\\nTesting {analyzer_type} Analysis\")\n",
        "    print(\"=\" * 50)\n",
        "    print(\"\\nInput text:\")\n",
        "    print(text[:200] + \"...\" if len(text) > 200 else text)\n",
        "    \n",
        "    try:\n",
        "        results = await analyzer.analyze(text)\n",
        "        \n",
        "        print(\"\\nResults:\")\n",
        "        print(\"-\" * 20)\n",
        "        \n",
        "        if isinstance(analyzer, KeywordAnalyzer):\n",
        "            if results.keywords:\n",
        "                print(\"\\nKeywords:\")\n",
        "                for kw in results.keywords[:10]:  # Show top 10\n",
        "                    print(f\"• {kw.keyword:<20} ({kw.score:.2f})\")\n",
        "                    if kw.domain:\n",
        "                        print(f\"  Domain: {kw.domain}\")\n",
        "                \n",
        "                if results.compound_words:\n",
        "                    print(\"\\nCompound Words:\")\n",
        "                    print(\", \".join(results.compound_words))\n",
        "                    \n",
        "                if results.domain_keywords:\n",
        "                    print(\"\\nKeywords by Domain:\")\n",
        "                    for domain, kws in results.domain_keywords.items():\n",
        "                        print(f\"\\n{domain}:\")\n",
        "                        print(\", \".join(kws))\n",
        "                        \n",
        "        elif isinstance(analyzer, ThemeAnalyzer):\n",
        "            if results.themes:\n",
        "                print(\"\\nThemes:\")\n",
        "                for theme in results.themes:\n",
        "                    print(f\"\\n• {theme.name}\")\n",
        "                    print(f\"  Confidence: {theme.confidence:.2f}\")\n",
        "                    print(f\"  Description: {theme.description}\")\n",
        "                    if theme.keywords:\n",
        "                        print(f\"  Keywords: {', '.join(theme.keywords)}\")\n",
        "                \n",
        "                if results.theme_hierarchy:\n",
        "                    print(\"\\nTheme Hierarchy:\")\n",
        "                    for parent, children in results.theme_hierarchy.items():\n",
        "                        print(f\"{parent} -> {', '.join(children)}\")\n",
        "                        \n",
        "        elif isinstance(analyzer, CategoryAnalyzer):\n",
        "            if results.categories:\n",
        "                print(\"\\nCategories:\")\n",
        "                for cat in results.categories:\n",
        "                    print(f\"\\n• {cat.name}\")\n",
        "                    print(f\"  Confidence: {cat.confidence:.2f}\")\n",
        "                    if cat.description:\n",
        "                        print(f\"  Description: {cat.description}\")\n",
        "                    if cat.evidence:\n",
        "                        print(\"\\n  Evidence:\")\n",
        "                        for ev in cat.evidence:\n",
        "                            print(f\"  - {ev.text} (relevance: {ev.relevance:.2f})\")\n",
        "                            \n",
        "        if hasattr(results, 'error') and results.error:\n",
        "            print(f\"\\nErrors occurred: {results.error}\")\n",
        "            \n",
        "        return results\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"\\nError in analysis: {e}\")\n",
        "        return None"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1733728404099
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: Test individual keyword analyzer\n",
        "async def test_keyword_analyzer(provider: str = \"openai\"):\n",
        "    \"\"\"Test keyword analyzer with different languages.\"\"\"\n",
        "    print(\"Testing Keyword Analyzer\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Initialize components\n",
        "    parameter_handler = ParameterHandler(\"parameters_fi.xlsx\")\n",
        "    # llm = create_llm()\n",
        "    # config = AnalyzerConfig()\n",
        "    llm = create_llm(provider=provider, config=config)\n",
        "    # analyzer = SemanticAnalyzer(llm=llm)\n",
        "    \n",
        "    # Test English\n",
        "    print(\"\\nTesting English Technical Content:\")\n",
        "    en_processor = create_text_processor(language=\"en\")\n",
        "    keyword_analyzer_en = KeywordAnalyzer(\n",
        "        llm=llm,\n",
        "        config=parameter_handler.parameters.general.model_dump(),\n",
        "        language_processor=en_processor\n",
        "    )\n",
        "    await test_individual_analyzer(keyword_analyzer_en, test_texts[\"en\"][\"technical\"], \"Keyword\")\n",
        "    \n",
        "    # Test Finnish\n",
        "    print(\"\\nTesting Finnish Technical Content:\")\n",
        "    fi_processor = create_text_processor(language=\"fi\")\n",
        "    keyword_analyzer_fi = KeywordAnalyzer(\n",
        "        llm=llm,\n",
        "        config=parameter_handler.parameters.general.model_dump(),\n",
        "        language_processor=fi_processor\n",
        "    )\n",
        "    await test_individual_analyzer(keyword_analyzer_fi, test_texts[\"fi\"][\"technical\"], \"Keyword\")\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1733728404295
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2: Test all components\n",
        "async def test_components_for_language(language: str, provider: str = \"openai\"):\n",
        "    \"\"\"Test all components for a specific language.\"\"\"\n",
        "    print(f\"\\nTesting All Components for {language.upper()}\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Initialize components\n",
        "    parameter_handler = ParameterHandler(f\"parameters_{language}.xlsx\")\n",
        "     # llm = create_llm()\n",
        "    # config = AnalyzerConfig()\n",
        "    llm = create_llm(provider=provider, config=config)\n",
        "\n",
        "    language_processor = create_text_processor(language=language)\n",
        "    \n",
        "    # Create analyzers\n",
        "    keyword_analyzer = KeywordAnalyzer(\n",
        "        llm=llm,\n",
        "        config=parameter_handler.parameters.general.model_dump(),\n",
        "        language_processor=language_processor\n",
        "    )\n",
        "    \n",
        "    theme_analyzer = ThemeAnalyzer(\n",
        "        llm=llm,\n",
        "        config=parameter_handler.parameters.general.model_dump(),\n",
        "        language_processor=language_processor\n",
        "    )\n",
        "    \n",
        "    category_analyzer = CategoryAnalyzer(\n",
        "        categories=parameter_handler.parameters.categories,\n",
        "        llm=llm,\n",
        "        config=parameter_handler.parameters.general.model_dump(),\n",
        "        language_processor=language_processor\n",
        "    )\n",
        "    \n",
        "    # Test technical content\n",
        "    print(f\"\\nTesting {language.upper()} Technical Content:\")\n",
        "    await test_individual_analyzer(keyword_analyzer, test_texts[language][\"technical\"], \"Keyword\")\n",
        "    await test_individual_analyzer(theme_analyzer, test_texts[language][\"technical\"], \"Theme\")\n",
        "    await test_individual_analyzer(category_analyzer, test_texts[language][\"technical\"], \"Category\")\n",
        "    \n",
        "    # Test business content\n",
        "    print(f\"\\nTesting {language.upper()} Business Content:\")\n",
        "    await test_individual_analyzer(keyword_analyzer, test_texts[language][\"business\"], \"Keyword\")\n",
        "    await test_individual_analyzer(theme_analyzer, test_texts[language][\"business\"], \"Theme\")\n",
        "    await test_individual_analyzer(category_analyzer, test_texts[language][\"business\"], \"Category\")\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1733728404495
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3: Quick test of full pipeline\n",
        "async def test_pipeline(provider='openai'):\n",
        "    \"\"\"Test full pipeline with both languages.\"\"\"\n",
        "    print(\"Testing Full Pipeline\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    llm = create_llm(provider=provider, config=config)\n",
        "    # analyzer = SemanticAnalyzer(llm=llm)\n",
        "    # Test English pipeline\n",
        "    print(\"\\nEnglish Pipeline:\")\n",
        "    en_analyzer = SemanticAnalyzer(llm=llm, parameter_file=\"parameters_en.xlsx\")\n",
        "    result = await en_analyzer.analyze(test_texts[\"en\"][\"technical\"])\n",
        "    print(f\"Success: {result.success}\")\n",
        "    print(f\"Keywords found: {len(result.keywords.keywords)}\")\n",
        "    print(f\"Themes found: {len(result.themes.themes)}\")\n",
        "    print(f\"Categories found: {len(result.categories.matches)}\")\n",
        "    \n",
        "    # Test Finnish pipeline\n",
        "    print(\"\\nFinnish Pipeline:\")\n",
        "    fi_analyzer = SemanticAnalyzer(llm=llm, parameter_file=\"parameters_fi.xlsx\")\n",
        "    result = await fi_analyzer.analyze(test_texts[\"fi\"][\"technical\"])\n",
        "    print(f\"Success: {result.success}\")\n",
        "    print(f\"Keywords found: {len(result.keywords.keywords)}\")\n",
        "    print(f\"Themes found: {len(result.themes.themes)}\")\n",
        "    print(f\"Categories found: {len(result.categories.matches)}\")\n",
        "\n",
        "# Run the tests\n",
        "async def run_all_tests(provider='openai'):\n",
        "    \"\"\"Run all tests.\"\"\"\n",
        "    # Test individual component\n",
        "    await test_keyword_analyzer(provider=provider)\n",
        "    \n",
        "    # Test all components by language\n",
        "    await test_components_for_language(language=\"en\", provider=provider)\n",
        "    await test_components_for_language(language=\"fi\", provider=provider)\n",
        "    \n",
        "    # Test full pipeline\n",
        "    await test_pipeline(provider=provider)\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1733728404784
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import libvoikko"
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733728405077
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# help(libvoikko)"
      ],
      "outputs": [],
      "execution_count": 16,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733728405279
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dir(libvoikko)"
      ],
      "outputs": [],
      "execution_count": 17,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733728405489
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from pathlib import Path\n",
        "# scripts_dir = Path.cwd() / \"scripts\" / \"azure\"\n",
        "# print(f\"Looking for script in: {scripts_dir}\")\n",
        "# print(f\"Script exists: {(scripts_dir / 'setup_voikko.sh').exists()}\")"
      ],
      "outputs": [],
      "execution_count": 18,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733728405686
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import logging\n",
        "# from pathlib import Path\n",
        "\n",
        "# # Configure detailed logging\n",
        "# logging.basicConfig(\n",
        "#     level=logging.DEBUG,\n",
        "#     format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        "# )\n",
        "\n",
        "# # Get project root (you already have this)\n",
        "# project_root = Path().resolve().parent\n",
        "\n",
        "# from scripts.azure.setup_helpers import setup_voikko\n",
        "\n",
        "# # Run setup with explicit project root\n",
        "# if setup_voikko(project_root=project_root):\n",
        "#     print(\"Voikko ready\")\n",
        "# else:\n",
        "#     print(\"Using fallback mode\")"
      ],
      "outputs": [],
      "execution_count": 19,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733728405879
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import logging\n",
        "\n",
        "# # Configure detailed logging\n",
        "# logging.basicConfig(\n",
        "#     level=logging.DEBUG,\n",
        "#     format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        "# )\n",
        "\n",
        "# from scripts.azure.setup_helpers import setup_voikko\n",
        "\n",
        "# # Run setup with debug output\n",
        "# if setup_voikko():\n",
        "#     print(\"Voikko ready\")\n",
        "# else:\n",
        "#     print(\"Using fallback mode\")"
      ],
      "outputs": [],
      "execution_count": 20,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733728406072
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run in notebook\n",
        "# await run_all_tests()\n",
        "\n",
        "# Or run individual tests:\n",
        "# await test_keyword_analyzer(provider='azure')\n",
        "await test_components_for_language(\"fi\", provider='azure')\n",
        "# await test_pipeline()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\nTesting FI Technical Content:\n\nTesting Keyword Analysis\n==================================================\n\nInput text:\nKoneoppimismalleja koulutetaan suurilla datajoukolla tunnistamaan kaavoja. \n                     Neuroverkon arkkitehtuuri sisältää useita kerroksia piirteiden erottamiseen. \n                     Data...\n\nResults:\n--------------------\n\nKeywords:\n• koneoppimismalli     (0.95)\n  Domain: technical\n• datajoukko           (0.95)\n  Domain: technical\n• neuroverkon arkkitehtuuri (0.95)\n  Domain: technical\n• datan esikäsittely   (0.95)\n  Domain: technical\n• piirteiden suunnittelu (0.95)\n  Domain: technical\n• piirre               (0.90)\n  Domain: technical\n\nCompound Words:\nneuroverkon arkkitehtuuri, datan esikäsittely, piirteiden suunnittelu\n\nTesting Theme Analysis\n==================================================\n\nInput text:\nKoneoppimismalleja koulutetaan suurilla datajoukolla tunnistamaan kaavoja. \n                     Neuroverkon arkkitehtuuri sisältää useita kerroksia piirteiden erottamiseen. \n                     Data...\n\nRaw LLM response: {'themes': [{'name': 'Koneoppimismallien koulutus', 'description': 'Koneoppimismallien koulutus suurilla datajoukoilla on keskeinen prosessi, joka mahdollistaa kaavojen tunnistamisen.', 'confidence': 0.95, 'keywords': ['koneoppimismalli', 'datajoukko', 'kaava'], 'domain': 'general content analysis'}, {'name': 'Neuroverkon arkkitehtuuri', 'description': 'Neuroverkon arkkitehtuuri, joka koostuu useista kerroksista, on tärkeä piirteiden erottamisessa ja oppimisessa.', 'confidence': 0.9, 'keywords': ['neuroverkko', 'arkkitehtuuri', 'kerros'], 'domain': 'general content analysis', 'parent_theme': 'Koneoppimismallien koulutus'}, {'name': 'Datan esikäsittely ja piirteiden suunnittelu', 'description': 'Datan esikäsittely ja piirteiden suunnittelu ovat keskeisiä vaiheita koneoppimismallien tehokkuuden varmistamisessa.', 'confidence': 0.85, 'keywords': ['esikäsittely', 'suunnittelu', 'piirre'], 'domain': 'general content analysis', 'parent_theme': 'Koneoppimismallien koulutus'}], 'evidence': {'Koneoppimismallien koulutus': [{'text': 'Koneoppimismalleja koulutetaan suurilla datajoukolla tunnistamaan kaavoja.', 'relevance': 0.9, 'keywords': ['koneoppimismalli', 'datajoukko', 'kaava']}], 'Neuroverkon arkkitehtuuri': [{'text': 'Neuroverkon arkkitehtuuri sisältää useita kerroksia piirteiden erottamiseen.', 'relevance': 0.9, 'keywords': ['neuroverkko', 'arkkitehtuuri', 'kerros']}], 'Datan esikäsittely ja piirteiden suunnittelu': [{'text': 'Datan esikäsittely ja piirteiden suunnittelu ovat keskeisiä vaiheita.', 'relevance': 0.85, 'keywords': ['esikäsittely', 'suunnittelu', 'piirre']}]}, 'relationships': {'Koneoppimismallien koulutus': ['Neuroverkon arkkitehtuuri', 'Datan esikäsittely ja piirteiden suunnittelu'], 'Neuroverkon arkkitehtuuri': [], 'Datan esikäsittely ja piirteiden suunnittelu': []}}\n\nProcessed LLM response: {'themes': [{'name': 'Koneoppimismallien koulutus', 'description': 'Koneoppimismallien koulutus suurilla datajoukoilla on keskeinen prosessi, joka mahdollistaa kaavojen tunnistamisen.', 'confidence': 0.95, 'keywords': ['koneoppimismalli', 'datajoukko', 'kaava'], 'domain': 'general content analysis'}, {'name': 'Neuroverkon arkkitehtuuri', 'description': 'Neuroverkon arkkitehtuuri, joka koostuu useista kerroksista, on tärkeä piirteiden erottamisessa ja oppimisessa.', 'confidence': 0.9, 'keywords': ['neuroverkko', 'arkkitehtuuri', 'kerros'], 'domain': 'general content analysis', 'parent_theme': 'Koneoppimismallien koulutus'}, {'name': 'Datan esikäsittely ja piirteiden suunnittelu', 'description': 'Datan esikäsittely ja piirteiden suunnittelu ovat keskeisiä vaiheita koneoppimismallien tehokkuuden varmistamisessa.', 'confidence': 0.85, 'keywords': ['esikäsittely', 'suunnittelu', 'piirre'], 'domain': 'general content analysis', 'parent_theme': 'Koneoppimismallien koulutus'}], 'evidence': {'Koneoppimismallien koulutus': [{'text': 'Koneoppimismalleja koulutetaan suurilla datajoukolla tunnistamaan kaavoja.', 'relevance': 0.9, 'keywords': ['koneoppimismalli', 'datajoukko', 'kaava']}], 'Neuroverkon arkkitehtuuri': [{'text': 'Neuroverkon arkkitehtuuri sisältää useita kerroksia piirteiden erottamiseen.', 'relevance': 0.9, 'keywords': ['neuroverkko', 'arkkitehtuuri', 'kerros']}], 'Datan esikäsittely ja piirteiden suunnittelu': [{'text': 'Datan esikäsittely ja piirteiden suunnittelu ovat keskeisiä vaiheita.', 'relevance': 0.85, 'keywords': ['esikäsittely', 'suunnittelu', 'piirre']}]}, 'relationships': {'Koneoppimismallien koulutus': ['Neuroverkon arkkitehtuuri', 'Datan esikäsittely ja piirteiden suunnittelu'], 'Neuroverkon arkkitehtuuri': [], 'Datan esikäsittely ja piirteiden suunnittelu': []}}\n\nResults:\n--------------------\n\nThemes:\n\n• Koneoppimismallien koulutus\n  Confidence: 0.95\n  Description: Koneoppimismallien koulutus suurilla datajoukoilla on keskeinen prosessi, joka mahdollistaa kaavojen tunnistamisen.\n  Keywords: koneoppimismalli, datajoukko, kaava\n\n• Neuroverkon arkkitehtuuri\n  Confidence: 0.90\n  Description: Neuroverkon arkkitehtuuri, joka koostuu useista kerroksista, on tärkeä piirteiden erottamisessa ja oppimisessa.\n  Keywords: neuroverkko, arkkitehtuuri, kerros\n\n• Datan esikäsittely ja piirteiden suunnittelu\n  Confidence: 0.85\n  Description: Datan esikäsittely ja piirteiden suunnittelu ovat keskeisiä vaiheita koneoppimismallien tehokkuuden varmistamisessa.\n  Keywords: esikäsittely, suunnittelu, piirre\n\nTesting Category Analysis\n==================================================\n\nInput text:\nKoneoppimismalleja koulutetaan suurilla datajoukolla tunnistamaan kaavoja. \n                     Neuroverkon arkkitehtuuri sisältää useita kerroksia piirteiden erottamiseen. \n                     Data...\n\nProcessing response: {'categories': [{'category': 'Machine Learning', 'confidence': 0.95, 'explanation': 'The text discusses concepts directly related to machine learning, including model training, architecture, and data preprocessing.', 'evidence': [{'text': 'Koneoppimismalleja koulutetaan suurilla datajoukolla tunnistamaan kaavoja.', 'relevance': 0.9, 'matched_keywords': ['koneoppimismalli', 'datajoukko', 'kaava'], 'context': 'The sentence describes the training of machine learning models using large datasets to identify patterns.'}, {'text': 'Neuroverkon arkkitehtuuri sisältää useita kerroksia piirteiden erottamiseen.', 'relevance': 0.9, 'matched_keywords': ['neuroverkon', 'arkkitehtuuri', 'kerros', 'piirteiden'], 'context': 'This part explains the architecture of neural networks, which is a fundamental aspect of machine learning.'}, {'text': 'Datan esikäsittely ja piirteiden suunnittelu ovat keskeisiä vaiheita.', 'relevance': 0.9, 'matched_keywords': ['datan', 'esikäsittely', 'piirteiden', 'suunnittelu', 'keskeisiä'], 'context': 'It highlights the importance of data preprocessing and feature design, which are critical steps in machine learning.'}], 'themes': ['data science', 'artificial intelligence']}], 'relationships': {'Machine Learning': ['Data Science', 'Artificial Intelligence']}}\n\nProcessing category: {'category': 'Machine Learning', 'confidence': 0.95, 'explanation': 'The text discusses concepts directly related to machine learning, including model training, architecture, and data preprocessing.', 'evidence': [{'text': 'Koneoppimismalleja koulutetaan suurilla datajoukolla tunnistamaan kaavoja.', 'relevance': 0.9, 'matched_keywords': ['koneoppimismalli', 'datajoukko', 'kaava'], 'context': 'The sentence describes the training of machine learning models using large datasets to identify patterns.'}, {'text': 'Neuroverkon arkkitehtuuri sisältää useita kerroksia piirteiden erottamiseen.', 'relevance': 0.9, 'matched_keywords': ['neuroverkon', 'arkkitehtuuri', 'kerros', 'piirteiden'], 'context': 'This part explains the architecture of neural networks, which is a fundamental aspect of machine learning.'}, {'text': 'Datan esikäsittely ja piirteiden suunnittelu ovat keskeisiä vaiheita.', 'relevance': 0.9, 'matched_keywords': ['datan', 'esikäsittely', 'piirteiden', 'suunnittelu', 'keskeisiä'], 'context': 'It highlights the importance of data preprocessing and feature design, which are critical steps in machine learning.'}], 'themes': ['data science', 'artificial intelligence']}\nCreated category: name='Machine Learning' confidence=0.95 description='The text discusses concepts directly related to machine learning, including model training, architecture, and data preprocessing.' evidence=[Evidence(text='Koneoppimismalleja koulutetaan suurilla datajoukolla tunnistamaan kaavoja.', relevance=0.9), Evidence(text='Neuroverkon arkkitehtuuri sisältää useita kerroksia piirteiden erottamiseen.', relevance=0.9), Evidence(text='Datan esikäsittely ja piirteiden suunnittelu ovat keskeisiä vaiheita.', relevance=0.9)] themes=['data science', 'artificial intelligence']\n\nFinal categories: [CategoryMatch(name='Machine Learning', confidence=0.95, description='The text discusses concepts directly related to machine learning, including model training, architecture, and data preprocessing.', evidence=[Evidence(text='Koneoppimismalleja koulutetaan suurilla datajoukolla tunnistamaan kaavoja.', relevance=0.9), Evidence(text='Neuroverkon arkkitehtuuri sisältää useita kerroksia piirteiden erottamiseen.', relevance=0.9), Evidence(text='Datan esikäsittely ja piirteiden suunnittelu ovat keskeisiä vaiheita.', relevance=0.9)], themes=['data science', 'artificial intelligence'])]\n\nResults:\n--------------------\n\nCategories:\n\n• Machine Learning\n  Confidence: 0.95\n  Description: The text discusses concepts directly related to machine learning, including model training, architecture, and data preprocessing.\n\n  Evidence:\n  - Koneoppimismalleja koulutetaan suurilla datajoukolla tunnistamaan kaavoja. (relevance: 0.90)\n  - Neuroverkon arkkitehtuuri sisältää useita kerroksia piirteiden erottamiseen. (relevance: 0.90)\n  - Datan esikäsittely ja piirteiden suunnittelu ovat keskeisiä vaiheita. (relevance: 0.90)\n\nTesting FI Business Content:\n\nTesting Keyword Analysis\n==================================================\n\nInput text:\nQ3 taloudelliset tulokset osoittavat 15% liikevaihdon kasvun ja parantuneet katteet. \n                    Asiakashankinnan kustannukset laskivat ja asiakaspysyvyys parani. \n                    Markkin...\n\nResults:\n--------------------\n\nKeywords:\n• taloudellinen        (0.95)\n  Domain: business\n• tulos                (0.95)\n  Domain: business\n• liikevaihto          (0.95)\n  Domain: business\n• kasvu                (0.95)\n  Domain: business\n• parantunut           (0.95)\n  Domain: business\n• asiakashankinnan kustannukset (0.90)\n  Domain: business\n• asiakaspysyvyys      (0.90)\n  Domain: business\n• markkinalaajennusstrategia (0.90)\n  Domain: business\n• nousevat teknologiasektorit (0.90)\n  Domain: technical\n\nCompound Words:\nasiakashankinnan kustannukset, markkinalaajennusstrategia, nousevat teknologiasektorit\n\nTesting Theme Analysis\n==================================================\n\nInput text:\nQ3 taloudelliset tulokset osoittavat 15% liikevaihdon kasvun ja parantuneet katteet. \n                    Asiakashankinnan kustannukset laskivat ja asiakaspysyvyys parani. \n                    Markkin...\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "INFO: HTTP Request: POST https://ri-feedback-analysis.openai.azure.com/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-02-15-preview \"HTTP/1.1 200 OK\"\nINFO: HTTP Request: POST https://ri-feedback-analysis.openai.azure.com/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-02-15-preview \"HTTP/1.1 200 OK\"\n"
        }
      ],
      "execution_count": 21,
      "metadata": {
        "gather": {
          "logged": 1733728166303
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "semantic-analyzer",
      "language": "python",
      "display_name": "Python (semantic-analyzer)"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.21",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "semantic-analyzer"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}