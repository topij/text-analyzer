{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Import required modules\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any, Tuple, Union\n",
        "import logging\n",
        "import asyncio\n",
        "\n",
        "# Add project root to Python path if needed\n",
        "project_root = str(Path().resolve().parent)\n",
        "if project_root not in sys.path:\n",
        "    sys.path.append(project_root)\n",
        "\n",
        "# Import necessary components\n",
        "# from src.nb_helpers.environment import setup_notebook_env, verify_environment\n",
        "from src.semantic_analyzer import SemanticAnalyzer\n",
        "from src.core.config import AnalyzerConfig\n",
        "\n",
        "from src.core.language_processing import create_text_processor\n",
        "from src.core.llm.factory import create_llm\n",
        "from src.loaders.parameter_handler import ParameterHandler\n",
        "from src.analyzers.keyword_analyzer import KeywordAnalyzer\n",
        "from src.analyzers.theme_analyzer import ThemeAnalyzer\n",
        "from src.analyzers.category_analyzer import CategoryAnalyzer\n",
        "\n",
        "import FileUtils\n"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1733687977385
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scripts.azure.setup_helpers import setup_voikko\n",
        "\n",
        "from src.az_helpers.setup_azure import init_azure_ml\n",
        "from src.az_helpers.az_environment import setup_notebook_env, verify_environment\n",
        "\n",
        "# Set up environment and logging\n",
        "setup_notebook_env(log_level=\"DEBUG\")\n",
        "verify_environment()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "2024-12-08 19:59:45,553 - FileUtils.core.file_utils - INFO - Project root: /mnt/batch/tasks/shared/LS_root/mounts/clusters/basic-cpu/code/Users/topi.jarvinen/semantic-text-analyzer\n2024-12-08 19:59:45,612 - FileUtils.core.file_utils - INFO - FileUtils initialized with local storage\n2024-12-08 19:59:45,657 - FileUtils.core.file_utils - INFO - Project root: /mnt/batch/tasks/shared/LS_root/mounts/clusters/basic-cpu/code/Users/topi.jarvinen/semantic-text-analyzer\n2024-12-08 19:59:45,667 - FileUtils.core.file_utils - INFO - FileUtils initialized with local storage\nEnvironment Check Results:\n==================================================\n✓ Project root in path\n✓ FileUtils initialized\n✓ .env file loaded\n✓ OPENAI_API_KEY set\n✓ ANTHROPIC_API_KEY set\n✓ Raw data exists\n✓ Processed data exists\n✓ Configuration exists\n✓ Main config.yaml exists\n\n==================================================\nEnvironment Status: Ready ✓\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 2,
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733687985904
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset logging configuration for Voikko setup\n",
        "logging.getLogger().setLevel(logging.DEBUG)\n",
        "for handler in logging.getLogger().handlers:\n",
        "    handler.setLevel(logging.DEBUG)\n",
        "\n",
        "# Also set debug level specifically for the setup helper logger\n",
        "logging.getLogger(\"scripts.azure.setup_helpers\").setLevel(logging.DEBUG)\n",
        "\n",
        "from scripts.azure.setup_helpers import setup_voikko\n",
        "\n",
        "# Run setup with explicit project root\n",
        "if setup_voikko(project_root=project_root):\n",
        "    print(\"Voikko ready\")\n",
        "else:\n",
        "    print(\"Using fallback mode\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Using fallback mode\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733687986105
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Run setup with explicit project root\n",
        "if setup_voikko(project_root=project_root):\n",
        "    print(\"Voikko ready\")\n",
        "else:\n",
        "    print(\"Using fallback mode\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Using fallback mode\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733687986323
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In analyzer_demo_local_nb.ipynb and azure_notebook.ipynb\n",
        "# from src.core.llm.factory import create_llm\n",
        "\n",
        "# Setup\n",
        "# config = AnalyzerConfig()\n",
        "# llm = create_llm(config=config)\n",
        "# analyzer = SemanticAnalyzer(llm=llm)"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1733687986541
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(FileUtils.__version__)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "0.5.3\n"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1733687986745
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up environment and logging\n",
        "setup_notebook_env(log_level=\"DEBUG\")\n",
        "verify_environment()\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "2024-12-08 19:59:45,959 - FileUtils.core.file_utils - INFO - Project root: /mnt/batch/tasks/shared/LS_root/mounts/clusters/basic-cpu/code/Users/topi.jarvinen/semantic-text-analyzer\n2024-12-08 19:59:45,968 - FileUtils.core.file_utils - INFO - FileUtils initialized with local storage\n2024-12-08 19:59:46,019 - FileUtils.core.file_utils - INFO - Project root: /mnt/batch/tasks/shared/LS_root/mounts/clusters/basic-cpu/code/Users/topi.jarvinen/semantic-text-analyzer\n2024-12-08 19:59:46,020 - FileUtils.core.file_utils - INFO - FileUtils initialized with local storage\nEnvironment Check Results:\n==================================================\n✓ Project root in path\n✓ FileUtils initialized\n✓ .env file loaded\n✓ OPENAI_API_KEY set\n✓ ANTHROPIC_API_KEY set\n✓ Raw data exists\n✓ Processed data exists\n✓ Configuration exists\n✓ Main config.yaml exists\n\n==================================================\nEnvironment Status: Ready ✓\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1733687987325
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test data to use\n",
        "test_texts = {\n",
        "    \"en\": {\n",
        "        \"technical\": \"\"\"Machine learning models are trained using large datasets to recognize patterns. \n",
        "                     The neural network architecture includes multiple layers for feature extraction. \n",
        "                     Data preprocessing and feature engineering are crucial steps.\"\"\",\n",
        "        \"business\": \"\"\"Q3 financial results show 15% revenue growth and improved profit margins. \n",
        "                    Customer acquisition costs decreased while retention rates increased. \n",
        "                    Market expansion strategy focuses on emerging technology sectors.\"\"\"\n",
        "    },\n",
        "    \"fi\": {\n",
        "        \"technical\": \"\"\"Koneoppimismalleja koulutetaan suurilla datajoukolla tunnistamaan kaavoja. \n",
        "                     Neuroverkon arkkitehtuuri sisältää useita kerroksia piirteiden erottamiseen. \n",
        "                     Datan esikäsittely ja piirteiden suunnittelu ovat keskeisiä vaiheita.\"\"\",\n",
        "        \"business\": \"\"\"Q3 taloudelliset tulokset osoittavat 15% liikevaihdon kasvun ja parantuneet katteet. \n",
        "                    Asiakashankinnan kustannukset laskivat ja asiakaspysyvyys parani. \n",
        "                    Markkinalaajennusstrategia keskittyy nouseviin teknologiasektoreihin.\"\"\"\n",
        "    }\n",
        "}\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1733687987688
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_texts['en']['technical']"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": "'Machine learning models are trained using large datasets to recognize patterns. \\n                     The neural network architecture includes multiple layers for feature extraction. \\n                     Data preprocessing and feature engineering are crucial steps.'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733687987907
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "     # llm = create_llm()\n",
        "config = AnalyzerConfig()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "2024-12-08 19:59:46,162 - FileUtils.core.file_utils - INFO - Project root: /mnt/batch/tasks/shared/LS_root/mounts/clusters/basic-cpu/code/Users/topi.jarvinen/semantic-text-analyzer\n2024-12-08 19:59:46,164 - FileUtils.core.file_utils - INFO - FileUtils initialized with local storage\n"
        }
      ],
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1733687988119
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async def test_individual_analyzer(\n",
        "    analyzer: Union[KeywordAnalyzer, ThemeAnalyzer, CategoryAnalyzer], \n",
        "    text: str, \n",
        "    analyzer_type: str,\n",
        "    provider: str = 'openai'\n",
        "):\n",
        "    \"\"\"Test individual analyzer component.\"\"\"\n",
        "    print(f\"\\nTesting {analyzer_type} Analysis\")\n",
        "    print(\"=\" * 50)\n",
        "    print(\"\\nInput text:\")\n",
        "    print(text[:200] + \"...\" if len(text) > 200 else text)\n",
        "    \n",
        "    try:\n",
        "        results = await analyzer.analyze(text)\n",
        "        \n",
        "        print(\"\\nResults:\")\n",
        "        print(\"-\" * 20)\n",
        "        \n",
        "        if isinstance(analyzer, KeywordAnalyzer):\n",
        "            if results.keywords:\n",
        "                print(\"\\nKeywords:\")\n",
        "                for kw in results.keywords[:10]:  # Show top 10\n",
        "                    print(f\"• {kw.keyword:<20} ({kw.score:.2f})\")\n",
        "                    if kw.domain:\n",
        "                        print(f\"  Domain: {kw.domain}\")\n",
        "                \n",
        "                if results.compound_words:\n",
        "                    print(\"\\nCompound Words:\")\n",
        "                    print(\", \".join(results.compound_words))\n",
        "                    \n",
        "                if results.domain_keywords:\n",
        "                    print(\"\\nKeywords by Domain:\")\n",
        "                    for domain, kws in results.domain_keywords.items():\n",
        "                        print(f\"\\n{domain}:\")\n",
        "                        print(\", \".join(kws))\n",
        "                        \n",
        "        elif isinstance(analyzer, ThemeAnalyzer):\n",
        "            if results.themes:\n",
        "                print(\"\\nThemes:\")\n",
        "                for theme in results.themes:\n",
        "                    print(f\"\\n• {theme.name}\")\n",
        "                    print(f\"  Confidence: {theme.confidence:.2f}\")\n",
        "                    print(f\"  Description: {theme.description}\")\n",
        "                    if theme.keywords:\n",
        "                        print(f\"  Keywords: {', '.join(theme.keywords)}\")\n",
        "                \n",
        "                if results.theme_hierarchy:\n",
        "                    print(\"\\nTheme Hierarchy:\")\n",
        "                    for parent, children in results.theme_hierarchy.items():\n",
        "                        print(f\"{parent} -> {', '.join(children)}\")\n",
        "                        \n",
        "        elif isinstance(analyzer, CategoryAnalyzer):\n",
        "            if results.categories:\n",
        "                print(\"\\nCategories:\")\n",
        "                for cat in results.categories:\n",
        "                    print(f\"\\n• {cat.name}\")\n",
        "                    print(f\"  Confidence: {cat.confidence:.2f}\")\n",
        "                    if cat.description:\n",
        "                        print(f\"  Description: {cat.description}\")\n",
        "                    if cat.evidence:\n",
        "                        print(\"\\n  Evidence:\")\n",
        "                        for ev in cat.evidence:\n",
        "                            print(f\"  - {ev.text} (relevance: {ev.relevance:.2f})\")\n",
        "                            \n",
        "        if hasattr(results, 'error') and results.error:\n",
        "            print(f\"\\nErrors occurred: {results.error}\")\n",
        "            \n",
        "        return results\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"\\nError in analysis: {e}\")\n",
        "        return None"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1733687988318
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: Test individual keyword analyzer\n",
        "async def test_keyword_analyzer(provider: str = \"openai\"):\n",
        "    \"\"\"Test keyword analyzer with different languages.\"\"\"\n",
        "    print(\"Testing Keyword Analyzer\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Initialize components\n",
        "    parameter_handler = ParameterHandler(\"parameters_fi.xlsx\")\n",
        "    # llm = create_llm()\n",
        "    # config = AnalyzerConfig()\n",
        "    llm = create_llm(provider=provider, config=config)\n",
        "    # analyzer = SemanticAnalyzer(llm=llm)\n",
        "    \n",
        "    # Test English\n",
        "    print(\"\\nTesting English Technical Content:\")\n",
        "    en_processor = create_text_processor(language=\"en\")\n",
        "    keyword_analyzer_en = KeywordAnalyzer(\n",
        "        llm=llm,\n",
        "        config=parameter_handler.parameters.general.model_dump(),\n",
        "        language_processor=en_processor\n",
        "    )\n",
        "    await test_individual_analyzer(keyword_analyzer_en, test_texts[\"en\"][\"technical\"], \"Keyword\")\n",
        "    \n",
        "    # Test Finnish\n",
        "    print(\"\\nTesting Finnish Technical Content:\")\n",
        "    fi_processor = create_text_processor(language=\"fi\")\n",
        "    keyword_analyzer_fi = KeywordAnalyzer(\n",
        "        llm=llm,\n",
        "        config=parameter_handler.parameters.general.model_dump(),\n",
        "        language_processor=fi_processor\n",
        "    )\n",
        "    await test_individual_analyzer(keyword_analyzer_fi, test_texts[\"fi\"][\"technical\"], \"Keyword\")\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1733687988530
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2: Test all components\n",
        "async def test_components_for_language(language: str, provider: str = \"openai\"):\n",
        "    \"\"\"Test all components for a specific language.\"\"\"\n",
        "    print(f\"\\nTesting All Components for {language.upper()}\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Initialize components\n",
        "    parameter_handler = ParameterHandler(f\"parameters_{language}.xlsx\")\n",
        "     # llm = create_llm()\n",
        "    # config = AnalyzerConfig()\n",
        "    llm = create_llm(provider=provider, config=config)\n",
        "\n",
        "    language_processor = create_text_processor(language=language)\n",
        "    \n",
        "    # Create analyzers\n",
        "    keyword_analyzer = KeywordAnalyzer(\n",
        "        llm=llm,\n",
        "        config=parameter_handler.parameters.general.model_dump(),\n",
        "        language_processor=language_processor\n",
        "    )\n",
        "    \n",
        "    theme_analyzer = ThemeAnalyzer(\n",
        "        llm=llm,\n",
        "        config=parameter_handler.parameters.general.model_dump(),\n",
        "        language_processor=language_processor\n",
        "    )\n",
        "    \n",
        "    category_analyzer = CategoryAnalyzer(\n",
        "        categories=parameter_handler.parameters.categories,\n",
        "        llm=llm,\n",
        "        config=parameter_handler.parameters.general.model_dump(),\n",
        "        language_processor=language_processor\n",
        "    )\n",
        "    \n",
        "    # Test technical content\n",
        "    print(f\"\\nTesting {language.upper()} Technical Content:\")\n",
        "    await test_individual_analyzer(keyword_analyzer, test_texts[language][\"technical\"], \"Keyword\")\n",
        "    await test_individual_analyzer(theme_analyzer, test_texts[language][\"technical\"], \"Theme\")\n",
        "    await test_individual_analyzer(category_analyzer, test_texts[language][\"technical\"], \"Category\")\n",
        "    \n",
        "    # Test business content\n",
        "    print(f\"\\nTesting {language.upper()} Business Content:\")\n",
        "    await test_individual_analyzer(keyword_analyzer, test_texts[language][\"business\"], \"Keyword\")\n",
        "    await test_individual_analyzer(theme_analyzer, test_texts[language][\"business\"], \"Theme\")\n",
        "    await test_individual_analyzer(category_analyzer, test_texts[language][\"business\"], \"Category\")\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1733687988748
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3: Quick test of full pipeline\n",
        "async def test_pipeline(provider='openai'):\n",
        "    \"\"\"Test full pipeline with both languages.\"\"\"\n",
        "    print(\"Testing Full Pipeline\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    llm = create_llm(provider=provider, config=config)\n",
        "    # analyzer = SemanticAnalyzer(llm=llm)\n",
        "    # Test English pipeline\n",
        "    print(\"\\nEnglish Pipeline:\")\n",
        "    en_analyzer = SemanticAnalyzer(llm=llm, parameter_file=\"parameters_en.xlsx\")\n",
        "    result = await en_analyzer.analyze(test_texts[\"en\"][\"technical\"])\n",
        "    print(f\"Success: {result.success}\")\n",
        "    print(f\"Keywords found: {len(result.keywords.keywords)}\")\n",
        "    print(f\"Themes found: {len(result.themes.themes)}\")\n",
        "    print(f\"Categories found: {len(result.categories.matches)}\")\n",
        "    \n",
        "    # Test Finnish pipeline\n",
        "    print(\"\\nFinnish Pipeline:\")\n",
        "    fi_analyzer = SemanticAnalyzer(llm=llm, parameter_file=\"parameters_fi.xlsx\")\n",
        "    result = await fi_analyzer.analyze(test_texts[\"fi\"][\"technical\"])\n",
        "    print(f\"Success: {result.success}\")\n",
        "    print(f\"Keywords found: {len(result.keywords.keywords)}\")\n",
        "    print(f\"Themes found: {len(result.themes.themes)}\")\n",
        "    print(f\"Categories found: {len(result.categories.matches)}\")\n",
        "\n",
        "# Run the tests\n",
        "async def run_all_tests(provider='openai'):\n",
        "    \"\"\"Run all tests.\"\"\"\n",
        "    # Test individual component\n",
        "    await test_keyword_analyzer(provider=provider)\n",
        "    \n",
        "    # Test all components by language\n",
        "    await test_components_for_language(language=\"en\", provider=provider)\n",
        "    await test_components_for_language(language=\"fi\", provider=provider)\n",
        "    \n",
        "    # Test full pipeline\n",
        "    await test_pipeline(provider=provider)\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1733687988957
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import libvoikko"
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733687989209
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# help(libvoikko)"
      ],
      "outputs": [],
      "execution_count": 16,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733687989413
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dir(libvoikko)"
      ],
      "outputs": [],
      "execution_count": 17,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733687989618
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from pathlib import Path\n",
        "# scripts_dir = Path.cwd() / \"scripts\" / \"azure\"\n",
        "# print(f\"Looking for script in: {scripts_dir}\")\n",
        "# print(f\"Script exists: {(scripts_dir / 'setup_voikko.sh').exists()}\")"
      ],
      "outputs": [],
      "execution_count": 18,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733687989823
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import logging\n",
        "# from pathlib import Path\n",
        "\n",
        "# # Configure detailed logging\n",
        "# logging.basicConfig(\n",
        "#     level=logging.DEBUG,\n",
        "#     format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        "# )\n",
        "\n",
        "# # Get project root (you already have this)\n",
        "# project_root = Path().resolve().parent\n",
        "\n",
        "# from scripts.azure.setup_helpers import setup_voikko\n",
        "\n",
        "# # Run setup with explicit project root\n",
        "# if setup_voikko(project_root=project_root):\n",
        "#     print(\"Voikko ready\")\n",
        "# else:\n",
        "#     print(\"Using fallback mode\")"
      ],
      "outputs": [],
      "execution_count": 19,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733687990026
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import logging\n",
        "\n",
        "# # Configure detailed logging\n",
        "# logging.basicConfig(\n",
        "#     level=logging.DEBUG,\n",
        "#     format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        "# )\n",
        "\n",
        "# from scripts.azure.setup_helpers import setup_voikko\n",
        "\n",
        "# # Run setup with debug output\n",
        "# if setup_voikko():\n",
        "#     print(\"Voikko ready\")\n",
        "# else:\n",
        "#     print(\"Using fallback mode\")"
      ],
      "outputs": [],
      "execution_count": 20,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1733687990239
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run in notebook\n",
        "# await run_all_tests()\n",
        "\n",
        "# Or run individual tests:\n",
        "# await test_keyword_analyzer(provider='azure')\n",
        "await test_components_for_language(\"fi\", provider='azure')\n",
        "# await test_pipeline()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "INFO: HTTP Request: POST https://ri-feedback-analysis.openai.azure.com/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-02-15-preview \"HTTP/1.1 200 OK\"\nINFO: HTTP Request: POST https://ri-feedback-analysis.openai.azure.com/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-02-15-preview \"HTTP/1.1 200 OK\"\nINFO: HTTP Request: POST https://ri-feedback-analysis.openai.azure.com/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-02-15-preview \"HTTP/1.1 200 OK\"\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\nResults:\n--------------------\n\nKeywords:\n• koneoppimismalli     (0.95)\n  Domain: technical\n• datajoukko           (0.95)\n  Domain: technical\n• neuroverkon arkkitehtuuri (0.95)\n  Domain: technical\n• datan esikäsittely   (0.95)\n  Domain: technical\n• piirteiden suunnittelu (0.95)\n  Domain: technical\n\nCompound Words:\nkoneoppimismalli, datajoukko, neuroverkon arkkitehtuuri, datan esikäsittely, piirteiden suunnittelu\n\nTesting Theme Analysis\n==================================================\n\nInput text:\nKoneoppimismalleja koulutetaan suurilla datajoukolla tunnistamaan kaavoja. \n                     Neuroverkon arkkitehtuuri sisältää useita kerroksia piirteiden erottamiseen. \n                     Data...\n\nRaw LLM response: {'themes': [{'name': 'Koneoppimismallien koulutus', 'description': 'Koneoppimismallien koulutus suurilla datajoukoilla on keskeinen prosessi, jossa malleja opetetaan tunnistamaan kaavoja datasta.', 'confidence': 0.95, 'keywords': ['koneoppimismalli', 'datajoukko', 'kaava'], 'domain': 'general content analysis'}, {'name': 'Neuroverkon arkkitehtuuri', 'description': 'Neuroverkon arkkitehtuuri koostuu useista kerroksista, jotka mahdollistavat piirteiden erottamisen ja analysoinnin.', 'confidence': 0.9, 'keywords': ['neuroverkko', 'arkkitehtuuri', 'kerros'], 'domain': 'general content analysis', 'parent_theme': 'Koneoppimismallien koulutus'}, {'name': 'Datan esikäsittely ja piirteiden suunnittelu', 'description': 'Datan esikäsittely ja piirteiden suunnittelu ovat keskeisiä vaiheita koneoppimismallien tehokkuuden varmistamiseksi.', 'confidence': 0.85, 'keywords': ['esikäsittely', 'piirre', 'suunnittelu'], 'domain': 'general content analysis', 'parent_theme': 'Koneoppimismallien koulutus'}], 'evidence': {'Koneoppimismallien koulutus': [{'text': 'Koneoppimismalleja koulutetaan suurilla datajoukolla tunnistamaan kaavoja.', 'relevance': 0.9, 'keywords': ['koneoppimismalli', 'datajoukko', 'kaava']}], 'Neuroverkon arkkitehtuuri': [{'text': 'Neuroverkon arkkitehtuuri sisältää useita kerroksia piirteiden erottamiseen.', 'relevance': 0.9, 'keywords': ['neuroverkko', 'arkkitehtuuri', 'kerros']}], 'Datan esikäsittely ja piirteiden suunnittelu': [{'text': 'Datan esikäsittely ja piirteiden suunnittelu ovat keskeisiä vaiheita.', 'relevance': 0.85, 'keywords': ['esikäsittely', 'piirre', 'suunnittelu']}]}, 'relationships': {'Koneoppimismallien koulutus': ['Neuroverkon arkkitehtuuri', 'Datan esikäsittely ja piirteiden suunnittelu'], 'Neuroverkon arkkitehtuuri': [], 'Datan esikäsittely ja piirteiden suunnittelu': []}}\n\nProcessed LLM response: {'themes': [{'name': 'Koneoppimismallien koulutus', 'description': 'Koneoppimismallien koulutus suurilla datajoukoilla on keskeinen prosessi, jossa malleja opetetaan tunnistamaan kaavoja datasta.', 'confidence': 0.95, 'keywords': ['koneoppimismalli', 'datajoukko', 'kaava'], 'domain': 'general content analysis'}, {'name': 'Neuroverkon arkkitehtuuri', 'description': 'Neuroverkon arkkitehtuuri koostuu useista kerroksista, jotka mahdollistavat piirteiden erottamisen ja analysoinnin.', 'confidence': 0.9, 'keywords': ['neuroverkko', 'arkkitehtuuri', 'kerros'], 'domain': 'general content analysis', 'parent_theme': 'Koneoppimismallien koulutus'}, {'name': 'Datan esikäsittely ja piirteiden suunnittelu', 'description': 'Datan esikäsittely ja piirteiden suunnittelu ovat keskeisiä vaiheita koneoppimismallien tehokkuuden varmistamiseksi.', 'confidence': 0.85, 'keywords': ['esikäsittely', 'piirre', 'suunnittelu'], 'domain': 'general content analysis', 'parent_theme': 'Koneoppimismallien koulutus'}], 'evidence': {'Koneoppimismallien koulutus': [{'text': 'Koneoppimismalleja koulutetaan suurilla datajoukolla tunnistamaan kaavoja.', 'relevance': 0.9, 'keywords': ['koneoppimismalli', 'datajoukko', 'kaava']}], 'Neuroverkon arkkitehtuuri': [{'text': 'Neuroverkon arkkitehtuuri sisältää useita kerroksia piirteiden erottamiseen.', 'relevance': 0.9, 'keywords': ['neuroverkko', 'arkkitehtuuri', 'kerros']}], 'Datan esikäsittely ja piirteiden suunnittelu': [{'text': 'Datan esikäsittely ja piirteiden suunnittelu ovat keskeisiä vaiheita.', 'relevance': 0.85, 'keywords': ['esikäsittely', 'piirre', 'suunnittelu']}]}, 'relationships': {'Koneoppimismallien koulutus': ['Neuroverkon arkkitehtuuri', 'Datan esikäsittely ja piirteiden suunnittelu'], 'Neuroverkon arkkitehtuuri': [], 'Datan esikäsittely ja piirteiden suunnittelu': []}}\n\nResults:\n--------------------\n\nThemes:\n\n• Koneoppimismallien koulutus\n  Confidence: 0.95\n  Description: Koneoppimismallien koulutus suurilla datajoukoilla on keskeinen prosessi, jossa malleja opetetaan tunnistamaan kaavoja datasta.\n  Keywords: koneoppimismalli, datajoukko, kaava\n\n• Neuroverkon arkkitehtuuri\n  Confidence: 0.90\n  Description: Neuroverkon arkkitehtuuri koostuu useista kerroksista, jotka mahdollistavat piirteiden erottamisen ja analysoinnin.\n  Keywords: neuroverkko, arkkitehtuuri, kerros\n\n• Datan esikäsittely ja piirteiden suunnittelu\n  Confidence: 0.85\n  Description: Datan esikäsittely ja piirteiden suunnittelu ovat keskeisiä vaiheita koneoppimismallien tehokkuuden varmistamiseksi.\n  Keywords: esikäsittely, piirre, suunnittelu\n\nTesting Category Analysis\n==================================================\n\nInput text:\nKoneoppimismalleja koulutetaan suurilla datajoukolla tunnistamaan kaavoja. \n                     Neuroverkon arkkitehtuuri sisältää useita kerroksia piirteiden erottamiseen. \n                     Data...\n\nProcessing response: {'categories': [{'category': 'Machine Learning', 'confidence': 0.95, 'explanation': 'The text discusses concepts related to machine learning, including model training, neural network architecture, and data preprocessing.', 'evidence': [{'text': 'Koneoppimismalleja koulutetaan suurilla datajoukolla tunnistamaan kaavoja.', 'relevance': 0.9, 'matched_keywords': ['koneoppimismalli', 'datajoukko', 'kaava'], 'context': 'The sentence describes the training of machine learning models using large datasets to identify patterns.'}, {'text': 'Neuroverkon arkkitehtuuri sisältää useita kerroksia piirteiden erottamiseen.', 'relevance': 0.9, 'matched_keywords': ['neuroverkon', 'arkkitehtuuri', 'kerros', 'piirre'], 'context': 'This part explains the architecture of neural networks, emphasizing layers for feature extraction.'}, {'text': 'Datan esikäsittely ja piirteiden suunnittelu ovat keskeisiä vaiheita.', 'relevance': 0.9, 'matched_keywords': ['datan', 'esikäsittely', 'piirteiden', 'suunnittelu', 'keskeisiä'], 'context': 'It highlights the importance of data preprocessing and feature design as critical steps in the machine learning process.'}], 'themes': ['data science', 'artificial intelligence']}], 'relationships': {'Machine Learning': ['Data Science', 'Artificial Intelligence']}}\n\nProcessing category: {'category': 'Machine Learning', 'confidence': 0.95, 'explanation': 'The text discusses concepts related to machine learning, including model training, neural network architecture, and data preprocessing.', 'evidence': [{'text': 'Koneoppimismalleja koulutetaan suurilla datajoukolla tunnistamaan kaavoja.', 'relevance': 0.9, 'matched_keywords': ['koneoppimismalli', 'datajoukko', 'kaava'], 'context': 'The sentence describes the training of machine learning models using large datasets to identify patterns.'}, {'text': 'Neuroverkon arkkitehtuuri sisältää useita kerroksia piirteiden erottamiseen.', 'relevance': 0.9, 'matched_keywords': ['neuroverkon', 'arkkitehtuuri', 'kerros', 'piirre'], 'context': 'This part explains the architecture of neural networks, emphasizing layers for feature extraction.'}, {'text': 'Datan esikäsittely ja piirteiden suunnittelu ovat keskeisiä vaiheita.', 'relevance': 0.9, 'matched_keywords': ['datan', 'esikäsittely', 'piirteiden', 'suunnittelu', 'keskeisiä'], 'context': 'It highlights the importance of data preprocessing and feature design as critical steps in the machine learning process.'}], 'themes': ['data science', 'artificial intelligence']}\nCreated category: name='Machine Learning' confidence=0.95 description='The text discusses concepts related to machine learning, including model training, neural network architecture, and data preprocessing.' evidence=[Evidence(text='Koneoppimismalleja koulutetaan suurilla datajoukolla tunnistamaan kaavoja.', relevance=0.9), Evidence(text='Neuroverkon arkkitehtuuri sisältää useita kerroksia piirteiden erottamiseen.', relevance=0.9), Evidence(text='Datan esikäsittely ja piirteiden suunnittelu ovat keskeisiä vaiheita.', relevance=0.9)] themes=['data science', 'artificial intelligence']\n\nFinal categories: [CategoryMatch(name='Machine Learning', confidence=0.95, description='The text discusses concepts related to machine learning, including model training, neural network architecture, and data preprocessing.', evidence=[Evidence(text='Koneoppimismalleja koulutetaan suurilla datajoukolla tunnistamaan kaavoja.', relevance=0.9), Evidence(text='Neuroverkon arkkitehtuuri sisältää useita kerroksia piirteiden erottamiseen.', relevance=0.9), Evidence(text='Datan esikäsittely ja piirteiden suunnittelu ovat keskeisiä vaiheita.', relevance=0.9)], themes=['data science', 'artificial intelligence'])]\n\nResults:\n--------------------\n\nCategories:\n\n• Machine Learning\n  Confidence: 0.95\n  Description: The text discusses concepts related to machine learning, including model training, neural network architecture, and data preprocessing.\n\n  Evidence:\n  - Koneoppimismalleja koulutetaan suurilla datajoukolla tunnistamaan kaavoja. (relevance: 0.90)\n  - Neuroverkon arkkitehtuuri sisältää useita kerroksia piirteiden erottamiseen. (relevance: 0.90)\n  - Datan esikäsittely ja piirteiden suunnittelu ovat keskeisiä vaiheita. (relevance: 0.90)\n\nTesting FI Business Content:\n\nTesting Keyword Analysis\n==================================================\n\nInput text:\nQ3 taloudelliset tulokset osoittavat 15% liikevaihdon kasvun ja parantuneet katteet. \n                    Asiakashankinnan kustannukset laskivat ja asiakaspysyvyys parani. \n                    Markkin...\n\nResults:\n--------------------\n\nKeywords:\n• taloudellinen        (0.95)\n  Domain: business\n• tulos                (0.95)\n  Domain: business\n• liikevaihto          (0.95)\n  Domain: business\n• kasvu                (0.95)\n  Domain: business\n• parantunut           (0.95)\n  Domain: business\n• asiakashankinnan kustannukset (0.90)\n  Domain: business\n• asiakaspysyvyys      (0.90)\n  Domain: business\n• markkinalaajennusstrategia (0.90)\n  Domain: business\n• nousevat teknologiasektorit (0.90)\n  Domain: technical\n\nCompound Words:\nasiakashankinnan kustannukset, markkinalaajennusstrategia, nousevat teknologiasektorit\n\nTesting Theme Analysis\n==================================================\n\nInput text:\nQ3 taloudelliset tulokset osoittavat 15% liikevaihdon kasvun ja parantuneet katteet. \n                    Asiakashankinnan kustannukset laskivat ja asiakaspysyvyys parani. \n                    Markkin...\n\nRaw LLM response: {'themes': [{'name': 'Taloudellinen kasvu', 'description': 'Viittaa yrityksen taloudelliseen kehitykseen, erityisesti liikevaihdon ja katteiden parantumiseen.', 'confidence': 0.95, 'keywords': ['liikevaihto', 'kasvu', 'kate'], 'domain': 'general', 'parent_theme': None}, {'name': 'Asiakashankinta ja pysyvyys', 'description': 'Käsittelee asiakashankinnan kustannusten laskua ja asiakaspysyvyyden parantumista, mikä on tärkeää liiketoiminnan kestävyyden kannalta.', 'confidence': 0.9, 'keywords': ['asiakashankinta', 'asiakaspysyvyys', 'kustannus'], 'domain': 'general', 'parent_theme': 'Taloudellinen kasvu'}, {'name': 'Markkinalaajennusstrategia', 'description': 'Keskittyy yrityksen strategisiin toimiin markkinoiden laajentamiseksi, erityisesti nouseviin teknologiasektoreihin.', 'confidence': 0.85, 'keywords': ['markkinalaajennusstrategia', 'teknologia', 'sektori'], 'domain': 'general', 'parent_theme': None}], 'evidence': {'Taloudellinen kasvu': [{'text': 'Q3 taloudelliset tulokset osoittavat 15% liikevaihdon kasvun ja parantuneet katteet.', 'relevance': 0.9, 'keywords': ['liikevaihto', 'kasvu', 'kate']}], 'Asiakashankinta ja pysyvyys': [{'text': 'Asiakashankinnan kustannukset laskivat ja asiakaspysyvyys parani.', 'relevance': 0.9, 'keywords': ['asiakashankinta', 'asiakaspysyvyys', 'kustannus']}], 'Markkinalaajennusstrategia': [{'text': 'Markkinalaajennusstrategia keskittyy nouseviin teknologiasektoreihin.', 'relevance': 0.85, 'keywords': ['markkinalaajennusstrategia', 'teknologia', 'sektori']}]}, 'relationships': {'Taloudellinen kasvu': ['Asiakashankinta ja pysyvyys', 'Markkinalaajennusstrategia'], 'Asiakashankinta ja pysyvyys': [], 'Markkinalaajennusstrategia': []}}\n\nProcessed LLM response: {'themes': [{'name': 'Taloudellinen kasvu', 'description': 'Viittaa yrityksen taloudelliseen kehitykseen, erityisesti liikevaihdon ja katteiden parantumiseen.', 'confidence': 0.95, 'keywords': ['liikevaihto', 'kasvu', 'kate'], 'domain': 'general', 'parent_theme': None}, {'name': 'Asiakashankinta ja pysyvyys', 'description': 'Käsittelee asiakashankinnan kustannusten laskua ja asiakaspysyvyyden parantumista, mikä on tärkeää liiketoiminnan kestävyyden kannalta.', 'confidence': 0.9, 'keywords': ['asiakashankinta', 'asiakaspysyvyys', 'kustannus'], 'domain': 'general', 'parent_theme': 'Taloudellinen kasvu'}, {'name': 'Markkinalaajennusstrategia', 'description': 'Keskittyy yrityksen strategisiin toimiin markkinoiden laajentamiseksi, erityisesti nouseviin teknologiasektoreihin.', 'confidence': 0.85, 'keywords': ['markkinalaajennusstrategia', 'teknologia', 'sektori'], 'domain': 'general', 'parent_theme': None}], 'evidence': {'Taloudellinen kasvu': [{'text': 'Q3 taloudelliset tulokset osoittavat 15% liikevaihdon kasvun ja parantuneet katteet.', 'relevance': 0.9, 'keywords': ['liikevaihto', 'kasvu', 'kate']}], 'Asiakashankinta ja pysyvyys': [{'text': 'Asiakashankinnan kustannukset laskivat ja asiakaspysyvyys parani.', 'relevance': 0.9, 'keywords': ['asiakashankinta', 'asiakaspysyvyys', 'kustannus']}], 'Markkinalaajennusstrategia': [{'text': 'Markkinalaajennusstrategia keskittyy nouseviin teknologiasektoreihin.', 'relevance': 0.85, 'keywords': ['markkinalaajennusstrategia', 'teknologia', 'sektori']}]}, 'relationships': {'Taloudellinen kasvu': ['Asiakashankinta ja pysyvyys', 'Markkinalaajennusstrategia'], 'Asiakashankinta ja pysyvyys': [], 'Markkinalaajennusstrategia': []}}\n\nResults:\n--------------------\n\nThemes:\n\n• Taloudellinen kasvu\n  Confidence: 0.95\n  Description: Viittaa yrityksen taloudelliseen kehitykseen, erityisesti liikevaihdon ja katteiden parantumiseen.\n  Keywords: liikevaihto, kasvu, kate\n\n• Asiakashankinta ja pysyvyys\n  Confidence: 0.90\n  Description: Käsittelee asiakashankinnan kustannusten laskua ja asiakaspysyvyyden parantumista, mikä on tärkeää liiketoiminnan kestävyyden kannalta.\n  Keywords: asiakashankinta, asiakaspysyvyys, kustannus\n\n• Markkinalaajennusstrategia\n  Confidence: 0.85\n  Description: Keskittyy yrityksen strategisiin toimiin markkinoiden laajentamiseksi, erityisesti nouseviin teknologiasektoreihin.\n  Keywords: markkinalaajennusstrategia, teknologia, sektori\n\nTesting Category Analysis\n==================================================\n\nInput text:\nQ3 taloudelliset tulokset osoittavat 15% liikevaihdon kasvun ja parantuneet katteet. \n                    Asiakashankinnan kustannukset laskivat ja asiakaspysyvyys parani. \n                    Markkin...\n\nProcessing response: {'categories': [{'category': 'Financial Performance', 'confidence': 0.9, 'explanation': 'The text discusses financial results, specifically mentioning revenue growth and improved margins, which are key indicators of financial performance.', 'evidence': [{'text': 'Q3 taloudelliset tulokset osoittavat 15% liikevaihdon kasvun ja parantuneet katteet.', 'relevance': 0.95, 'matched_keywords': ['liikevaihto', 'kasvu', 'katteet', 'taloudelliset tulokset'], 'context': 'The text provides specific financial metrics for Q3.'}], 'themes': ['revenue growth', 'profit margins']}, {'category': 'Customer Acquisition', 'confidence': 0.85, 'explanation': 'The text mentions a decrease in customer acquisition costs and an improvement in customer retention, which are critical aspects of customer acquisition strategies.', 'evidence': [{'text': 'Asiakashankinnan kustannukset laskivat ja asiakaspysyvyys parani.', 'relevance': 0.9, 'matched_keywords': ['asiakashankinta', 'kustannukset', 'asiakaspysyvyys', 'parani'], 'context': 'The text highlights changes in customer acquisition costs and retention rates.'}], 'themes': ['customer retention', 'cost reduction']}, {'category': 'Market Strategy', 'confidence': 0.8, 'explanation': 'The text refers to a market expansion strategy focusing on emerging technology sectors, indicating a strategic approach to market growth.', 'evidence': [{'text': 'Markkinalaajennusstrategia keskittyy nouseviin teknologiasektoreihin.', 'relevance': 0.85, 'matched_keywords': ['markkinalaajennusstrategia', 'nouseviin', 'teknologiasektoreihin'], 'context': 'The text outlines a specific strategy for market expansion.'}], 'themes': ['market expansion', 'technology sectors']}], 'relationships': {'Financial Performance': ['Customer Acquisition', 'Market Strategy'], 'Customer Acquisition': ['Financial Performance', 'Market Strategy'], 'Market Strategy': ['Financial Performance', 'Customer Acquisition']}}\n\nProcessing category: {'category': 'Financial Performance', 'confidence': 0.9, 'explanation': 'The text discusses financial results, specifically mentioning revenue growth and improved margins, which are key indicators of financial performance.', 'evidence': [{'text': 'Q3 taloudelliset tulokset osoittavat 15% liikevaihdon kasvun ja parantuneet katteet.', 'relevance': 0.95, 'matched_keywords': ['liikevaihto', 'kasvu', 'katteet', 'taloudelliset tulokset'], 'context': 'The text provides specific financial metrics for Q3.'}], 'themes': ['revenue growth', 'profit margins']}\nCreated category: name='Financial Performance' confidence=0.9 description='The text discusses financial results, specifically mentioning revenue growth and improved margins, which are key indicators of financial performance.' evidence=[Evidence(text='Q3 taloudelliset tulokset osoittavat 15% liikevaihdon kasvun ja parantuneet katteet.', relevance=0.95)] themes=['revenue growth', 'profit margins']\n\nProcessing category: {'category': 'Customer Acquisition', 'confidence': 0.85, 'explanation': 'The text mentions a decrease in customer acquisition costs and an improvement in customer retention, which are critical aspects of customer acquisition strategies.', 'evidence': [{'text': 'Asiakashankinnan kustannukset laskivat ja asiakaspysyvyys parani.', 'relevance': 0.9, 'matched_keywords': ['asiakashankinta', 'kustannukset', 'asiakaspysyvyys', 'parani'], 'context': 'The text highlights changes in customer acquisition costs and retention rates.'}], 'themes': ['customer retention', 'cost reduction']}\nCreated category: name='Customer Acquisition' confidence=0.85 description='The text mentions a decrease in customer acquisition costs and an improvement in customer retention, which are critical aspects of customer acquisition strategies.' evidence=[Evidence(text='Asiakashankinnan kustannukset laskivat ja asiakaspysyvyys parani.', relevance=0.9)] themes=['customer retention', 'cost reduction']\n\nProcessing category: {'category': 'Market Strategy', 'confidence': 0.8, 'explanation': 'The text refers to a market expansion strategy focusing on emerging technology sectors, indicating a strategic approach to market growth.', 'evidence': [{'text': 'Markkinalaajennusstrategia keskittyy nouseviin teknologiasektoreihin.', 'relevance': 0.85, 'matched_keywords': ['markkinalaajennusstrategia', 'nouseviin', 'teknologiasektoreihin'], 'context': 'The text outlines a specific strategy for market expansion.'}], 'themes': ['market expansion', 'technology sectors']}\nCreated category: name='Market Strategy' confidence=0.8 description='The text refers to a market expansion strategy focusing on emerging technology sectors, indicating a strategic approach to market growth.' evidence=[Evidence(text='Markkinalaajennusstrategia keskittyy nouseviin teknologiasektoreihin.', relevance=0.85)] themes=['market expansion', 'technology sectors']\n\nFinal categories: [CategoryMatch(name='Financial Performance', confidence=0.9, description='The text discusses financial results, specifically mentioning revenue growth and improved margins, which are key indicators of financial performance.', evidence=[Evidence(text='Q3 taloudelliset tulokset osoittavat 15% liikevaihdon kasvun ja parantuneet katteet.', relevance=0.95)], themes=['revenue growth', 'profit margins']), CategoryMatch(name='Customer Acquisition', confidence=0.85, description='The text mentions a decrease in customer acquisition costs and an improvement in customer retention, which are critical aspects of customer acquisition strategies.', evidence=[Evidence(text='Asiakashankinnan kustannukset laskivat ja asiakaspysyvyys parani.', relevance=0.9)], themes=['customer retention', 'cost reduction']), CategoryMatch(name='Market Strategy', confidence=0.8, description='The text refers to a market expansion strategy focusing on emerging technology sectors, indicating a strategic approach to market growth.', evidence=[Evidence(text='Markkinalaajennusstrategia keskittyy nouseviin teknologiasektoreihin.', relevance=0.85)], themes=['market expansion', 'technology sectors'])]\n\nResults:\n--------------------\n\nCategories:\n\n• Financial Performance\n  Confidence: 0.90\n  Description: The text discusses financial results, specifically mentioning revenue growth and improved margins, which are key indicators of financial performance.\n\n  Evidence:\n  - Q3 taloudelliset tulokset osoittavat 15% liikevaihdon kasvun ja parantuneet katteet. (relevance: 0.95)\n\n• Customer Acquisition\n  Confidence: 0.85\n  Description: The text mentions a decrease in customer acquisition costs and an improvement in customer retention, which are critical aspects of customer acquisition strategies.\n\n  Evidence:\n  - Asiakashankinnan kustannukset laskivat ja asiakaspysyvyys parani. (relevance: 0.90)\n\n• Market Strategy\n  Confidence: 0.80\n  Description: The text refers to a market expansion strategy focusing on emerging technology sectors, indicating a strategic approach to market growth.\n\n  Evidence:\n  - Markkinalaajennusstrategia keskittyy nouseviin teknologiasektoreihin. (relevance: 0.85)\n"
        }
      ],
      "execution_count": 21,
      "metadata": {
        "gather": {
          "logged": 1733688013343
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "semantic-analyzer",
      "language": "python",
      "display_name": "Python (semantic-analyzer)"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.21",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "semantic-analyzer"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}