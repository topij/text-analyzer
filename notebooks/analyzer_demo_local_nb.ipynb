{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple, Union\n",
    "import logging\n",
    "import asyncio\n",
    "\n",
    "# Add project root to Python path if needed\n",
    "project_root = str(Path().resolve().parent)\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Import necessary components\n",
    "from src.nb_helpers.environment import setup_notebook_env, verify_environment\n",
    "from src.semantic_analyzer import SemanticAnalyzer\n",
    "from src.core.config import AnalyzerConfig\n",
    "\n",
    "from src.core.language_processing import create_text_processor\n",
    "from src.core.llm.factory import create_llm\n",
    "from src.loaders.parameter_handler import ParameterHandler\n",
    "from src.analyzers.keyword_analyzer import KeywordAnalyzer\n",
    "from src.analyzers.theme_analyzer import ThemeAnalyzer\n",
    "from src.analyzers.category_analyzer import CategoryAnalyzer\n",
    "\n",
    "import FileUtils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In analyzer_demo_local_nb.ipynb and azure_notebook.ipynb\n",
    "# from src.core.llm.factory import create_llm\n",
    "\n",
    "# Setup\n",
    "# config = AnalyzerConfig()\n",
    "# llm = create_llm(config=config)\n",
    "# analyzer = SemanticAnalyzer(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5.1\n"
     ]
    }
   ],
   "source": [
    "print(FileUtils.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up environment and logging\n",
    "setup_notebook_env(log_level=\"DEBUG\")\n",
    "verify_environment()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data to use\n",
    "test_texts = {\n",
    "    \"en\": {\n",
    "        \"technical\": \"\"\"Machine learning models are trained using large datasets to recognize patterns. \n",
    "                     The neural network architecture includes multiple layers for feature extraction. \n",
    "                     Data preprocessing and feature engineering are crucial steps.\"\"\",\n",
    "        \"business\": \"\"\"Q3 financial results show 15% revenue growth and improved profit margins. \n",
    "                    Customer acquisition costs decreased while retention rates increased. \n",
    "                    Market expansion strategy focuses on emerging technology sectors.\"\"\"\n",
    "    },\n",
    "    \"fi\": {\n",
    "        \"technical\": \"\"\"Koneoppimismalleja koulutetaan suurilla datajoukolla tunnistamaan kaavoja. \n",
    "                     Neuroverkon arkkitehtuuri sisältää useita kerroksia piirteiden erottamiseen. \n",
    "                     Datan esikäsittely ja piirteiden suunnittelu ovat keskeisiä vaiheita.\"\"\",\n",
    "        \"business\": \"\"\"Q3 taloudelliset tulokset osoittavat 15% liikevaihdon kasvun ja parantuneet katteet. \n",
    "                    Asiakashankinnan kustannukset laskivat ja asiakaspysyvyys parani. \n",
    "                    Markkinalaajennusstrategia keskittyy nouseviin teknologiasektoreihin.\"\"\"\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "     # llm = create_llm()\n",
    "config = AnalyzerConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_individual_analyzer(\n",
    "    analyzer: Union[KeywordAnalyzer, ThemeAnalyzer, CategoryAnalyzer], \n",
    "    text: str, \n",
    "    analyzer_type: str\n",
    "):\n",
    "    \"\"\"Test individual analyzer component.\"\"\"\n",
    "    print(f\"\\nTesting {analyzer_type} Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"\\nInput text:\")\n",
    "    print(text[:200] + \"...\" if len(text) > 200 else text)\n",
    "    \n",
    "    try:\n",
    "        results = await analyzer.analyze(text)\n",
    "        \n",
    "        print(\"\\nResults:\")\n",
    "        print(\"-\" * 20)\n",
    "        \n",
    "        if isinstance(analyzer, KeywordAnalyzer):\n",
    "            if results.keywords:\n",
    "                print(\"\\nKeywords:\")\n",
    "                for kw in results.keywords[:10]:  # Show top 10\n",
    "                    print(f\"• {kw.keyword:<20} ({kw.score:.2f})\")\n",
    "                    if kw.domain:\n",
    "                        print(f\"  Domain: {kw.domain}\")\n",
    "                \n",
    "                if results.compound_words:\n",
    "                    print(\"\\nCompound Words:\")\n",
    "                    print(\", \".join(results.compound_words))\n",
    "                    \n",
    "                if results.domain_keywords:\n",
    "                    print(\"\\nKeywords by Domain:\")\n",
    "                    for domain, kws in results.domain_keywords.items():\n",
    "                        print(f\"\\n{domain}:\")\n",
    "                        print(\", \".join(kws))\n",
    "                        \n",
    "        elif isinstance(analyzer, ThemeAnalyzer):\n",
    "            if results.themes:\n",
    "                print(\"\\nThemes:\")\n",
    "                for theme in results.themes:\n",
    "                    print(f\"\\n• {theme.name}\")\n",
    "                    print(f\"  Confidence: {theme.confidence:.2f}\")\n",
    "                    print(f\"  Description: {theme.description}\")\n",
    "                    if theme.keywords:\n",
    "                        print(f\"  Keywords: {', '.join(theme.keywords)}\")\n",
    "                \n",
    "                if results.theme_hierarchy:\n",
    "                    print(\"\\nTheme Hierarchy:\")\n",
    "                    for parent, children in results.theme_hierarchy.items():\n",
    "                        print(f\"{parent} -> {', '.join(children)}\")\n",
    "                        \n",
    "        elif isinstance(analyzer, CategoryAnalyzer):\n",
    "            if results.categories:\n",
    "                print(\"\\nCategories:\")\n",
    "                for cat in results.categories:\n",
    "                    print(f\"\\n• {cat.name}\")\n",
    "                    print(f\"  Confidence: {cat.confidence:.2f}\")\n",
    "                    if cat.description:\n",
    "                        print(f\"  Description: {cat.description}\")\n",
    "                    if cat.evidence:\n",
    "                        print(\"\\n  Evidence:\")\n",
    "                        for ev in cat.evidence:\n",
    "                            print(f\"  - {ev.text} (relevance: {ev.relevance:.2f})\")\n",
    "                            \n",
    "        if hasattr(results, 'error') and results.error:\n",
    "            print(f\"\\nErrors occurred: {results.error}\")\n",
    "            \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in analysis: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Test individual keyword analyzer\n",
    "async def test_keyword_analyzer():\n",
    "    \"\"\"Test keyword analyzer with different languages.\"\"\"\n",
    "    print(\"Testing Keyword Analyzer\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Initialize components\n",
    "    parameter_handler = ParameterHandler(\"parameters_fi.xlsx\")\n",
    "    # llm = create_llm()\n",
    "    # config = AnalyzerConfig()\n",
    "    llm = create_llm(config=config)\n",
    "    # analyzer = SemanticAnalyzer(llm=llm)\n",
    "    \n",
    "    # Test English\n",
    "    print(\"\\nTesting English Technical Content:\")\n",
    "    en_processor = create_text_processor(language=\"en\")\n",
    "    keyword_analyzer_en = KeywordAnalyzer(\n",
    "        llm=llm,\n",
    "        config=parameter_handler.parameters.general.model_dump(),\n",
    "        language_processor=en_processor\n",
    "    )\n",
    "    await test_individual_analyzer(keyword_analyzer_en, test_texts[\"en\"][\"technical\"], \"Keyword\")\n",
    "    \n",
    "    # Test Finnish\n",
    "    print(\"\\nTesting Finnish Technical Content:\")\n",
    "    fi_processor = create_text_processor(language=\"fi\")\n",
    "    keyword_analyzer_fi = KeywordAnalyzer(\n",
    "        llm=llm,\n",
    "        config=parameter_handler.parameters.general.model_dump(),\n",
    "        language_processor=fi_processor\n",
    "    )\n",
    "    await test_individual_analyzer(keyword_analyzer_fi, test_texts[\"fi\"][\"technical\"], \"Keyword\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Test all components\n",
    "async def test_components_for_language(language: str):\n",
    "    \"\"\"Test all components for a specific language.\"\"\"\n",
    "    print(f\"\\nTesting All Components for {language.upper()}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Initialize components\n",
    "    parameter_handler = ParameterHandler(f\"parameters_{language}.xlsx\")\n",
    "     # llm = create_llm()\n",
    "    # config = AnalyzerConfig()\n",
    "    llm = create_llm(provider='openai', config=config)\n",
    "\n",
    "    language_processor = create_text_processor(language=language)\n",
    "    \n",
    "    # Create analyzers\n",
    "    keyword_analyzer = KeywordAnalyzer(\n",
    "        llm=llm,\n",
    "        config=parameter_handler.parameters.general.model_dump(),\n",
    "        language_processor=language_processor\n",
    "    )\n",
    "    \n",
    "    theme_analyzer = ThemeAnalyzer(\n",
    "        llm=llm,\n",
    "        config=parameter_handler.parameters.general.model_dump(),\n",
    "        language_processor=language_processor\n",
    "    )\n",
    "    \n",
    "    category_analyzer = CategoryAnalyzer(\n",
    "        categories=parameter_handler.parameters.categories,\n",
    "        llm=llm,\n",
    "        config=parameter_handler.parameters.general.model_dump(),\n",
    "        language_processor=language_processor\n",
    "    )\n",
    "    \n",
    "    # Test technical content\n",
    "    print(f\"\\nTesting {language.upper()} Technical Content:\")\n",
    "    await test_individual_analyzer(keyword_analyzer, test_texts[language][\"technical\"], \"Keyword\")\n",
    "    await test_individual_analyzer(theme_analyzer, test_texts[language][\"technical\"], \"Theme\")\n",
    "    await test_individual_analyzer(category_analyzer, test_texts[language][\"technical\"], \"Category\")\n",
    "    \n",
    "    # Test business content\n",
    "    print(f\"\\nTesting {language.upper()} Business Content:\")\n",
    "    await test_individual_analyzer(keyword_analyzer, test_texts[language][\"business\"], \"Keyword\")\n",
    "    await test_individual_analyzer(theme_analyzer, test_texts[language][\"business\"], \"Theme\")\n",
    "    await test_individual_analyzer(category_analyzer, test_texts[language][\"business\"], \"Category\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Quick test of full pipeline\n",
    "async def test_pipeline():\n",
    "    \"\"\"Test full pipeline with both languages.\"\"\"\n",
    "    print(\"Testing Full Pipeline\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    llm = create_llm(provider='openai', config=config)\n",
    "    # analyzer = SemanticAnalyzer(llm=llm)\n",
    "    # Test English pipeline\n",
    "    print(\"\\nEnglish Pipeline:\")\n",
    "    en_analyzer = SemanticAnalyzer(llm=llm, parameter_file=\"parameters_en.xlsx\")\n",
    "    result = await en_analyzer.analyze(test_texts[\"en\"][\"technical\"])\n",
    "    print(f\"Success: {result.success}\")\n",
    "    print(f\"Keywords found: {len(result.keywords.keywords)}\")\n",
    "    print(f\"Themes found: {len(result.themes.themes)}\")\n",
    "    print(f\"Categories found: {len(result.categories.matches)}\")\n",
    "    \n",
    "    # Test Finnish pipeline\n",
    "    print(\"\\nFinnish Pipeline:\")\n",
    "    fi_analyzer = SemanticAnalyzer(parameter_file=\"parameters_fi.xlsx\")\n",
    "    result = await fi_analyzer.analyze(test_texts[\"fi\"][\"technical\"])\n",
    "    print(f\"Success: {result.success}\")\n",
    "    print(f\"Keywords found: {len(result.keywords.keywords)}\")\n",
    "    print(f\"Themes found: {len(result.themes.themes)}\")\n",
    "    print(f\"Categories found: {len(result.categories.matches)}\")\n",
    "\n",
    "# Run the tests\n",
    "async def run_all_tests():\n",
    "    \"\"\"Run all tests.\"\"\"\n",
    "    # Test individual component\n",
    "    await test_keyword_analyzer()\n",
    "    \n",
    "    # Test all components by language\n",
    "    await test_components_for_language(\"en\")\n",
    "    await test_components_for_language(\"fi\")\n",
    "    \n",
    "    # Test full pipeline\n",
    "    await test_pipeline()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run in notebook\n",
    "# await run_all_tests()\n",
    "\n",
    "# Or run individual tests:\n",
    "# await test_keyword_analyzer()\n",
    "await test_components_for_language(\"fi\")\n",
    "# await test_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semantic-analyzer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
